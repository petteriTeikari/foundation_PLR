# Classification Metrics Registry
# ================================
# AIDEV-NOTE: All metrics logged in PLR_Classification and PLR_Featurization experiments.
# Generated from MLflow on 2026-01-22.
#
# Naming convention:
# - Base metric: e.g., "AUROC"
# - With CI: "AUROC_CI_hi", "AUROC_CI_lo"
# - Split prefixes: "test/", "train/", "val/"

version: "1.0.0"
experiment_ids:
  - "253031330985650090"  # PLR_Classification
  - "143964216992376241"  # PLR_Featurization

# All unique base metrics (excluding CI bounds)
# Total: 26 metrics
metrics:
  # Discrimination metrics
  discrimination:
    AUROC:
      display_name: "AUROC"
      description: "Area Under ROC Curve"
      higher_is_better: true
      range: [0, 1]

    AUPR:
      display_name: "AUPR"
      description: "Area Under Precision-Recall Curve"
      higher_is_better: true
      range: [0, 1]

    tpAUC:
      display_name: "Partial AUC"
      description: "Two-point AUC (sensitivity/specificity constrained)"
      higher_is_better: true
      range: [0, 1]

  # Uncertainty and calibration metrics
  uncertainty:
    AURC:
      display_name: "AURC"
      description: "Area Under Risk-Coverage Curve"
      higher_is_better: false
      range: [0, 1]

    AURC_E:
      display_name: "Excess AURC"
      description: "Excess AURC (above optimal)"
      higher_is_better: false
      range: [0, 1]

    Brier:
      display_name: "Brier Score"
      description: "Mean squared error of probabilistic predictions"
      higher_is_better: false
      range: [0, 1]

    ECE:
      display_name: "ECE"
      description: "Expected Calibration Error"
      higher_is_better: false
      range: [0, 1]

    confidence:
      display_name: "Confidence"
      description: "Mean predicted confidence"
      higher_is_better: null  # Depends on context
      range: [0, 1]

    entropy:
      display_name: "Entropy"
      description: "Predictive entropy (total uncertainty)"
      higher_is_better: null  # Higher = more uncertain
      range: [0, null]

    entropy_of_expected:
      display_name: "Entropy of Expected"
      description: "Entropy of expected prediction (aleatoric)"
      higher_is_better: null
      range: [0, null]

    expected_entropy:
      display_name: "Expected Entropy"
      description: "Expected entropy under posterior"
      higher_is_better: null
      range: [0, null]

    MI:
      display_name: "Mutual Information"
      description: "Epistemic uncertainty (model uncertainty)"
      higher_is_better: null
      range: [0, null]

    mutual_information:
      display_name: "Mutual Information"
      description: "Alias for MI"
      higher_is_better: null
      range: [0, null]

  # Classification performance metrics
  classification:
    accuracy:
      display_name: "Accuracy"
      description: "Classification accuracy"
      higher_is_better: true
      range: [0, 1]

    accuracy_balanced:
      display_name: "Balanced Accuracy"
      description: "Average of sensitivity and specificity"
      higher_is_better: true
      range: [0, 1]

    F1:
      display_name: "F1 Score"
      description: "Harmonic mean of precision and recall"
      higher_is_better: true
      range: [0, 1]

    sensitivity:
      display_name: "Sensitivity"
      description: "True Positive Rate / Recall"
      higher_is_better: true
      range: [0, 1]

    specificity:
      display_name: "Specificity"
      description: "True Negative Rate"
      higher_is_better: true
      range: [0, 1]

    NPV:
      display_name: "NPV"
      description: "Negative Predictive Value"
      higher_is_better: true
      range: [0, 1]

    NPV_prev:
      display_name: "NPV (prev-adjusted)"
      description: "NPV adjusted for disease prevalence"
      higher_is_better: true
      range: [0, 1]

    PPV:
      display_name: "PPV"
      description: "Positive Predictive Value (Precision)"
      higher_is_better: true
      range: [0, 1]

    PPV_prev:
      display_name: "PPV (prev-adjusted)"
      description: "PPV adjusted for disease prevalence"
      higher_is_better: true
      range: [0, 1]

  # Class-specific probability statistics
  class_probs:
    probs_Control_mean_:
      display_name: "Control Prob Mean"
      description: "Mean predicted probability for control subjects"
      higher_is_better: false  # Lower means model is confident they're controls
      range: [0, 1]

    probs_Control_std_:
      display_name: "Control Prob Std"
      description: "Std of predicted probability for control subjects"
      higher_is_better: null
      range: [0, 0.5]

    probs_Glaucoma_mean_:
      display_name: "Glaucoma Prob Mean"
      description: "Mean predicted probability for glaucoma subjects"
      higher_is_better: true
      range: [0, 1]

    probs_Glaucoma_std_:
      display_name: "Glaucoma Prob Std"
      description: "Std of predicted probability for glaucoma subjects"
      higher_is_better: null
      range: [0, 0.5]

# Computed metrics (not directly from MLflow)
computed_metrics:
  scaled_brier:
    display_name: "Scaled Brier (IPA)"
    description: "Index of Prediction Accuracy: 1 - Brier/Brier_null"
    formula: "1 - Brier / (prevalence * (1 - prevalence))"
    higher_is_better: true
    range: [-inf, 1]

  net_benefit:
    display_name: "Net Benefit"
    description: "Decision curve analysis metric"
    formula: "TP/n - FP/n * (threshold / (1-threshold))"
    higher_is_better: true
    range: [-inf, prevalence]
    default_threshold: 0.15

# Recommended metric groups for visualization
visualization_groups:
  retention_curves:
    description: "Metrics suitable for retention/risk-coverage curves"
    recommended: [AUROC, Brier, AURC, F1, accuracy, sensitivity, specificity]

  calibration_plots:
    description: "Metrics for calibration assessment"
    recommended: [Brier, ECE, confidence]

  uncertainty_analysis:
    description: "Metrics for uncertainty quantification"
    recommended: [AURC, entropy, MI, confidence]

  clinical_decision:
    description: "Metrics for clinical decision making"
    recommended: [sensitivity, specificity, PPV, NPV, net_benefit]
