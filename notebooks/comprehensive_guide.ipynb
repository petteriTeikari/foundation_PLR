{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation PLR: Comprehensive Research Guide\n",
    "\n",
    "**A complete guide for researchers who want to:**\n",
    "1. Understand what data is stored and how\n",
    "2. Reproduce our exact results\n",
    "3. Train new classifiers (including future ones released in 2027+)\n",
    "4. Apply new biostatistics methods\n",
    "5. Run the pipeline on their own PLR data\n",
    "6. Customize the featurization (bins, windows, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "> **Who is this for?**\n",
    ">\n",
    "> - Researchers wanting to validate or extend our work\n",
    "> - Clinicians/PIs who want to understand what we did\n",
    "> - Developers implementing new classifiers or methods\n",
    "> - Anyone with their own PLR data who wants to use our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: Understanding the Data\n",
    "\n",
    "## 1.1 What is PLR (Pupillary Light Reflex)?\n",
    "\n",
    "> **ELI5 for PIs:**\n",
    ">\n",
    "> When you shine a light into someone's eye, their pupil gets smaller (constricts).\n",
    "> When you turn the light off, it gets bigger again (redilates).\n",
    ">\n",
    "> **The key insight:** In glaucoma, certain cells in the eye (melanopsin-containing retinal ganglion cells)\n",
    "> are damaged. These cells are responsible for a specific part of the pupil response - the\n",
    "> \"sustained\" constriction after blue light.\n",
    ">\n",
    "> By measuring the pupil response precisely with different colors of light (red vs blue),\n",
    "> we can potentially detect glaucoma damage.\n",
    "\n",
    "## 1.2 The Data Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw PLR Recording (pupillometer) \n",
    "    |\n",
    "    v\n",
    "[1] OUTLIER DETECTION - Remove blinks, artifacts\n",
    "    |\n",
    "    v\n",
    "[2] IMPUTATION - Fill in missing data points\n",
    "    |\n",
    "    v\n",
    "[3] FEATURIZATION - Extract meaningful numbers from the curves\n",
    "    |                (e.g., \"maximum constriction was 2.3mm at 0.8 seconds\")\n",
    "    v\n",
    "[4] CLASSIFICATION - Train ML models to predict glaucoma\n",
    "    |\n",
    "    v\n",
    "[5] EVALUATION - Bootstrap confidence intervals, effect sizes, calibration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 What's in the Shared DuckDB Files?\n",
    "\n",
    "We provide **two** DuckDB database files:\n",
    "\n",
    "### File 1: `foundation_plr_results.db` (~4 MB)\n",
    "\n",
    "Contains the **final results** - everything you need to reproduce our statistical analysis:\n",
    "\n",
    "| Table | What it contains | Rows |\n",
    "|-------|-----------------|------|\n",
    "| `predictions` | Every single prediction made by every classifier | ~20,000 |\n",
    "| `metrics_per_fold` | Performance metrics per cross-validation fold | ~10,000 |\n",
    "| `metrics_aggregate` | Summary statistics (mean, CI) per classifier | ~14,000 |\n",
    "| `mlflow_runs` | Experiment metadata | ~300 |\n",
    "\n",
    "### File 2: `foundation_plr_distributions.db` (~40 MB)\n",
    "\n",
    "Contains the **full distributions** for uncertainty quantification:\n",
    "\n",
    "| Table | What it contains | Rows |\n",
    "|-------|-----------------|------|\n",
    "| `bootstrap_distributions` | All bootstrap iterations (not just summary) | ~1.3 million |\n",
    "| `subject_predictions` | Per-subject predictions with uncertainty | ~115,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths - adjust to where you saved the files\n",
    "DATA_DIR = Path(\"../outputs\")\n",
    "RESULTS_DB = DATA_DIR / \"foundation_plr_results.db\"\n",
    "DISTRIBUTIONS_DB = DATA_DIR / \"foundation_plr_distributions.db\"\n",
    "\n",
    "print(f\"Results DB exists: {RESULTS_DB.exists()}\")\n",
    "print(f\"Distributions DB exists: {DISTRIBUTIONS_DB.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Complete Column Reference\n",
    "\n",
    "### Table: `predictions`\n",
    "\n",
    "> **What is this?** Each row = one prediction for one subject by one classifier in one fold.\n",
    "\n",
    "| Column | Type | What it means | Example |\n",
    "|--------|------|---------------|--------|\n",
    "| `prediction_id` | INT | Unique row identifier | 1, 2, 3... |\n",
    "| `subject_id` | VARCHAR | Anonymous patient ID | \"PLR1001\" |\n",
    "| `eye` | VARCHAR | Which eye | \"OD\" (right), \"OS\" (left) |\n",
    "| `fold` | INT | Cross-validation fold (0-4) | 0, 1, 2, 3, 4 |\n",
    "| `bootstrap_iter` | INT | Bootstrap iteration (0=original) | 0-999 |\n",
    "| `outlier_method` | VARCHAR | How blinks/artifacts were detected | \"ensemble-LOF-MOMENT-...\" |\n",
    "| `imputation_method` | VARCHAR | How missing data was filled | \"SAITS\" |\n",
    "| `featurization` | VARCHAR | Feature extraction method | \"simple1.0\" |\n",
    "| `classifier` | VARCHAR | ML algorithm | \"TabM\", \"XGBOOST\", \"TabPFN\", \"LogisticRegression\" |\n",
    "| `source_name` | VARCHAR | Full pipeline configuration string | \"XGBOOST_eval-auc__simple1.0__SAITS__ensemble-...\" |\n",
    "| `y_true` | INT | **Ground truth**: Does patient have glaucoma? | 0 (no), 1 (yes) |\n",
    "| `y_pred` | INT | **Binary prediction**: Classifier's decision | 0 (no), 1 (yes) |\n",
    "| `y_prob` | FLOAT | **Probability**: Classifier's confidence (0.0-1.0) | 0.73 = \"73% confident it's glaucoma\" |\n",
    "| `mlflow_run_id` | VARCHAR | Experiment tracking ID | \"abc123...\" |\n",
    "\n",
    "### Understanding `source_name`\n",
    "\n",
    "> **ELI5:** The source_name encodes the ENTIRE pipeline configuration in one string.\n",
    ">\n",
    "> Format: `{CLASSIFIER}_{METRIC}__{FEATURIZATION}__{IMPUTATION}__{OUTLIER_DETECTION}`\n",
    ">\n",
    "> Example: `XGBOOST_eval-auc__simple1.0__SAITS__ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune`\n",
    ">\n",
    "> Means:\n",
    "> - Classifier: XGBoost optimized for AUC\n",
    "> - Features: Simple v1.0 handcrafted features\n",
    "> - Imputation: SAITS (Self-Attention Imputation for Time Series)\n",
    "> - Outliers: Ensemble of 7 methods (LOF, MOMENT, OneClassSVM, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the predictions table\n",
    "con = duckdb.connect(str(RESULTS_DB), read_only=True)\n",
    "\n",
    "print(\"=== PREDICTIONS TABLE ===\")\n",
    "print(\"\\nColumn info:\")\n",
    "print(con.execute(\"DESCRIBE predictions\").fetchdf().to_string())\n",
    "\n",
    "print(\"\\n\\nSample rows:\")\n",
    "print(con.execute(\"SELECT * FROM predictions LIMIT 3\").fetchdf().to_string())\n",
    "\n",
    "print(\"\\n\\nUnique classifiers:\")\n",
    "print(con.execute(\"SELECT DISTINCT classifier FROM predictions\").fetchdf())\n",
    "\n",
    "print(\"\\n\\nData summary:\")\n",
    "summary = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_predictions,\n",
    "        COUNT(DISTINCT subject_id) as unique_subjects,\n",
    "        COUNT(DISTINCT classifier) as classifiers,\n",
    "        ROUND(AVG(y_true), 3) as glaucoma_prevalence\n",
    "    FROM predictions\n",
    "\"\"\").fetchdf()\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: `metrics_per_fold`\n",
    "\n",
    "> **What is this?** Performance metrics calculated separately for each CV fold.\n",
    "\n",
    "| Column | What it means |\n",
    "|--------|---------------|\n",
    "| `metric_id` | Unique row ID |\n",
    "| `classifier` | Which classifier |\n",
    "| `fold` | Which CV fold (0-4) |\n",
    "| `metric_name` | Which metric (see below) |\n",
    "| `metric_value` | The value |\n",
    "| `bootstrap_iter` | Bootstrap iteration |\n",
    "| `source_name` | Full pipeline config |\n",
    "\n",
    "**Available metrics:**\n",
    "\n",
    "| Metric | What it measures | Range | Ideal |\n",
    "|--------|-----------------|-------|-------|\n",
    "| `auroc` | Area Under ROC Curve | 0-1 | 1.0 |\n",
    "| `auprc` | Area Under Precision-Recall Curve | 0-1 | 1.0 |\n",
    "| `brier` | Brier Score (calibration) | 0-1 | 0.0 |\n",
    "| `accuracy` | Correct predictions / Total | 0-1 | 1.0 |\n",
    "| `sensitivity` | True Positives / All Positives | 0-1 | 1.0 |\n",
    "| `specificity` | True Negatives / All Negatives | 0-1 | 1.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: `bootstrap_distributions` (in distributions DB)\n",
    "\n",
    "> **ELI5 for PIs:** Bootstrap is like asking \"if we had slightly different patients, would we get the same result?\"\n",
    "> We resample the data 1000 times and calculate metrics each time.\n",
    "> This gives us a distribution of possible values, not just one number.\n",
    "\n",
    "| Column | What it means |\n",
    "|--------|---------------|\n",
    "| `dist_id` | Unique row ID |\n",
    "| `classifier` | Which classifier |\n",
    "| `metric_name` | Which metric |\n",
    "| `fold` | Which CV fold |\n",
    "| `bootstrap_iter` | Which bootstrap iteration (0-999) |\n",
    "| `metric_value` | Metric value for THIS bootstrap sample |\n",
    "| `source_name` | Pipeline config |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: `subject_predictions` (in distributions DB)\n",
    "\n",
    "> **What is this?** Per-subject predictions with uncertainty measures.\n",
    "> Use this for the \"probability distributions per outcome category\" plots.\n",
    "\n",
    "| Column | What it means |\n",
    "|--------|---------------|\n",
    "| `pred_id` | Unique row ID |\n",
    "| `source_name` | Full pipeline config |\n",
    "| `classifier` | Which classifier |\n",
    "| `split` | \"train\" or \"test\" |\n",
    "| `subject_code` | Anonymous subject ID |\n",
    "| `y_true` | Ground truth (0=healthy, 1=glaucoma) |\n",
    "| `y_pred_proba` | Predicted probability (0.0-1.0) |\n",
    "| `y_pred` | Binary prediction (0 or 1) |\n",
    "| `confidence` | Prediction confidence (if available) |\n",
    "| `entropy_of_expected` | Uncertainty measure |\n",
    "| `expected_entropy` | Uncertainty measure |\n",
    "| `mutual_information` | Uncertainty measure |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: Reproducing Our Results\n",
    "\n",
    "## 2.1 Get the Exact Numbers from Our Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Table 1: Classifier Performance\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 1: Classification Performance for Glaucoma Detection\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    classifier as Classifier,\n",
    "    ROUND(mean, 3) as AUROC,\n",
    "    '[' || ROUND(ci_lower, 3) || ', ' || ROUND(ci_upper, 3) || ']' as \"95% CI\",\n",
    "    ROUND(std, 3) as SE\n",
    "FROM metrics_aggregate\n",
    "WHERE metric_name = 'auroc'\n",
    "ORDER BY mean DESC\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the pairwise comparisons with effect sizes\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 2: Pairwise Comparisons (Effect Sizes)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get fold-level AUROCs for effect size calculation\n",
    "fold_aurocs = con.execute(\"\"\"\n",
    "    SELECT classifier, fold, metric_value as auroc\n",
    "    FROM metrics_per_fold\n",
    "    WHERE metric_name = 'auroc' AND bootstrap_iter = 0\n",
    "    ORDER BY classifier, fold\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Calculate Cohen's d for each pair\n",
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "classifiers = fold_aurocs['classifier'].unique()\n",
    "comparisons = []\n",
    "\n",
    "for clf1, clf2 in itertools.combinations(classifiers, 2):\n",
    "    x1 = fold_aurocs[fold_aurocs['classifier'] == clf1]['auroc'].values\n",
    "    x2 = fold_aurocs[fold_aurocs['classifier'] == clf2]['auroc'].values\n",
    "    \n",
    "    # Cohen's d\n",
    "    pooled_std = np.sqrt(((len(x1)-1)*np.var(x1, ddof=1) + (len(x2)-1)*np.var(x2, ddof=1)) / (len(x1)+len(x2)-2))\n",
    "    d = (np.mean(x1) - np.mean(x2)) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Paired t-test\n",
    "    if len(x1) == len(x2):  # Same folds\n",
    "        t_stat, p_val = stats.ttest_rel(x1, x2)\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_ind(x1, x2)\n",
    "    \n",
    "    # Interpretation\n",
    "    if abs(d) < 0.2:\n",
    "        interp = \"negligible\"\n",
    "    elif abs(d) < 0.5:\n",
    "        interp = \"small\"\n",
    "    elif abs(d) < 0.8:\n",
    "        interp = \"medium\"\n",
    "    else:\n",
    "        interp = \"large\"\n",
    "    \n",
    "    comparisons.append({\n",
    "        'Classifier 1': clf1,\n",
    "        'Classifier 2': clf2,\n",
    "        \"Cohen's d\": round(d, 3),\n",
    "        'p-value': f\"{p_val:.2e}\" if p_val < 0.001 else f\"{p_val:.4f}\",\n",
    "        'Effect': interp\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(comparisons).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reproduce Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Figure: Probability distributions per outcome category (Van Calster 2024 requirement)\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    con_dist = duckdb.connect(str(DISTRIBUTIONS_DB), read_only=True)\n",
    "    \n",
    "    # Get test set predictions\n",
    "    df = con_dist.execute(\"\"\"\n",
    "        SELECT classifier, y_true, y_pred_proba \n",
    "        FROM subject_predictions \n",
    "        WHERE split = 'test'\n",
    "        AND classifier IN ('TabM', 'XGBOOST', 'TabPFN', 'LogisticRegression')\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    for ax, clf in zip(axes.flatten(), ['TabM', 'XGBOOST', 'TabPFN', 'LogisticRegression']):\n",
    "        clf_df = df[df['classifier'] == clf]\n",
    "        \n",
    "        controls = clf_df[clf_df['y_true'] == 0]['y_pred_proba']\n",
    "        cases = clf_df[clf_df['y_true'] == 1]['y_pred_proba']\n",
    "        \n",
    "        ax.hist(controls, bins=30, alpha=0.6, label=f'Controls (n={len(controls)})', \n",
    "                color='#3498db', density=True)\n",
    "        ax.hist(cases, bins=30, alpha=0.6, label=f'Glaucoma (n={len(cases)})', \n",
    "                color='#e74c3c', density=True)\n",
    "        ax.axvline(x=0.5, color='gray', linestyle='--', label='Threshold')\n",
    "        ax.set_xlabel('Predicted Probability')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(clf)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('Probability Distributions by True Outcome (Test Set)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    con_dist.close()\n",
    "else:\n",
    "    print(\"Distributions database not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: Training New Classifiers\n",
    "\n",
    "> **Scenario:** It's 2027 and a new amazing classifier called \"SuperTabNet\" was just released.\n",
    "> You want to test it on our data.\n",
    "\n",
    "## 3.1 Load the Features\n",
    "\n",
    "The features we used are embedded in the predictions. For training new classifiers,\n",
    "you need to extract the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Use the predictions to get labels and subject IDs,\n",
    "# then join with features if you have access to them\n",
    "\n",
    "# Get unique subjects and their labels from the test fold of the best classifier\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT \n",
    "    subject_id,\n",
    "    eye,\n",
    "    y_true as label\n",
    "FROM predictions\n",
    "WHERE classifier = 'TabM' AND fold = 0\n",
    "ORDER BY subject_id, eye\n",
    "\"\"\"\n",
    "\n",
    "subjects = con.execute(query).fetchdf()\n",
    "print(f\"Subjects in the dataset: {len(subjects)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(subjects['label'].value_counts())\n",
    "print(f\"\\nFirst few subjects:\")\n",
    "print(subjects.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: If you have the features database (foundation_plr_features.db),\n",
    "# you can load the actual feature vectors\n",
    "\n",
    "FEATURES_DB = DATA_DIR / \"foundation_plr_features.db\"\n",
    "\n",
    "if FEATURES_DB.exists():\n",
    "    con_feat = duckdb.connect(str(FEATURES_DB), read_only=True)\n",
    "    \n",
    "    # Get schema\n",
    "    print(\"Feature columns:\")\n",
    "    schema = con_feat.execute(\"DESCRIBE plr_features\").fetchdf()\n",
    "    print(schema['column_name'].tolist())\n",
    "    \n",
    "    # Load features\n",
    "    X_df = con_feat.execute(\"SELECT * FROM plr_features\").fetchdf()\n",
    "    \n",
    "    # Separate features from metadata\n",
    "    metadata_cols = ['subject_id', 'eye', 'source_name', 'has_glaucoma', 'split']\n",
    "    feature_cols = [c for c in X_df.columns if c not in metadata_cols]\n",
    "    \n",
    "    X = X_df[feature_cols].values\n",
    "    y = X_df['has_glaucoma'].values\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Feature names: {feature_cols[:10]}...\")\n",
    "    \n",
    "    con_feat.close()\n",
    "else:\n",
    "    print(\"Features database not found.\")\n",
    "    print(\"Using prediction probabilities as proxy features for demonstration.\")\n",
    "    \n",
    "    # Get probabilities from all classifiers as pseudo-features\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        subject_id,\n",
    "        eye,\n",
    "        MAX(CASE WHEN classifier = 'TabM' THEN y_prob END) as prob_TabM,\n",
    "        MAX(CASE WHEN classifier = 'XGBOOST' THEN y_prob END) as prob_XGBOOST,\n",
    "        MAX(CASE WHEN classifier = 'TabPFN' THEN y_prob END) as prob_TabPFN,\n",
    "        MAX(CASE WHEN classifier = 'LogisticRegression' THEN y_prob END) as prob_LR,\n",
    "        MAX(y_true) as label\n",
    "    FROM predictions\n",
    "    WHERE fold = 0\n",
    "    GROUP BY subject_id, eye\n",
    "    \"\"\"\n",
    "    \n",
    "    df = con.execute(query).fetchdf().dropna()\n",
    "    X = df[['prob_TabM', 'prob_XGBOOST', 'prob_TabPFN', 'prob_LR']].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"Pseudo-feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train a New Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Example: Train a Random Forest (pretend this is \"SuperTabNet 2027\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize your new classifier\n",
    "new_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Use 5-fold CV like we did\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Get cross-validated predictions\n",
    "y_prob_cv = cross_val_predict(new_clf, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Calculate AUROC\n",
    "auroc = roc_auc_score(y, y_prob_cv)\n",
    "print(f\"\\nYour new classifier AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Compare to our results\n",
    "print(\"\\nComparison to our classifiers:\")\n",
    "our_results = con.execute(\"\"\"\n",
    "    SELECT classifier, ROUND(mean, 4) as auroc\n",
    "    FROM metrics_aggregate\n",
    "    WHERE metric_name = 'auroc'\n",
    "    ORDER BY mean DESC\n",
    "\"\"\").fetchdf()\n",
    "print(our_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bootstrap Confidence Intervals for Your Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auroc_ci(y_true, y_prob, n_iterations=1000, alpha=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval for AUROC.\n",
    "    \n",
    "    This is the same method we used in the paper.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(y_true)\n",
    "    aurocs = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Resample with replacement\n",
    "        idx = rng.choice(n, n, replace=True)\n",
    "        \n",
    "        # Skip if only one class in sample\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "            \n",
    "        aurocs.append(roc_auc_score(y_true[idx], y_prob[idx]))\n",
    "    \n",
    "    aurocs = np.array(aurocs)\n",
    "    ci_lower = np.percentile(aurocs, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(aurocs, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(aurocs),\n",
    "        'std': np.std(aurocs),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'n_valid_iterations': len(aurocs)\n",
    "    }\n",
    "\n",
    "# Calculate CI for your classifier\n",
    "ci_result = bootstrap_auroc_ci(y.astype(int), y_prob_cv, n_iterations=1000)\n",
    "\n",
    "print(f\"Your classifier results:\")\n",
    "print(f\"  AUROC: {ci_result['mean']:.4f}\")\n",
    "print(f\"  95% CI: [{ci_result['ci_lower']:.4f}, {ci_result['ci_upper']:.4f}]\")\n",
    "print(f\"  SE: {ci_result['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: Applying New Biostatistics Methods\n",
    "\n",
    "> **Scenario:** A new calibration method or clinical utility metric is published.\n",
    "> You want to apply it to our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Get Raw Predictions for Custom Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions for the best classifier (TabM)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    subject_id,\n",
    "    eye,\n",
    "    fold,\n",
    "    y_true,\n",
    "    y_prob,\n",
    "    y_pred\n",
    "FROM predictions\n",
    "WHERE classifier = 'TabM'\n",
    "  AND bootstrap_iter = 0  -- Original predictions (not bootstrap)\n",
    "ORDER BY subject_id, eye, fold\n",
    "\"\"\"\n",
    "\n",
    "df_predictions = con.execute(query).fetchdf()\n",
    "\n",
    "# Convert to numpy for analysis\n",
    "y_true_all = df_predictions['y_true'].values\n",
    "y_prob_all = df_predictions['y_prob'].values\n",
    "y_pred_all = df_predictions['y_pred'].values\n",
    "\n",
    "print(f\"Loaded {len(df_predictions)} predictions\")\n",
    "print(f\"Unique subjects: {df_predictions['subject_id'].nunique()}\")\n",
    "print(f\"Folds: {sorted(df_predictions['fold'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Example: Custom Calibration Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_calibration_index(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    Integrated Calibration Index (ICI) - a newer calibration metric.\n",
    "    \n",
    "    ICI = mean absolute difference between predicted and observed probabilities\n",
    "    across the probability range.\n",
    "    \n",
    "    Lower is better (0 = perfect calibration).\n",
    "    \"\"\"\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    # Get calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    \n",
    "    # ICI = mean absolute deviation\n",
    "    ici = np.mean(np.abs(prob_true - prob_pred))\n",
    "    \n",
    "    return ici\n",
    "\n",
    "# Calculate ICI for each classifier\n",
    "print(\"Integrated Calibration Index (ICI):\")\n",
    "print(\"(Lower is better, 0 = perfect)\\n\")\n",
    "\n",
    "for clf in ['TabM', 'XGBOOST', 'TabPFN', 'LogisticRegression']:\n",
    "    df_clf = con.execute(f\"\"\"\n",
    "        SELECT y_true, y_prob \n",
    "        FROM predictions \n",
    "        WHERE classifier = '{clf}' AND bootstrap_iter = 0 AND fold = 0\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    ici = integrated_calibration_index(df_clf['y_true'].values, df_clf['y_prob'].values)\n",
    "    print(f\"  {clf}: ICI = {ici:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Example: Custom Clinical Utility Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_benefit_at_threshold(y_true, y_prob, threshold):\n",
    "    \"\"\"\n",
    "    Calculate net benefit at a specific decision threshold.\n",
    "    \n",
    "    Net Benefit = TP/n - FP/n Ã— (pt / (1-pt))\n",
    "    \n",
    "    where pt is the threshold probability.\n",
    "    \n",
    "    This is from Vickers & Elkin (2006) Decision Curve Analysis.\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    \n",
    "    if threshold >= 1 or threshold <= 0:\n",
    "        return 0\n",
    "    \n",
    "    nb = (tp / n) - (fp / n) * (threshold / (1 - threshold))\n",
    "    return nb\n",
    "\n",
    "\n",
    "def standardized_net_benefit(y_true, y_prob, threshold):\n",
    "    \"\"\"\n",
    "    Standardized Net Benefit (sNB) - normalized to [0, 1] range.\n",
    "    \n",
    "    sNB = NB / prevalence\n",
    "    \n",
    "    This makes it easier to compare across studies with different prevalences.\n",
    "    \"\"\"\n",
    "    nb = net_benefit_at_threshold(y_true, y_prob, threshold)\n",
    "    prevalence = np.mean(y_true)\n",
    "    return nb / prevalence if prevalence > 0 else 0\n",
    "\n",
    "\n",
    "# Calculate at clinically relevant thresholds\n",
    "print(\"Net Benefit at Clinically Relevant Thresholds:\")\n",
    "print(\"(Higher is better, compare to 'treat all' baseline)\\n\")\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.5]\n",
    "prevalence = y_true_all.mean()\n",
    "\n",
    "print(f\"Prevalence: {prevalence:.3f}\\n\")\n",
    "\n",
    "for t in thresholds:\n",
    "    treat_all_nb = prevalence - (1 - prevalence) * (t / (1 - t))\n",
    "    print(f\"\\nThreshold = {t}:\")\n",
    "    print(f\"  Treat All NB: {treat_all_nb:.4f}\")\n",
    "    \n",
    "    for clf in ['TabM', 'XGBOOST', 'TabPFN', 'LogisticRegression']:\n",
    "        df_clf = con.execute(f\"\"\"\n",
    "            SELECT y_true, y_prob \n",
    "            FROM predictions \n",
    "            WHERE classifier = '{clf}' AND bootstrap_iter = 0 AND fold = 0\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        nb = net_benefit_at_threshold(df_clf['y_true'].values, df_clf['y_prob'].values, t)\n",
    "        improvement = nb - max(treat_all_nb, 0)\n",
    "        print(f\"  {clf}: NB = {nb:.4f} (improvement: {improvement:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 5: Running the Pipeline on Your Own PLR Data\n",
    "\n",
    "> **Important:** This section is for researchers who have their own PLR measurements\n",
    "> and want to use our exact pipeline.\n",
    "\n",
    "## 5.1 Data Requirements\n",
    "\n",
    "Your PLR data must be in a specific format:\n",
    "\n",
    "### Required Columns\n",
    "\n",
    "| Column | Type | Description | Example |\n",
    "|--------|------|-------------|--------|\n",
    "| `subject_code` | str | Unique subject ID | \"SUBJ001\" |\n",
    "| `time` | float | Time in seconds | 0.0, 0.033, 0.067... |\n",
    "| `pupil_raw` | float | Raw pupil size (mm) | 4.52, 4.48, 4.31... |\n",
    "| `Red` | int | Red light on (1) or off (0) | 0, 0, 1, 1, 1, 0... |\n",
    "| `Blue` | int | Blue light on (1) or off (0) | 0, 0, 0, 0, 1, 1... |\n",
    "| `class_label` | int | 0=control, 1=glaucoma | 0, 1 |\n",
    "\n",
    "### Optional Columns (if available)\n",
    "\n",
    "| Column | Description |\n",
    "|--------|--------------|\n",
    "| `pupil_gt` | Ground truth denoised signal |\n",
    "| `denoised` | CEEMD-denoised signal |\n",
    "| `age` | Subject age |\n",
    "| `sex` | Subject sex |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Expected PLR Recording Protocol\n",
    "\n",
    "Our pipeline expects a specific recording protocol:\n",
    "\n",
    "```\n",
    "Timeline (seconds):\n",
    "\n",
    "0     5      20     25     40     45     60     65\n",
    "|-----|------|------|------|------|------|------|---->\n",
    "  ^       ^          ^          ^          ^         \n",
    "  |       |          |          |          |         \n",
    "Baseline Red ON   Red OFF   Blue ON   Blue OFF      \n",
    "         (15s)     (5s)     (15s)     (recovery)\n",
    "```\n",
    "\n",
    "- **Sampling rate**: 30 Hz (30 samples per second)\n",
    "- **Total duration**: ~66 seconds (1981 samples)\n",
    "- **Light stimuli**: Red (620nm) and Blue (470nm)\n",
    "- **Stimulus duration**: ~15 seconds each\n",
    "\n",
    "> **What if my protocol is different?**\n",
    ">\n",
    "> You'll need to modify the feature extraction windows (see Part 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Prepare Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating properly formatted data\n",
    "\n",
    "def prepare_plr_for_pipeline(your_data_df):\n",
    "    \"\"\"\n",
    "    Convert your PLR data to the format expected by our pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    your_data_df : pd.DataFrame\n",
    "        Your data with columns: subject_id, timestamp_ms, pupil_diameter, \n",
    "        red_light_on, blue_light_on, has_glaucoma\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Data formatted for the pipeline\n",
    "    \"\"\"\n",
    "    # Rename columns to expected names\n",
    "    column_map = {\n",
    "        'subject_id': 'subject_code',\n",
    "        'timestamp_ms': 'time',  # Will convert to seconds\n",
    "        'pupil_diameter': 'pupil_raw',\n",
    "        'red_light_on': 'Red',\n",
    "        'blue_light_on': 'Blue',\n",
    "        'has_glaucoma': 'class_label'\n",
    "    }\n",
    "    \n",
    "    df = your_data_df.rename(columns=column_map).copy()\n",
    "    \n",
    "    # Convert time to seconds if needed\n",
    "    if df['time'].max() > 100:  # Probably in milliseconds\n",
    "        df['time'] = df['time'] / 1000\n",
    "    \n",
    "    # Ensure binary columns are integers\n",
    "    df['Red'] = df['Red'].astype(int)\n",
    "    df['Blue'] = df['Blue'].astype(int)\n",
    "    df['class_label'] = df['class_label'].astype(int)\n",
    "    \n",
    "    # Validate\n",
    "    assert 'subject_code' in df.columns\n",
    "    assert 'time' in df.columns\n",
    "    assert 'pupil_raw' in df.columns\n",
    "    assert 'Red' in df.columns\n",
    "    assert 'Blue' in df.columns\n",
    "    assert 'class_label' in df.columns\n",
    "    \n",
    "    print(f\"Data prepared: {df['subject_code'].nunique()} subjects\")\n",
    "    print(f\"Time range: {df['time'].min():.2f} to {df['time'].max():.2f} seconds\")\n",
    "    print(f\"Samples per subject: ~{len(df) // df['subject_code'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage (with synthetic data)\n",
    "print(\"Example data preparation:\")\n",
    "print(\"--\" * 30)\n",
    "\n",
    "# Create synthetic example\n",
    "n_subjects = 3\n",
    "n_samples = 1981  # Our standard length\n",
    "\n",
    "example_data = []\n",
    "for subj in range(n_subjects):\n",
    "    for t in range(n_samples):\n",
    "        time_s = t / 30  # 30 Hz\n",
    "        example_data.append({\n",
    "            'subject_id': f'SUBJ{subj:03d}',\n",
    "            'timestamp_ms': t * (1000/30),\n",
    "            'pupil_diameter': 4.5 + np.random.randn() * 0.3,\n",
    "            'red_light_on': 1 if 5 < time_s < 20 else 0,\n",
    "            'blue_light_on': 1 if 25 < time_s < 40 else 0,\n",
    "            'has_glaucoma': subj % 2  # Alternating labels\n",
    "        })\n",
    "\n",
    "example_df = pd.DataFrame(example_data)\n",
    "prepared_df = prepare_plr_for_pipeline(example_df)\n",
    "print(f\"\\nFirst few rows:\\n{prepared_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Save to DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plr_to_duckdb(df, output_path):\n",
    "    \"\"\"\n",
    "    Save prepared PLR data to DuckDB format for the pipeline.\n",
    "    \"\"\"\n",
    "    import duckdb\n",
    "    \n",
    "    con = duckdb.connect(str(output_path))\n",
    "    \n",
    "    # Create table\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS plr_recordings (\n",
    "            subject_code VARCHAR,\n",
    "            time FLOAT,\n",
    "            pupil_raw FLOAT,\n",
    "            Red INTEGER,\n",
    "            Blue INTEGER,\n",
    "            class_label INTEGER,\n",
    "            pupil_gt FLOAT,  -- Optional\n",
    "            denoised FLOAT   -- Optional\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert data\n",
    "    con.register('df', df)\n",
    "    con.execute(\"INSERT INTO plr_recordings SELECT * FROM df\")\n",
    "    \n",
    "    # Verify\n",
    "    count = con.execute(\"SELECT COUNT(*) FROM plr_recordings\").fetchone()[0]\n",
    "    print(f\"Saved {count} rows to {output_path}\")\n",
    "    \n",
    "    con.close()\n",
    "\n",
    "# Example (commented out to avoid creating files)\n",
    "# save_plr_to_duckdb(prepared_df, 'my_plr_data.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Run the Pipeline\n",
    "\n",
    "Once your data is in DuckDB format:\n",
    "\n",
    "```bash\n",
    "# 1. Clone the repository\n",
    "git clone https://github.com/YOUR_REPO/foundation-PLR.git\n",
    "cd foundation-PLR\n",
    "\n",
    "# 2. Install dependencies\n",
    "uv venv --python 3.11\n",
    "uv sync\n",
    "\n",
    "# 3. Update config to point to your data\n",
    "# Edit configs/defaults.yaml:\n",
    "#   DATA:\n",
    "#     filename_DuckDB: 'my_plr_data.db'\n",
    "\n",
    "# 4. Run the pipeline\n",
    "python src/pipeline_PLR.py\n",
    "\n",
    "# 5. View results in MLflow\n",
    "mlflow ui --port 5000\n",
    "# Open http://localhost:5000 in browser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 6: Customizing the Featurization (BINS)\n",
    "\n",
    "> **ELI5 for PIs:**\n",
    ">\n",
    "> \"Featurization\" means extracting meaningful numbers from the pupil curves.\n",
    "> Instead of using all 1981 data points, we calculate things like:\n",
    "> - \"Maximum constriction was 2.3mm\" \n",
    "> - \"Time to maximum constriction was 0.8 seconds\"\n",
    "> - \"Post-illumination pupil response (PIPR) was 15% below baseline\"\n",
    ">\n",
    "> The \"bins\" define WHAT to measure and WHEN (time windows).\n",
    "\n",
    "## 6.1 How Our Features Are Defined\n",
    "\n",
    "Each feature is defined by:\n",
    "\n",
    "| Parameter | What it means | Options |\n",
    "|-----------|--------------|--------|\n",
    "| `time_from` | Reference point | `onset` (light turns on) or `offset` (light turns off) |\n",
    "| `time_start` | Window start (seconds relative to reference) | Any number (negative = before) |\n",
    "| `time_end` | Window end (seconds relative to reference) | Any number |\n",
    "| `measure` | What to extract | `amplitude` (pupil size) or `timing` (latency) |\n",
    "| `stat` | How to summarize | `min`, `max`, `mean`, `median`, `AUC` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we used (featuresSimple.yaml)\n",
    "\n",
    "our_features = {\n",
    "    'BASELINE': {\n",
    "        'description': 'Pupil size before light stimulus',\n",
    "        'time_from': 'onset',\n",
    "        'time_start': -5,   # 5 seconds BEFORE light onset\n",
    "        'time_end': 0,      # Up to light onset\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'median'\n",
    "    },\n",
    "    'MAX_CONSTRICTION': {\n",
    "        'description': 'Maximum pupil constriction during light',\n",
    "        'time_from': 'onset',\n",
    "        'time_start': 0,    # From light onset\n",
    "        'time_end': 15,     # To 15 seconds after\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'min'       # Minimum = maximum constriction\n",
    "    },\n",
    "    'PHASIC': {\n",
    "        'description': 'Initial rapid constriction (first 5 seconds)',\n",
    "        'time_from': 'onset',\n",
    "        'time_start': 0,\n",
    "        'time_end': 5,\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'min'\n",
    "    },\n",
    "    'SUSTAINED': {\n",
    "        'description': 'Sustained constriction (last 5 seconds of light)',\n",
    "        'time_from': 'offset',\n",
    "        'time_start': -5,   # 5 seconds BEFORE light offset\n",
    "        'time_end': 0,\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'min'\n",
    "    },\n",
    "    'PIPR': {\n",
    "        'description': 'Post-Illumination Pupil Response (after light off)',\n",
    "        'time_from': 'offset',\n",
    "        'time_start': 0,    # From light offset\n",
    "        'time_end': 15,     # To 15 seconds after\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'min'\n",
    "    },\n",
    "    'PIPR_AUC': {\n",
    "        'description': 'Area under PIPR curve',\n",
    "        'time_from': 'offset',\n",
    "        'time_start': 0,\n",
    "        'time_end': 12,\n",
    "        'measure': 'amplitude',\n",
    "        'stat': 'AUC'\n",
    "    },\n",
    "    'LATENCY': {\n",
    "        'description': 'Time to reach maximum constriction',\n",
    "        'time_from': 'onset',\n",
    "        'time_start': 0,\n",
    "        'time_end': 5,\n",
    "        'measure': 'timing',  # Note: timing, not amplitude\n",
    "        'stat': 'min'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Our feature definitions:\")\n",
    "print(\"=\" * 70)\n",
    "for name, params in our_features.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {params['description']}\")\n",
    "    print(f\"  Window: {params['time_start']}s to {params['time_end']}s relative to {params['time_from']}\")\n",
    "    print(f\"  Measure: {params['measure']} with {params['stat']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Creating Custom Features\n",
    "\n",
    "> **When would you modify features?**\n",
    ">\n",
    "> 1. Your recording protocol has different timing (e.g., 10s light instead of 15s)\n",
    "> 2. You want to test new hypotheses (e.g., \"early vs late PIPR\")\n",
    "> 3. Your pupillometer has different sampling rate\n",
    "> 4. You're studying a different disease with different expected responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom feature config\n",
    "\n",
    "custom_features_yaml = \"\"\"\n",
    "# Save this as configs/PLR_FEATURIZATION/featuresCustom.yaml\n",
    "\n",
    "FEATURES_METADATA:\n",
    "  name: 'custom'\n",
    "  version: 1.0\n",
    "  feature_method: 'handcrafted_features'\n",
    "  \n",
    "FEATURES:\n",
    "  # Standard baseline\n",
    "  BASELINE:\n",
    "    time_from: 'onset'\n",
    "    time_start: -3      # Only 3 seconds (shorter protocol)\n",
    "    time_end: 0\n",
    "    measure: 'amplitude'\n",
    "    stat: 'median'\n",
    "    \n",
    "  # Your custom feature: very early constriction\n",
    "  EARLY_CONSTRICTION:\n",
    "    time_from: 'onset'\n",
    "    time_start: 0\n",
    "    time_end: 1         # Just first second!\n",
    "    measure: 'amplitude'\n",
    "    stat: 'min'\n",
    "    \n",
    "  # Your custom feature: late sustained response  \n",
    "  LATE_SUSTAINED:\n",
    "    time_from: 'onset'\n",
    "    time_start: 8       # 8-10 seconds after onset\n",
    "    time_end: 10\n",
    "    measure: 'amplitude'\n",
    "    stat: 'mean'\n",
    "    \n",
    "  # Your custom feature: early vs late PIPR ratio\n",
    "  PIPR_EARLY:\n",
    "    time_from: 'offset'\n",
    "    time_start: 0\n",
    "    time_end: 5         # First 5 seconds after light off\n",
    "    measure: 'amplitude'\n",
    "    stat: 'mean'\n",
    "    \n",
    "  PIPR_LATE:\n",
    "    time_from: 'offset'\n",
    "    time_start: 5\n",
    "    time_end: 15        # 5-15 seconds after light off\n",
    "    measure: 'amplitude'\n",
    "    stat: 'mean'\n",
    "\"\"\"\n",
    "\n",
    "print(custom_features_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Visualizing Feature Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the feature windows mean on a PLR curve\n",
    "\n",
    "def plot_feature_windows(features_dict):\n",
    "    \"\"\"\n",
    "    Create a visual diagram showing feature time windows on a PLR curve.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Simulate a PLR curve\n",
    "    t = np.linspace(0, 66, 1981)\n",
    "    \n",
    "    # Baseline\n",
    "    pupil = np.ones_like(t) * 5.0\n",
    "    \n",
    "    # Light onset at t=5, offset at t=20 (Red)\n",
    "    red_on = (t >= 5) & (t <= 20)\n",
    "    pupil[red_on] = 5.0 - 1.5 * (1 - np.exp(-(t[red_on] - 5) / 0.5))\n",
    "    \n",
    "    # Recovery after red\n",
    "    red_recovery = (t > 20) & (t <= 40)\n",
    "    pupil[red_recovery] = 3.5 + 1.2 * (1 - np.exp(-(t[red_recovery] - 20) / 3))\n",
    "    \n",
    "    # Blue onset at t=40, offset at t=55\n",
    "    blue_on = (t >= 40) & (t <= 55)\n",
    "    pupil[blue_on] = 4.7 - 1.8 * (1 - np.exp(-(t[blue_on] - 40) / 0.5))\n",
    "    \n",
    "    # PIPR after blue (slower recovery - key glaucoma marker)\n",
    "    pipr = t > 55\n",
    "    pupil[pipr] = 2.9 + 1.3 * (1 - np.exp(-(t[pipr] - 55) / 8))\n",
    "    \n",
    "    # Plot pupil\n",
    "    ax.plot(t, pupil, 'k-', linewidth=2, label='Pupil diameter')\n",
    "    \n",
    "    # Plot light stimuli\n",
    "    ax.fill_between(t, 0, 1, where=red_on, alpha=0.3, color='red', \n",
    "                    label='Red light', transform=ax.get_xaxis_transform())\n",
    "    ax.fill_between(t, 0, 1, where=blue_on, alpha=0.3, color='blue',\n",
    "                    label='Blue light', transform=ax.get_xaxis_transform())\n",
    "    \n",
    "    # Plot feature windows\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(features_dict)))\n",
    "    y_offset = 1.5  # Start position for annotations\n",
    "    \n",
    "    # Reference points\n",
    "    red_onset = 5\n",
    "    red_offset = 20\n",
    "    blue_onset = 40\n",
    "    blue_offset = 55\n",
    "    \n",
    "    for i, (name, params) in enumerate(features_dict.items()):\n",
    "        # Calculate absolute times (using Blue stimulus as example)\n",
    "        if params['time_from'] == 'onset':\n",
    "            ref = blue_onset\n",
    "        else:\n",
    "            ref = blue_offset\n",
    "            \n",
    "        t_start = ref + params['time_start']\n",
    "        t_end = ref + params['time_end']\n",
    "        \n",
    "        # Draw bracket\n",
    "        y_pos = y_offset + i * 0.25\n",
    "        ax.annotate('', xy=(t_start, y_pos), xytext=(t_end, y_pos),\n",
    "                   arrowprops=dict(arrowstyle='<->', color=colors[i], lw=2))\n",
    "        ax.text((t_start + t_end) / 2, y_pos + 0.1, name, \n",
    "               ha='center', va='bottom', fontsize=8, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Pupil diameter (mm)', fontsize=12)\n",
    "    ax.set_title('Feature Extraction Windows on PLR Curve\\n(Blue stimulus shown)', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlim(0, 66)\n",
    "    ax.set_ylim(1, 6)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot our features\n",
    "plot_feature_windows(our_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Extracting Features from Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(pupil, time, light_on, feature_params):\n",
    "    \"\"\"\n",
    "    Extract a single feature from PLR data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pupil : np.array\n",
    "        Pupil diameter time series\n",
    "    time : np.array\n",
    "        Time in seconds\n",
    "    light_on : np.array\n",
    "        Binary array (1 when light on, 0 when off)\n",
    "    feature_params : dict\n",
    "        Feature definition\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Feature value and statistics\n",
    "    \"\"\"\n",
    "    # Find light onset and offset\n",
    "    light_changes = np.diff(light_on)\n",
    "    onset_idx = np.where(light_changes == 1)[0]\n",
    "    offset_idx = np.where(light_changes == -1)[0]\n",
    "    \n",
    "    if len(onset_idx) == 0 or len(offset_idx) == 0:\n",
    "        return {'value': np.nan, 'error': 'Light stimulus not found'}\n",
    "    \n",
    "    onset_time = time[onset_idx[0] + 1]\n",
    "    offset_time = time[offset_idx[0] + 1]\n",
    "    \n",
    "    # Calculate absolute window\n",
    "    if feature_params['time_from'] == 'onset':\n",
    "        ref_time = onset_time\n",
    "    else:\n",
    "        ref_time = offset_time\n",
    "    \n",
    "    t_start = ref_time + feature_params['time_start']\n",
    "    t_end = ref_time + feature_params['time_end']\n",
    "    \n",
    "    # Extract samples in window\n",
    "    mask = (time >= t_start) & (time <= t_end)\n",
    "    window_pupil = pupil[mask]\n",
    "    window_time = time[mask]\n",
    "    \n",
    "    if len(window_pupil) == 0:\n",
    "        return {'value': np.nan, 'error': 'Empty window'}\n",
    "    \n",
    "    # Compute statistic\n",
    "    stat = feature_params['stat']\n",
    "    \n",
    "    if feature_params['measure'] == 'amplitude':\n",
    "        if stat == 'min':\n",
    "            value = np.min(window_pupil)\n",
    "        elif stat == 'max':\n",
    "            value = np.max(window_pupil)\n",
    "        elif stat == 'mean':\n",
    "            value = np.mean(window_pupil)\n",
    "        elif stat == 'median':\n",
    "            value = np.median(window_pupil)\n",
    "        elif stat == 'AUC':\n",
    "            value = np.trapz(window_pupil, window_time)\n",
    "            \n",
    "    elif feature_params['measure'] == 'timing':\n",
    "        # Time to minimum (latency)\n",
    "        min_idx = np.argmin(window_pupil)\n",
    "        value = window_time[min_idx] - ref_time\n",
    "    \n",
    "    return {\n",
    "        'value': value,\n",
    "        'std': np.std(window_pupil),\n",
    "        'n_samples': len(window_pupil)\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Extract features from synthetic data\n",
    "# (In practice, you'd use your actual PLR recordings)\n",
    "\n",
    "# Generate synthetic PLR\n",
    "t = np.linspace(0, 66, 1981)\n",
    "pupil = 5.0 * np.ones_like(t)\n",
    "\n",
    "# Blue light on at t=40, off at t=55\n",
    "blue = np.zeros_like(t, dtype=int)\n",
    "blue[(t >= 40) & (t <= 55)] = 1\n",
    "\n",
    "# Simulate pupil response\n",
    "for i, ti in enumerate(t):\n",
    "    if 40 <= ti <= 55:\n",
    "        pupil[i] = 5.0 - 1.8 * (1 - np.exp(-(ti - 40) / 0.5))\n",
    "    elif ti > 55:\n",
    "        pupil[i] = 3.2 + 1.0 * (1 - np.exp(-(ti - 55) / 5))\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracted features from synthetic PLR:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, params in our_features.items():\n",
    "    result = extract_feature(pupil, t, blue, params)\n",
    "    print(f\"{name}: {result['value']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 7: Quick Reference\n",
    "\n",
    "## SQL Queries Cheatsheet\n",
    "\n",
    "```sql\n",
    "-- Get all results for one classifier\n",
    "SELECT * FROM metrics_aggregate WHERE classifier = 'TabM';\n",
    "\n",
    "-- Get predictions for analysis\n",
    "SELECT y_true, y_prob FROM predictions WHERE classifier = 'TabM' AND fold = 0;\n",
    "\n",
    "-- Get bootstrap distribution for custom CI\n",
    "SELECT metric_value FROM bootstrap_distributions \n",
    "WHERE classifier = 'TabM' AND metric_name = 'auroc';\n",
    "\n",
    "-- Compare classifiers\n",
    "SELECT classifier, mean, ci_lower, ci_upper \n",
    "FROM metrics_aggregate WHERE metric_name = 'auroc' ORDER BY mean DESC;\n",
    "```\n",
    "\n",
    "## File Locations\n",
    "\n",
    "| File | Purpose |\n",
    "|------|--------|\n",
    "| `configs/defaults.yaml` | Main configuration |\n",
    "| `configs/PLR_FEATURIZATION/*.yaml` | Feature definitions |\n",
    "| `configs/CLS_MODELS/*.yaml` | Classifier settings |\n",
    "| `src/featurization/` | Feature extraction code |\n",
    "| `src/classification/` | Classifier training code |\n",
    "| `src/stats/` | Biostatistics modules |\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions about this data or pipeline:\n",
    "- GitHub Issues: [repository URL]\n",
    "- Email: [contact email]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "con.close()\n",
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
