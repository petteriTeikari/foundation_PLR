{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation PLR: Data Access Tutorial\n",
    "\n",
    "**Purpose**: Learn how to access, understand, and work with the shared research data.\n",
    "\n",
    "---\n",
    "\n",
    "> **ELI5 (Explain Like I'm 5) - What is this?**\n",
    ">\n",
    "> This study tested whether we can detect glaucoma from how the pupil reacts to light.\n",
    "> We collected pupil measurements from 63 people, extracted features (numbers describing the pupil response),\n",
    "> and trained computers (classifiers) to predict who has glaucoma.\n",
    ">\n",
    "> All the data is stored in **DuckDB** databases - think of them as super-efficient Excel files that can hold millions of rows.\n",
    "\n",
    "---\n",
    "\n",
    "## What Data is Available?\n",
    "\n",
    "| Database File | What's Inside | Size | Rows |\n",
    "|--------------|---------------|------|------|\n",
    "| `foundation_plr_results.db` | Classifier predictions and performance metrics | ~4 MB | 20,349 predictions |\n",
    "| `foundation_plr_distributions.db` | Bootstrap statistics and per-subject predictions | ~40 MB | 1.3M+ rows |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup - Install Required Libraries\n",
    "\n",
    "You only need **duckdb** to access the data. Optional libraries for data science work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install required libraries (only needed once)\n",
    "# Remove the # to uncomment and run\n",
    "\n",
    "# !pip install duckdb pandas numpy matplotlib\n",
    "# !pip install polars  # Optional: faster alternative to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Try importing Polars (optional)\n",
    "try:\n",
    "    import polars as pl\n",
    "    HAS_POLARS = True\n",
    "    print(\"✓ Polars available\")\n",
    "except ImportError:\n",
    "    HAS_POLARS = False\n",
    "    print(\"Polars not installed (optional)\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Connect to the Database\n",
    "\n",
    "> **ELI5 - What is DuckDB?**\n",
    ">\n",
    "> DuckDB is like SQLite but optimized for data analysis. It's a single file that contains\n",
    "> tables of data. You can query it using SQL (a language for asking questions about data)\n",
    "> or convert data to Pandas/Polars for Python analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data files\n",
    "# Adjust these paths to match where you saved the files\n",
    "DATA_DIR = Path(\"../outputs\")  # Or wherever you downloaded the files\n",
    "\n",
    "RESULTS_DB = DATA_DIR / \"foundation_plr_results.db\"\n",
    "DISTRIBUTIONS_DB = DATA_DIR / \"foundation_plr_distributions.db\"\n",
    "\n",
    "# Check if files exist\n",
    "print(\"Checking data files:\")\n",
    "print(f\"  Results DB: {'✓ Found' if RESULTS_DB.exists() else '✗ Not found'} ({RESULTS_DB})\")\n",
    "print(f\"  Distributions DB: {'✓ Found' if DISTRIBUTIONS_DB.exists() else '✗ Not found'} ({DISTRIBUTIONS_DB})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the results database (read-only to prevent accidental changes)\n",
    "con = duckdb.connect(str(RESULTS_DB), read_only=True)\n",
    "\n",
    "# See what tables are available\n",
    "tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables in results database:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding the Data: Table Schemas\n",
    "\n",
    "Let's look at exactly what each table contains. **Every column is explained below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The `predictions` Table\n",
    "\n",
    "> **ELI5**: Each row is one prediction - \"for this person, this classifier predicted they have/don't have glaucoma\".\n",
    "> The table stores both the prediction AND the actual answer (ground truth) so we can measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema of predictions table\n",
    "print(\"PREDICTIONS TABLE SCHEMA\")\n",
    "print(\"=\" * 70)\n",
    "schema = con.execute(\"DESCRIBE predictions\").fetchdf()\n",
    "print(schema.to_string(index=False))\n",
    "\n",
    "# Count rows\n",
    "count = con.execute(\"SELECT COUNT(*) FROM predictions\").fetchone()[0]\n",
    "print(f\"\\nTotal rows: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Explanations for `predictions`:\n",
    "\n",
    "| Column | Type | Description | Example Values |\n",
    "|--------|------|-------------|----------------|\n",
    "| `prediction_id` | INTEGER | Unique ID for each prediction | 1, 2, 3, ... |\n",
    "| `subject_id` | VARCHAR | Anonymous patient identifier | \"PLR1001\", \"PLR1002\" |\n",
    "| `eye` | VARCHAR | Which eye was measured | \"OD\" (right), \"OS\" (left) |\n",
    "| `fold` | INTEGER | Cross-validation fold number | 0, 1, 2, 3, 4 |\n",
    "| `bootstrap_iter` | INTEGER | Bootstrap iteration (0 = original) | 0, 1, 2, ... |\n",
    "| `outlier_method` | VARCHAR | How outliers were detected | \"ensemble-LOF-...\" |\n",
    "| `imputation_method` | VARCHAR | How missing data was filled | \"SAITS\" |\n",
    "| `featurization` | VARCHAR | Feature extraction method | \"simple1.0\" |\n",
    "| `classifier` | VARCHAR | ML algorithm used | \"TabM\", \"XGBOOST\", \"LogisticRegression\", \"TabPFN\" |\n",
    "| `source_name` | VARCHAR | Full pipeline config string | \"XGBOOST_eval-auc__...\" |\n",
    "| `y_true` | INTEGER | **Ground truth**: Does patient have glaucoma? | 0 (no), 1 (yes) |\n",
    "| `y_pred` | INTEGER | **Prediction**: Classifier's binary decision | 0 (no), 1 (yes) |\n",
    "| `y_prob` | FLOAT | **Probability**: Classifier's confidence | 0.0 to 1.0 |\n",
    "| `mlflow_run_id` | VARCHAR | Experiment tracking ID | \"abc123...\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example data\n",
    "print(\"\\nExample rows from predictions table:\")\n",
    "print(con.execute(\"SELECT * FROM predictions LIMIT 3\").fetchdf().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The `metrics_per_fold` Table\n",
    "\n",
    "> **ELI5**: Performance metrics (like accuracy) calculated separately for each cross-validation fold.\n",
    "> Cross-validation means we split the data 5 ways and test 5 times to get robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"METRICS_PER_FOLD TABLE SCHEMA\")\n",
    "print(\"=\" * 70)\n",
    "schema = con.execute(\"DESCRIBE metrics_per_fold\").fetchdf()\n",
    "print(schema.to_string(index=False))\n",
    "\n",
    "count = con.execute(\"SELECT COUNT(*) FROM metrics_per_fold\").fetchone()[0]\n",
    "print(f\"\\nTotal rows: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Explanations for `metrics_per_fold`:\n",
    "\n",
    "| Column | Type | Description | Ideal Value |\n",
    "|--------|------|-------------|-------------|\n",
    "| `metric_id` | INTEGER | Unique row ID | - |\n",
    "| `classifier` | VARCHAR | ML algorithm | - |\n",
    "| `fold` | INTEGER | CV fold (0-4) | - |\n",
    "| `metric_name` | VARCHAR | Which metric | \"auroc\", \"auprc\", etc. |\n",
    "| `metric_value` | FLOAT | The measured value | Depends on metric |\n",
    "| `bootstrap_iter` | INTEGER | Bootstrap iteration | - |\n",
    "| `source_name` | VARCHAR | Pipeline config | - |\n",
    "\n",
    "**Common Metrics:**\n",
    "- `auroc`: Area Under ROC Curve (0-1, higher=better, 0.5=random, 1.0=perfect)\n",
    "- `auprc`: Area Under Precision-Recall Curve (higher=better)\n",
    "- `brier`: Brier Score (0-1, lower=better, measures calibration)\n",
    "- `accuracy`: Correct predictions / Total predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The `metrics_aggregate` Table\n",
    "\n",
    "> **ELI5**: Summary statistics (mean, median, confidence intervals) calculated across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"METRICS_AGGREGATE TABLE SCHEMA\")\n",
    "print(\"=\" * 70)\n",
    "schema = con.execute(\"DESCRIBE metrics_aggregate\").fetchdf()\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Explanations for `metrics_aggregate`:\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `aggregate_id` | Unique row ID |\n",
    "| `classifier` | ML algorithm name |\n",
    "| `metric_name` | Metric being summarized |\n",
    "| `mean` | Average across folds |\n",
    "| `std` | Standard deviation |\n",
    "| `median` | Middle value |\n",
    "| `q25` | 25th percentile |\n",
    "| `q75` | 75th percentile |\n",
    "| `ci_lower` | 95% CI lower bound |\n",
    "| `ci_upper` | 95% CI upper bound |\n",
    "| `source_name` | Pipeline config |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The `mlflow_runs` Table (if present)\n",
    "\n",
    "> **ELI5**: Metadata about each experiment run - when it ran, what settings were used, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if mlflow_runs exists\n",
    "tables = [t[0] for t in con.execute(\"SHOW TABLES\").fetchall()]\n",
    "if 'mlflow_runs' in tables:\n",
    "    print(\"MLFLOW_RUNS TABLE SCHEMA\")\n",
    "    print(\"=\" * 70)\n",
    "    schema = con.execute(\"DESCRIBE mlflow_runs\").fetchdf()\n",
    "    print(schema.to_string(index=False))\n",
    "else:\n",
    "    print(\"mlflow_runs table not present in this database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Querying Data: SQL Basics\n",
    "\n",
    "> **ELI5 - What is SQL?**\n",
    ">\n",
    "> SQL (Structured Query Language) is how you ask questions to a database.\n",
    "> - `SELECT` = which columns you want\n",
    "> - `FROM` = which table\n",
    "> - `WHERE` = filter conditions\n",
    "> - `GROUP BY` = aggregate by category\n",
    "> - `ORDER BY` = sort results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Get AUROC for each classifier\n",
    "print(\"AUROC by Classifier:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    classifier,\n",
    "    ROUND(mean, 4) as auroc_mean,\n",
    "    ROUND(ci_lower, 4) as ci_lower,\n",
    "    ROUND(ci_upper, 4) as ci_upper\n",
    "FROM metrics_aggregate\n",
    "WHERE metric_name = 'auroc'\n",
    "ORDER BY mean DESC\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Count predictions per classifier\n",
    "print(\"\\nPredictions per Classifier:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    classifier,\n",
    "    COUNT(*) as n_predictions,\n",
    "    COUNT(DISTINCT subject_id) as n_subjects,\n",
    "    ROUND(AVG(y_true), 3) as glaucoma_prevalence\n",
    "FROM predictions\n",
    "GROUP BY classifier\n",
    "ORDER BY classifier\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Get predictions for a specific subject\n",
    "print(\"\\nPredictions for first subject:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# First, find a subject ID\n",
    "first_subject = con.execute(\"SELECT DISTINCT subject_id FROM predictions LIMIT 1\").fetchone()[0]\n",
    "print(f\"Subject: {first_subject}\\n\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    classifier,\n",
    "    eye,\n",
    "    y_true as has_glaucoma,\n",
    "    ROUND(y_prob, 3) as predicted_probability,\n",
    "    y_pred as predicted_class\n",
    "FROM predictions\n",
    "WHERE subject_id = '{first_subject}'\n",
    "  AND fold = 0  -- Just one fold to keep it simple\n",
    "ORDER BY classifier\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run Statistics Directly in DuckDB (No Pandas Needed!)\n",
    "\n",
    "> **ELI5**: DuckDB can do math and statistics directly - you don't need to load data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics directly in DuckDB\n",
    "print(\"Statistics computed directly in DuckDB:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    classifier,\n",
    "    -- Basic stats\n",
    "    COUNT(*) as n,\n",
    "    ROUND(AVG(y_prob), 4) as mean_prob,\n",
    "    ROUND(STDDEV(y_prob), 4) as std_prob,\n",
    "    \n",
    "    -- Percentiles\n",
    "    ROUND(PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY y_prob), 4) as q25,\n",
    "    ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY y_prob), 4) as median,\n",
    "    ROUND(PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY y_prob), 4) as q75,\n",
    "    \n",
    "    -- Min/Max\n",
    "    ROUND(MIN(y_prob), 4) as min_prob,\n",
    "    ROUND(MAX(y_prob), 4) as max_prob\n",
    "FROM predictions\n",
    "WHERE fold = 0  -- Use first fold for cleaner stats\n",
    "GROUP BY classifier\n",
    "ORDER BY classifier\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix metrics in DuckDB\n",
    "print(\"\\nConfusion Matrix Metrics (at threshold=0.5):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query = \"\"\"\n",
    "WITH confusion AS (\n",
    "    SELECT \n",
    "        classifier,\n",
    "        SUM(CASE WHEN y_true = 1 AND y_pred = 1 THEN 1 ELSE 0 END) as TP,\n",
    "        SUM(CASE WHEN y_true = 0 AND y_pred = 1 THEN 1 ELSE 0 END) as FP,\n",
    "        SUM(CASE WHEN y_true = 1 AND y_pred = 0 THEN 1 ELSE 0 END) as FN,\n",
    "        SUM(CASE WHEN y_true = 0 AND y_pred = 0 THEN 1 ELSE 0 END) as TN\n",
    "    FROM predictions\n",
    "    WHERE fold = 0\n",
    "    GROUP BY classifier\n",
    ")\n",
    "SELECT \n",
    "    classifier,\n",
    "    TP, FP, FN, TN,\n",
    "    ROUND(CAST(TP AS FLOAT) / (TP + FN), 3) as Sensitivity,\n",
    "    ROUND(CAST(TN AS FLOAT) / (TN + FP), 3) as Specificity,\n",
    "    ROUND(CAST(TP + TN AS FLOAT) / (TP + TN + FP + FN), 3) as Accuracy\n",
    "FROM confusion\n",
    "ORDER BY classifier\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Converting to DataFrames\n",
    "\n",
    "Sometimes you need data in Python for visualization or custom analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Convert to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using .fetchdf() - returns Pandas DataFrame\n",
    "query = \"SELECT * FROM predictions WHERE classifier = 'TabM' AND fold = 0\"\n",
    "df_pandas = con.execute(query).fetchdf()\n",
    "\n",
    "print(f\"Type: {type(df_pandas)}\")\n",
    "print(f\"Shape: {df_pandas.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_pandas.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using pd.read_sql (if you prefer this syntax)\n",
    "# This opens a separate connection\n",
    "df_pandas2 = pd.read_sql(\n",
    "    \"SELECT classifier, AVG(y_prob) as mean_prob FROM predictions GROUP BY classifier\",\n",
    "    duckdb.connect(str(RESULTS_DB), read_only=True)\n",
    ")\n",
    "print(df_pandas2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Convert to Polars DataFrame (Faster for Large Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_POLARS:\n",
    "    # Method 1: Using .pl() method (DuckDB >= 0.8.0)\n",
    "    try:\n",
    "        df_polars = con.execute(\n",
    "            \"SELECT * FROM predictions WHERE classifier = 'TabM' AND fold = 0\"\n",
    "        ).pl()\n",
    "        print(f\"Type: {type(df_polars)}\")\n",
    "        print(f\"Shape: {df_polars.shape}\")\n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(df_polars.head(3))\n",
    "    except AttributeError:\n",
    "        # Fallback: Convert via Arrow\n",
    "        arrow_table = con.execute(\n",
    "            \"SELECT * FROM predictions WHERE classifier = 'TabM' AND fold = 0\"\n",
    "        ).arrow()\n",
    "        df_polars = pl.from_arrow(arrow_table)\n",
    "        print(f\"Type: {type(df_polars)}\")\n",
    "        print(f\"Shape: {df_polars.shape}\")\n",
    "else:\n",
    "    print(\"Polars not installed. Install with: pip install polars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Converting to NumPy Arrays\n",
    "\n",
    "> **ELI5**: NumPy arrays are the basic data structure for numerical computing in Python.\n",
    "> Machine learning libraries like scikit-learn work with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single column as NumPy array\n",
    "y_probs = con.execute(\n",
    "    \"SELECT y_prob FROM predictions WHERE classifier = 'TabM' AND fold = 0\"\n",
    ").fetchnumpy()['y_prob']\n",
    "\n",
    "print(f\"Type: {type(y_probs)}\")\n",
    "print(f\"Shape: {y_probs.shape}\")\n",
    "print(f\"Dtype: {y_probs.dtype}\")\n",
    "print(f\"First 5 values: {y_probs[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract multiple columns as NumPy arrays\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT y_true, y_prob, y_pred \n",
    "    FROM predictions \n",
    "    WHERE classifier = 'TabM' AND fold = 0\n",
    "\"\"\").fetchnumpy()\n",
    "\n",
    "y_true = result['y_true']\n",
    "y_prob = result['y_prob']\n",
    "y_pred = result['y_pred']\n",
    "\n",
    "print(f\"y_true shape: {y_true.shape}, dtype: {y_true.dtype}\")\n",
    "print(f\"y_prob shape: {y_prob.shape}, dtype: {y_prob.dtype}\")\n",
    "print(f\"y_pred shape: {y_pred.shape}, dtype: {y_pred.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use these arrays with scikit-learn!\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"Metrics computed from NumPy arrays:\")\n",
    "print(f\"  AUROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Working with the Distributions Database\n",
    "\n",
    "> **ELI5 - What are bootstrap distributions?**\n",
    ">\n",
    "> Bootstrap is a statistical technique where we resample the data many times (e.g., 1000 times)\n",
    "> and calculate a metric each time. This gives us a distribution of values, which we use to\n",
    "> estimate uncertainty (confidence intervals) instead of just a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to distributions database\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    con_dist = duckdb.connect(str(DISTRIBUTIONS_DB), read_only=True)\n",
    "    \n",
    "    print(\"Tables in distributions database:\")\n",
    "    for table in con_dist.execute(\"SHOW TABLES\").fetchall():\n",
    "        count = con_dist.execute(f\"SELECT COUNT(*) FROM {table[0]}\").fetchone()[0]\n",
    "        print(f\"  - {table[0]}: {count:,} rows\")\n",
    "else:\n",
    "    print(\"Distributions database not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore bootstrap_distributions table\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    print(\"\\nBOOTSTRAP_DISTRIBUTIONS TABLE SCHEMA\")\n",
    "    print(\"=\" * 70)\n",
    "    schema = con_dist.execute(\"DESCRIBE bootstrap_distributions\").fetchdf()\n",
    "    print(schema.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nExample rows:\")\n",
    "    print(con_dist.execute(\"SELECT * FROM bootstrap_distributions LIMIT 3\").fetchdf().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Explanations for `bootstrap_distributions`:\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `dist_id` | Unique row ID |\n",
    "| `classifier` | ML algorithm |\n",
    "| `metric_name` | Metric being bootstrapped |\n",
    "| `fold` | Cross-validation fold |\n",
    "| `bootstrap_iter` | Bootstrap iteration (0 to N-1) |\n",
    "| `metric_value` | Metric value for this bootstrap sample |\n",
    "| `source_name` | Pipeline configuration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore subject_predictions table\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    print(\"\\nSUBJECT_PREDICTIONS TABLE SCHEMA\")\n",
    "    print(\"=\" * 70)\n",
    "    schema = con_dist.execute(\"DESCRIBE subject_predictions\").fetchdf()\n",
    "    print(schema.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nExample rows:\")\n",
    "    print(con_dist.execute(\"SELECT * FROM subject_predictions LIMIT 3\").fetchdf().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Explanations for `subject_predictions`:\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `pred_id` | Unique row ID |\n",
    "| `source_name` | Full pipeline config |\n",
    "| `classifier` | ML algorithm |\n",
    "| `split` | Data split (train/test) |\n",
    "| `subject_code` | Anonymous subject ID |\n",
    "| `y_true` | Ground truth (0=healthy, 1=glaucoma) |\n",
    "| `y_pred_proba` | Predicted probability |\n",
    "| `y_pred` | Binary prediction |\n",
    "| `confidence` | Prediction confidence (if available) |\n",
    "| `entropy_of_expected` | Uncertainty measure |\n",
    "| `expected_entropy` | Uncertainty measure |\n",
    "| `mutual_information` | Uncertainty measure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get bootstrap CI for AUROC\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    print(\"\\nBootstrap 95% CI for AUROC:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        classifier,\n",
    "        COUNT(*) as n_bootstraps,\n",
    "        ROUND(AVG(metric_value), 4) as mean_auroc,\n",
    "        ROUND(PERCENTILE_CONT(0.025) WITHIN GROUP (ORDER BY metric_value), 4) as ci_lower,\n",
    "        ROUND(PERCENTILE_CONT(0.975) WITHIN GROUP (ORDER BY metric_value), 4) as ci_upper\n",
    "    FROM bootstrap_distributions\n",
    "    WHERE metric_name = 'auroc'\n",
    "    GROUP BY classifier\n",
    "    ORDER BY mean_auroc DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    result = con_dist.execute(query).fetchdf()\n",
    "    print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training Your Own Classifier\n",
    "\n",
    "> **ELI5**: If you want to test a different classifier or settings, you can load the features\n",
    "> and train your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll use the predictions to demonstrate\n",
    "# In practice, you'd use the features database\n",
    "\n",
    "# Get unique subjects and their outcomes for one classifier\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT \n",
    "    subject_id,\n",
    "    eye,\n",
    "    y_true,\n",
    "    y_prob as original_prob\n",
    "FROM predictions\n",
    "WHERE classifier = 'TabM' AND fold = 0\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).fetchdf()\n",
    "print(f\"Subjects: {len(df)}\")\n",
    "print(f\"Class distribution: {df['y_true'].value_counts().to_dict()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple classifier (using prediction probabilities as features for demo)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Using the original probability as a single feature (just for demonstration)\n",
    "X_demo = df['original_prob'].values.reshape(-1, 1)\n",
    "y_demo = df['y_true'].values\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(random_state=42)\n",
    "scores = cross_val_score(clf, X_demo, y_demo, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Cross-validation AUROC scores: {scores}\")\n",
    "print(f\"Mean AUROC: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Saving and Exporting Data\n",
    "\n",
    "DuckDB can export data to many formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT classifier, mean, ci_lower, ci_upper\n",
    "        FROM metrics_aggregate\n",
    "        WHERE metric_name = 'auroc'\n",
    "    ) TO 'auroc_results.csv' (HEADER, DELIMITER ',')\n",
    "\"\"\")\n",
    "print(\"✓ Exported to auroc_results.csv\")\n",
    "\n",
    "# Read it back to verify\n",
    "print(\"\\nContents:\")\n",
    "print(open('auroc_results.csv').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet (efficient columnar format)\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM predictions WHERE classifier = 'TabM'\n",
    "    ) TO 'tabm_predictions.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "print(\"✓ Exported to tabm_predictions.parquet\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size_mb = os.path.getsize('tabm_predictions.parquet') / (1024 * 1024)\n",
    "print(f\"File size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up exported files\n",
    "import os\n",
    "for f in ['auroc_results.csv', 'tabm_predictions.parquet']:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Cleaned up: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Quick Reference\n",
    "\n",
    "### Common SQL Queries\n",
    "\n",
    "```sql\n",
    "-- Get all metrics for a classifier\n",
    "SELECT * FROM metrics_aggregate WHERE classifier = 'TabM';\n",
    "\n",
    "-- Get predictions for a specific subject\n",
    "SELECT * FROM predictions WHERE subject_id = 'PLR1001';\n",
    "\n",
    "-- Count unique subjects\n",
    "SELECT COUNT(DISTINCT subject_id) FROM predictions;\n",
    "\n",
    "-- Get class balance\n",
    "SELECT y_true, COUNT(*) FROM predictions GROUP BY y_true;\n",
    "\n",
    "-- Filter by multiple conditions\n",
    "SELECT * FROM predictions \n",
    "WHERE classifier = 'TabM' \n",
    "  AND fold = 0 \n",
    "  AND y_prob > 0.5;\n",
    "```\n",
    "\n",
    "### Data Conversion Cheatsheet\n",
    "\n",
    "| Want | Code |\n",
    "|------|------|\n",
    "| Pandas DataFrame | `con.execute(query).fetchdf()` |\n",
    "| Polars DataFrame | `con.execute(query).pl()` |\n",
    "| NumPy arrays | `con.execute(query).fetchnumpy()` |\n",
    "| Python list | `con.execute(query).fetchall()` |\n",
    "| Arrow Table | `con.execute(query).arrow()` |\n",
    "\n",
    "### Database Files\n",
    "\n",
    "| File | Contents | Use Case |\n",
    "|------|----------|----------|\n",
    "| `foundation_plr_results.db` | Predictions, metrics | Main analysis |\n",
    "| `foundation_plr_distributions.db` | Bootstrap samples | Uncertainty analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections when done\n",
    "con.close()\n",
    "if DISTRIBUTIONS_DB.exists():\n",
    "    con_dist.close()\n",
    "print(\"✓ Connections closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Need Help?\n",
    "\n",
    "- **DuckDB Documentation**: https://duckdb.org/docs/\n",
    "- **SQL Tutorial**: https://www.w3schools.com/sql/\n",
    "- **Pandas Documentation**: https://pandas.pydata.org/docs/\n",
    "- **Questions about this data**: Contact the study authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
