{
  "hash": "4364220054faa18ed0f2f0caa30a67cf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Foundation PLR: Pipeline Walkthrough\"\nsubtitle: \"Understanding the end-to-end analysis pipeline\"\nauthor: \"Foundation PLR Team\"\ndate: today\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show code\"\n    toc: true\n    toc-depth: 3\n    number-sections: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  error: true\n---\n\n\n# Research Question {#sec-research-question}\n\nThis project investigates a focused question:\n\n> **How do preprocessing choices (outlier detection followed by imputation) affect\n> ALL STRATOS-compliant downstream metrics when using handcrafted physiological\n> features for glaucoma screening?**\n\nThe key design decision is to **fix the classifier** (CatBoost) and **vary the\npreprocessing pipeline**. We measure the downstream effect on discrimination,\ncalibration, and clinical utility -- not just AUROC.\n\n**What this study is NOT about:**\n\n- Comparing classifiers (CatBoost vs. XGBoost) -- irrelevant to the research question\n- Maximizing AUROC -- not the goal\n- Generic ML benchmarking -- misses the point\n\nThe pipeline has four stages, where stages 3 and 4 are held constant:\n\n\n```{mermaid}\nflowchart LR\n    A[\"1. Outlier Detection\\n(11 methods)\"] --> B[\"2. Imputation\\n(8 methods)\"]\n    B --> C[\"3. Featurization\\n(FIXED: handcrafted)\"]\n    C --> D[\"4. Classification\\n(FIXED: CatBoost)\"]\n    D --> E[\"STRATOS Metrics\\nAUROC, Calibration,\\nNet Benefit, DCA, Brier\"]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:1px\n    style D fill:#bbf,stroke:#333,stroke-width:1px\n    style E fill:#bfb,stroke:#333,stroke-width:1px\n```\n\n\nErrors at each preprocessing stage propagate downstream. A poor outlier detector\ncorrupts the signal, which degrades imputation quality, which distorts extracted\nfeatures, which ultimately biases classification metrics. This cascading effect\nis the core phenomenon we quantify.\n\n\n# The Data: Chromatic Pupillometry {#sec-data}\n\n## What is the Pupillary Light Reflex?\n\nThe **pupillary light reflex (PLR)** is the involuntary constriction and\nsubsequent dilation of the pupil in response to light. Think of it as the eye's\n\"auto-brightness\" mechanism. When a bright light hits your eye, the pupil\nshrinks to protect the retina; when the light goes away, the pupil slowly\nreturns to its resting size.\n\nIn **chromatic pupillometry**, we use precisely controlled colored lights to\nprobe different retinal pathways:\n\n- **Blue light (469 nm)** stimulates melanopsin-containing retinal ganglion\n  cells (ipRGCs). These cells are part of the non-image-forming visual system\n  and drive the **sustained** post-illumination pupil response (PIPR).\n- **Red light (640 nm)** primarily stimulates rods and cones and produces a\n  **transient** (phasic) response that recovers quickly.\n\nIn glaucoma, the retinal ganglion cells degenerate. Since ipRGCs are a subtype\nof ganglion cells, the melanopsin-driven sustained response to blue light is\npreferentially affected. This makes chromatic pupillometry a promising\nnon-invasive screening tool.\n\n## Stimulus Protocol\n\nThe recording protocol lasts approximately 60 seconds with alternating stimulus\nepochs:\n\n\n```{mermaid}\nsequenceDiagram\n    participant R as Recorder\n    participant E as Eye\n    participant L as Light Source\n\n    R->>E: Baseline (10s, darkness)\n    Note over E: Pupil at resting diameter\n\n    L->>E: Blue Light ON (469nm, 1s)\n    Note over E: Rapid constriction\n    L-->>E: Light OFF\n    Note over E: Sustained response (PIPR)\n    R->>E: Recovery period (20s)\n\n    L->>E: Red Light ON (640nm, 1s)\n    Note over E: Rapid constriction\n    L-->>E: Light OFF\n    Note over E: Quick recovery (no PIPR)\n    R->>E: Recovery period (20s)\n```\n\n\n## Handcrafted Features\n\nFrom each PLR recording, we extract **8 physiological features** that capture\nthe key dynamics of the pupil response:\n\n| Feature | Description | Clinical Relevance |\n|---------|-------------|--------------------|\n| Max constriction (blue) | Peak pupil narrowing after blue light | Overall pupillary reactivity |\n| Max constriction (red) | Peak pupil narrowing after red light | Cone/rod pathway integrity |\n| Phasic response (blue) | Rapid initial constriction amplitude | Inner retinal function |\n| Phasic response (red) | Rapid initial constriction amplitude | Photoreceptor function |\n| Sustained response (blue) | Late-phase constriction level | Melanopsin (ipRGC) function |\n| Sustained response (red) | Late-phase constriction level | Baseline comparison |\n| PIPR area (blue) | Post-illumination pupil response area | Key glaucoma biomarker |\n| PIPR area (red) | Post-illumination pupil response area | Non-melanopsin baseline |\n\n## Subject Counts\n\n| Task | N | Breakdown |\n|------|---|-----------|\n| Outlier Detection | **507** | All subjects with ground truth outlier masks |\n| Imputation | **507** | All subjects with ground truth denoised signals |\n| Classification | **208** | 152 control + 56 glaucoma (labeled subset) |\n\n299 subjects have preprocessing ground truth but no classification labels.\n\n**Data provenance:** Najjar et al. 2023, *British Journal of Ophthalmology*\n(DOI: [10.1136/bjophthalmol-2021-319938](https://doi.org/10.1136/bjophthalmol-2021-319938)).\nThe original Singapore dataset (SNEC) contained 322 subjects with AUROC = 0.94.\nOur classification subset uses 208 subjects, so direct AUROC comparison with the\noriginal paper is not appropriate.\n\n\n# Exploring Results in DuckDB {#sec-duckdb}\n\nAll experiment results are stored in a DuckDB database. This is the **single\nsource of truth** for all figures and analyses: visualization code reads from\nDuckDB and never recomputes metrics.\n\n## Connecting to the Database\n\n::: {#db-connect .cell execution_count=1}\n``` {.python .cell-code}\nimport duckdb\nimport pandas as pd\n\nconn = duckdb.connect(\"../data/public/foundation_plr_results.db\", read_only=True)\n\n# List all available tables\ntables = conn.execute(\"SHOW TABLES\").fetchdf()\nprint(\"Available tables:\")\nfor name in tables[\"name\"]:\n    row_count = conn.execute(f\"SELECT COUNT(*) FROM {name}\").fetchone()[0]\n    print(f\"  {name}: {row_count:,} rows\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAvailable tables:\n  calibration_curves: 4,060 rows\n  cohort_metrics: 22,344 rows\n  dca_curves: 20,300 rows\n  distribution_stats: 406 rows\n  essential_metrics: 406 rows\n  extraction_checkpoints: 408 rows\n  predictions: 25,578 rows\n  probability_distributions: 8,120 rows\n  retention_metrics: 29,792 rows\n  supplementary_metrics: 406 rows\n```\n:::\n:::\n\n\n## Top Configurations by AUROC\n\n::: {#cell-top-configs .cell execution_count=2}\n``` {.python .cell-code}\ntop5 = conn.execute(\"\"\"\n    SELECT outlier_method, imputation_method,\n           ROUND(auroc, 4) AS auroc,\n           ROUND(calibration_slope, 3) AS cal_slope,\n           ROUND(brier, 4) AS brier\n    FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n      AND featurization NOT LIKE '%embedding%'\n    ORDER BY auroc DESC\n    LIMIT 5\n\"\"\").fetchdf()\n\ntop5\n```\n\n::: {#top-configs .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>outlier_method</th>\n      <th>imputation_method</th>\n      <th>auroc</th>\n      <th>cal_slope</th>\n      <th>brier</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA...</td>\n      <td>CSDI</td>\n      <td>0.9130</td>\n      <td>2.121</td>\n      <td>0.1074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA...</td>\n      <td>TimesNet</td>\n      <td>0.9122</td>\n      <td>6.276</td>\n      <td>0.1590</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ensembleThresholded-MOMENT-TimesNet-UniTS-gt-f...</td>\n      <td>CSDI</td>\n      <td>0.9116</td>\n      <td>3.779</td>\n      <td>0.1359</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ensembleThresholded-MOMENT-TimesNet-UniTS-gt-f...</td>\n      <td>TimesNet</td>\n      <td>0.9113</td>\n      <td>6.098</td>\n      <td>0.1591</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pupil-gt</td>\n      <td>ensemble-CSDI-MOMENT-SAITS</td>\n      <td>0.9110</td>\n      <td>2.128</td>\n      <td>0.1079</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## AUROC Distribution Across All Configurations\n\n::: {#cell-auroc-summary-stats .cell execution_count=3}\n``` {.python .cell-code}\nsummary = conn.execute(\"\"\"\n    SELECT\n        COUNT(*) AS n_configs,\n        ROUND(MIN(auroc), 4) AS min_auroc,\n        ROUND(MAX(auroc), 4) AS max_auroc,\n        ROUND(AVG(auroc), 4) AS mean_auroc,\n        ROUND(MEDIAN(auroc), 4) AS median_auroc,\n        ROUND(STDDEV(auroc), 4) AS std_auroc\n    FROM essential_metrics\n\"\"\").fetchdf()\n\nsummary\n```\n\n::: {#auroc-summary-stats .cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_configs</th>\n      <th>min_auroc</th>\n      <th>max_auroc</th>\n      <th>mean_auroc</th>\n      <th>median_auroc</th>\n      <th>std_auroc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>406</td>\n      <td>0.5</td>\n      <td>0.913</td>\n      <td>0.8287</td>\n      <td>0.8478</td>\n      <td>0.0705</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Method Counts Per Stage\n\n::: {#method-counts .cell execution_count=4}\n``` {.python .cell-code}\nfor col_name, label in [\n    (\"outlier_method\", \"Outlier methods\"),\n    (\"imputation_method\", \"Imputation methods\"),\n    (\"classifier\", \"Classifiers\"),\n    (\"featurization\", \"Featurization types\"),\n]:\n    n = conn.execute(\n        f\"SELECT COUNT(DISTINCT {col_name}) FROM essential_metrics\"\n    ).fetchone()[0]\n    print(f\"  {label}: {n}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Outlier methods: 18\n  Imputation methods: 9\n  Classifiers: 6\n  Featurization types: 6\n```\n:::\n:::\n\n\n## AUROC Histogram\n\n::: {#cell-fig-auroc-histogram .cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nauroc_values = conn.execute(\n    \"SELECT auroc FROM essential_metrics\"\n).fetchdf()\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.hist(auroc_values[\"auroc\"], bins=30, edgecolor=\"white\", linewidth=0.5)\nax.set_xlabel(\"AUROC\")\nax.set_ylabel(\"Number of Configurations\")\nax.set_title(\"AUROC Distribution Across All Pipeline Configurations\")\nax.axvline(\n    auroc_values[\"auroc\"].median(),\n    color=\"gray\", linestyle=\"--\", linewidth=1,\n    label=f'Median = {auroc_values[\"auroc\"].median():.3f}',\n)\nax.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of AUROC across all 406 preprocessing configurations.](01-pipeline-walkthrough_files/figure-html/fig-auroc-histogram-output-1.png){#fig-auroc-histogram}\n:::\n:::\n\n\n# Key Findings {#sec-findings}\n\n## Best Performing Configurations\n\n::: {#key-findings .cell execution_count=6}\n``` {.python .cell-code}\n# Best overall (any classifier)\nbest_overall = conn.execute(\"\"\"\n    SELECT outlier_method, imputation_method, classifier,\n           ROUND(auroc, 4) AS auroc\n    FROM essential_metrics\n    ORDER BY auroc DESC\n    LIMIT 1\n\"\"\").fetchone()\n\n# Ground truth baseline (CatBoost with ground truth preprocessing)\ngt_baseline = conn.execute(\"\"\"\n    SELECT ROUND(auroc, 4) AS auroc\n    FROM essential_metrics\n    WHERE outlier_method = 'pupil-gt'\n      AND imputation_method = 'pupil-gt'\n      AND classifier = 'CatBoost'\n      AND featurization NOT LIKE '%embedding%'\n    LIMIT 1\n\"\"\").fetchone()\n\n# Handcrafted vs embedding comparison\nfeat_comparison = conn.execute(\"\"\"\n    SELECT featurization,\n           ROUND(AVG(auroc), 4) AS mean_auroc,\n           COUNT(*) AS n_configs\n    FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n    GROUP BY featurization\n    ORDER BY mean_auroc DESC\n\"\"\").fetchdf()\n\nprint(f\"Best overall config: {best_overall[0]} + {best_overall[1]} \"\n      f\"({best_overall[2]}), AUROC = {best_overall[3]}\")\nprint(f\"Ground truth baseline (CatBoost): AUROC = {gt_baseline[0]}\")\nprint(f\"\\nMean AUROC by featurization type:\")\nprint(feat_comparison.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest overall config: ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune + CSDI (CatBoost), AUROC = 0.9129999876022339\nGround truth baseline (CatBoost): AUROC = 0.9110000133514404\n\nMean AUROC by featurization type:\n       featurization  mean_auroc  n_configs\n           simple1.0      0.8810         79\n    MOMENT-embedding      0.7795          1\nMOMENT-embedding-PCA      0.7747          1\n```\n:::\n:::\n\n\n## Summary of Key Results\n\n| Finding | Value | Interpretation |\n|---------|-------|----------------|\n| Best AUROC | ~0.913 | Ensemble outlier + CSDI imputation + CatBoost |\n| Ground truth baseline | ~0.911 | Preprocessing ceiling (nearly matched by best pipeline) |\n| Preprocessing effect | eta-squared ~ 0.15 | Moderate effect -- preprocessing matters |\n| Handcrafted vs. Embeddings | ~9pp gap | Handcrafted features win due to EPV constraints |\n\nThe fact that the best automated pipeline (AUROC ~ 0.913) slightly exceeds the\nground truth baseline (AUROC ~ 0.911) is not paradoxical: ensemble methods can\naverage out annotation noise that affects even human ground truth labels.\n\nFoundation models are competitive for **preprocessing** (outlier detection and\nimputation) but their **embeddings** underperform handcrafted features by\napproximately 9 percentage points. This is likely due to the small sample size\n(N=208 for classification) and the events-per-variable (EPV) constraint: with\nonly 56 glaucoma cases, high-dimensional embeddings overfit while 8 handcrafted\nfeatures remain stable.\n\n\n# STRATOS Metrics: Beyond AUROC {#sec-stratos}\n\nThe STRATOS initiative (Van Calster et al. 2024) recommends evaluating\nprediction models across **five domains**, not just discrimination:\n\n\n```{mermaid}\nflowchart TB\n    S[\"STRATOS-Compliant\\nModel Evaluation\"] --> D[\"Discrimination\\nAUROC + CI\"]\n    S --> C[\"Calibration\\nSlope, Intercept, O:E\"]\n    S --> O[\"Overall\\nBrier Score,\\nScaled Brier (IPA)\"]\n    S --> U[\"Clinical Utility\\nNet Benefit, DCA\"]\n    S --> P[\"Distributions\\nPredicted probability\\nby outcome\"]\n\n    style S fill:#ffe,stroke:#333,stroke-width:2px\n    style D fill:#ddf,stroke:#333\n    style C fill:#ddf,stroke:#333\n    style O fill:#ddf,stroke:#333\n    style U fill:#ddf,stroke:#333\n    style P fill:#ddf,stroke:#333\n```\n\n\n## Why AUROC Alone Is Insufficient\n\nAUROC measures **discrimination** -- the ability to rank patients by risk. But a\nmodel can have excellent discrimination while being **poorly calibrated**: it\nranks patients correctly but assigns wrong absolute probabilities. In clinical\ndecision-making, absolute probabilities matter because they determine treatment\nthresholds.\n\nFor example, a model might correctly identify that Patient A has higher risk than\nPatient B, but if it says \"60% risk\" when the true risk is 10%, a clinician\nwould recommend unnecessary treatment. Calibration metrics (slope, intercept,\nO:E ratio) catch this.\n\n**Net Benefit** and **Decision Curve Analysis (DCA)** go further: they quantify\nwhether using the model leads to better clinical decisions than the alternatives\n(treat-all or treat-none) across a range of clinically reasonable thresholds.\n\n## All STRATOS Metrics for the Best Configuration\n\n::: {#stratos-best-config .cell execution_count=7}\n``` {.python .cell-code}\nbest_stratos = conn.execute(\"\"\"\n    SELECT\n        outlier_method,\n        imputation_method,\n        ROUND(auroc, 4) AS auroc,\n        ROUND(auroc_ci_lower, 4) AS auroc_ci_lower,\n        ROUND(auroc_ci_upper, 4) AS auroc_ci_upper,\n        ROUND(calibration_slope, 3) AS cal_slope,\n        ROUND(calibration_intercept, 3) AS cal_intercept,\n        ROUND(o_e_ratio, 3) AS o_e_ratio,\n        ROUND(brier, 4) AS brier,\n        ROUND(net_benefit_5pct, 4) AS nb_5pct,\n        ROUND(net_benefit_10pct, 4) AS nb_10pct,\n        ROUND(net_benefit_15pct, 4) AS nb_15pct,\n        ROUND(net_benefit_20pct, 4) AS nb_20pct\n    FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n      AND featurization NOT LIKE '%embedding%'\n    ORDER BY auroc DESC\n    LIMIT 1\n\"\"\").fetchdf()\n\n# Transpose for readability\nprint(\"Best CatBoost configuration (all STRATOS metrics):\")\nprint(best_stratos.T.to_string(header=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest CatBoost configuration (all STRATOS metrics):\noutlier_method     ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune\nimputation_method                                                                       CSDI\nauroc                                                                                  0.913\nauroc_ci_lower                                                                        0.9041\nauroc_ci_upper                                                                        0.9194\ncal_slope                                                                              2.121\ncal_intercept                                                                          0.296\no_e_ratio                                                                               0.86\nbrier                                                                                 0.1074\nnb_5pct                                                                               0.2314\nnb_10pct                                                                              0.1887\nnb_15pct                                                                              0.1699\nnb_20pct                                                                              0.1667\n```\n:::\n:::\n\n\n## Miscalibrated Configurations\n\nA high AUROC does not guarantee good calibration. Let us find configurations\nwhere discrimination is reasonable but calibration slope falls outside the\nacceptable range (ideal = 1.0):\n\n::: {#miscalibrated-configs .cell execution_count=8}\n``` {.python .cell-code}\nmiscalibrated = conn.execute(\"\"\"\n    SELECT\n        outlier_method,\n        imputation_method,\n        classifier,\n        ROUND(auroc, 4) AS auroc,\n        ROUND(calibration_slope, 3) AS cal_slope,\n        ROUND(calibration_intercept, 3) AS cal_intercept\n    FROM essential_metrics\n    WHERE auroc > 0.80\n      AND (calibration_slope > 2.0 OR calibration_slope < 0.5)\n    ORDER BY auroc DESC\n    LIMIT 10\n\"\"\").fetchdf()\n\nif len(miscalibrated) > 0:\n    print(f\"Found {len(miscalibrated)} configs with AUROC > 0.80 \"\n          \"but poor calibration (slope > 2.0 or < 0.5):\")\n    print(miscalibrated.to_string(index=False))\nelse:\n    print(\"No configurations found with high AUROC but extreme \"\n          \"miscalibration -- a reassuring finding.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 10 configs with AUROC > 0.80 but poor calibration (slope > 2.0 or < 0.5):\n                                                           outlier_method                   imputation_method classifier  auroc  cal_slope  cal_intercept\nensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune                                CSDI   CatBoost 0.9130      2.121          0.296\nensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune                            TimesNet   CatBoost 0.9122      6.276          2.331\n                    ensembleThresholded-MOMENT-TimesNet-UniTS-gt-finetune                                CSDI   CatBoost 0.9116      3.779          1.108\n                    ensembleThresholded-MOMENT-TimesNet-UniTS-gt-finetune                            TimesNet   CatBoost 0.9113      6.098          2.233\n                                                                 pupil-gt          ensemble-CSDI-MOMENT-SAITS   CatBoost 0.9110      2.128          0.308\n                                                                  anomaly ensemble-CSDI-MOMENT-SAITS-TimesNet   CatBoost 0.9110      2.135          0.314\n                                                                 pupil-gt                            pupil-gt   CatBoost 0.9110      2.960          0.687\nensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune                               SAITS   CatBoost 0.9104      2.150          0.325\n                                                                 pupil-gt                                CSDI   CatBoost 0.9103      2.131          0.309\n                                                       MOMENT-gt-finetune                               SAITS   CatBoost 0.9099      3.665          1.018\n```\n:::\n:::\n\n\n**Reference:** Van Calster B, Collins GS, Vickers AJ, et al. (2024).\n\"Performance evaluation of predictive AI models to support medical decisions.\"\n*STRATOS Initiative Topic Group 6.*\n\n\n# Pipeline Architecture {#sec-architecture}\n\nThe analysis pipeline follows a strict **two-block architecture** that separates\ncomputation from visualization:\n\n\n```{mermaid}\nflowchart LR\n    subgraph Block1[\"Block 1: Extraction\"]\n        direction TB\n        ML[\"MLflow\\n(542 experiments)\"] --> EX[\"Extraction Scripts\"]\n        EX --> DB[\"DuckDB\\n(406 configs)\"]\n    end\n\n    subgraph Block2[\"Block 2: Analysis\"]\n        direction TB\n        DB2[\"DuckDB\\n(read-only)\"] --> FIG[\"Figures\\n(R + Python)\"]\n        DB2 --> STAT[\"Statistical\\nAnalysis\"]\n        DB2 --> TEX[\"LaTeX\\nTables\"]\n    end\n\n    DB --> DB2\n\n    style Block1 fill:#ffe8e8,stroke:#c00,stroke-width:2px\n    style Block2 fill:#e8ffe8,stroke:#0a0,stroke-width:2px\n```\n\n\n**Block 1 (Extraction)** requires access to raw MLflow data at\n`/home/petteri/mlruns/`. This block reads experiment results, computes all\nSTRATOS metrics, and stores them in DuckDB. This only needs to run once (or when\nnew experiments are added).\n\n**Block 2 (Analysis)** reads from DuckDB only. It generates all figures,\nstatistical summaries, and LaTeX tables. This block is fully portable: any\nresearcher with the DuckDB file can reproduce all downstream outputs without\nneeding MLflow or raw data access.\n\nThis separation is enforced architecturally: visualization code in `src/viz/` is\n**banned** from importing `sklearn.metrics` or any computation module. All\nmetrics come pre-computed from DuckDB.\n\n## Commands\n\n| Command | What it does |\n|---------|-------------|\n| `make reproduce` | Full pipeline (Block 1 + Block 2) |\n| `make extract` | Block 1 only (MLflow to DuckDB) |\n| `make analyze` | Block 2 only (DuckDB to figures/stats) |\n| `make reproduce-from-checkpoint` | Block 2 only (alias) |\n\n\n# Generating Publication Figures {#sec-figures}\n\nAll publication figures are generated from DuckDB data using either **Python\nmatplotlib** or **R ggplot2**, depending on the figure type.\n\n## Figure Generation System\n\nThe figure system follows strict rules:\n\n1. **Load combos from YAML** -- never hardcode method names or colors\n2. **Maximum 4 curves** in main figures (configurable for supplementary)\n3. **Ground truth required** in every comparison figure\n4. **JSON data saved** alongside every figure for reproducibility\n\n## Available Figure Commands\n\n| Command | Purpose |\n|---------|---------|\n| `make figures` | Generate all Python figures |\n| `make r-figures-all` | Generate all R figures |\n| `python src/viz/generate_all_figures.py --list` | List available figure IDs |\n| `python src/viz/generate_all_figures.py --figure R7` | Generate a specific figure |\n\n## Figure Data Source\n\nEvery figure reads exclusively from DuckDB. For example, a ROC curve figure\nqueries the `predictions` table to get `y_true` and `y_prob` per configuration,\nwhile a calibration plot reads from the `calibration_curves` table.\n\n::: {#figure-data-example .cell execution_count=9}\n``` {.python .cell-code}\n# Example: data available for calibration plots\ncal_data = conn.execute(\"\"\"\n    SELECT\n        e.outlier_method,\n        e.imputation_method,\n        COUNT(*) AS n_bins\n    FROM calibration_curves c\n    JOIN essential_metrics e ON c.config_id = e.config_id\n    GROUP BY e.outlier_method, e.imputation_method\n    ORDER BY n_bins DESC\n    LIMIT 5\n\"\"\").fetchdf()\n\nprint(\"Calibration curve data available for top configs:\")\nprint(cal_data.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalibration curve data available for top configs:\n      outlier_method imputation_method  n_bins\n            pupil-gt          pupil-gt     140\n       TimesNet-orig          TimesNet      50\nMOMENT-orig-finetune              CSDI      50\n  MOMENT-gt-zeroshot             SAITS      50\n                 LOF   MOMENT-zeroshot      50\n```\n:::\n:::\n\n\n## Computation Decoupling\n\nA critical architectural principle: **visualization code never computes\nmetrics**. The following imports are banned in `src/viz/`:\n\n```python\n# BANNED in visualization code:\nfrom sklearn.metrics import roc_auc_score      # Use DuckDB auroc column\nfrom sklearn.metrics import brier_score_loss    # Use DuckDB brier column\nfrom src.stats.calibration_extended import ...  # Use DuckDB cal columns\n\n# CORRECT in visualization code:\ndf = conn.execute(\"SELECT auroc, calibration_slope FROM essential_metrics\").fetchdf()\n```\n\nThis ensures that figures always reflect the exact same numbers as statistical\nanalyses and LaTeX tables.\n\n\n# Next Steps {#sec-next-steps}\n\n## For Reproducers\n\nIf you want to reproduce the published results:\n\n1. **Clone the repository** and install dependencies with `uv sync`\n2. **Run Block 2**: `make analyze` (requires only the DuckDB file)\n3. All figures, tables, and statistics will be regenerated in `outputs/`\n\nIf you have access to the raw MLflow data and want to verify from scratch:\n\n1. Run `make reproduce` (full pipeline, Block 1 + Block 2)\n\n## For Extenders\n\nTo add a new preprocessing method to the pipeline:\n\n1. Register the method in `configs/mlflow_registry/parameters/classification.yaml`\n2. Run experiments and log results to MLflow\n3. Re-run extraction: `make extract`\n4. All downstream figures and analyses will automatically include the new method\n\n## Related Resources\n\n| Resource | Location |\n|----------|----------|\n| Project README | `README.md` in repository root |\n| Architecture overview | `ARCHITECTURE.md` in repository root |\n| API documentation | `docs/` (build with `make docs`) |\n| Manuscript | Sister repository (see `CLAUDE.md`) |\n\n## Citation\n\nIf you use this pipeline or data in your research, please cite:\n\n> Najjar RP, et al. (2023). Pupillary light reflex as a diagnostic aid for\n> glaucoma: a Bayesian approach. *British Journal of Ophthalmology.*\n> DOI: [10.1136/bjophthalmol-2021-319938](https://doi.org/10.1136/bjophthalmol-2021-319938)\n\n::: {#cleanup .cell execution_count=10}\n``` {.python .cell-code}\nconn.close()\n```\n:::\n\n\n",
    "supporting": [
      "01-pipeline-walkthrough_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}