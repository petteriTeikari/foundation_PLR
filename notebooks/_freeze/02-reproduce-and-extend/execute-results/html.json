{
  "hash": "f74fbb8979182312b57be81a47caa6fd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Foundation PLR: Reproduce and Extend\"\nsubtitle: \"Reproduction guide and data scientist contribution workflow\"\nauthor: \"Foundation PLR Team\"\ndate: today\n---\n\n\n# Reproduction Overview\n\nThis notebook is a pedagogical tutorial for two audiences:\n\n1. **Researchers** wanting to reproduce the published results from the Foundation PLR\n   paper (Pupillary Light Reflex preprocessing with foundation models).\n2. **Data scientists** wanting to contribute new analyses, metrics, or visualizations\n   to the project.\n\nThe key architectural insight is that the project uses a **two-block pipeline**\nthat separates _extraction_ (which requires access to the original MLflow experiment\ndata) from _analysis_ (which works entirely from a portable DuckDB database).\n\n## Two-Block Architecture\n\n\n```{mermaid}\n%%| label: fig-architecture\n%%| fig-cap: \"Two-block pipeline separating extraction from analysis.\"\nflowchart LR\n    subgraph Block1[\"Block 1: Extraction\"]\n        direction TB\n        ML[\"/home/petteri/mlruns\\n(542 pickles)\"]\n        ML --> EXT[\"extraction_flow()\"]\n        EXT --> DB[(\"DuckDB\\nfoundation_plr_results.db\")]\n    end\n\n    subgraph Block2[\"Block 2: Analysis\"]\n        direction TB\n        DB2[(\"DuckDB\\n406 configs\")] --> FIG[\"Figures\"]\n        DB2 --> STAT[\"Statistics\"]\n        DB2 --> TEX[\"LaTeX artifacts\\nnumbers.tex\"]\n    end\n\n    DB --> DB2\n\n    style Block1 fill:#f9f0ff,stroke:#7B68EE\n    style Block2 fill:#f0fff4,stroke:#45B29D\n```\n\n\n## What Each Block Produces\n\n| Block | Input | Output | Who Needs It |\n|-------|-------|--------|-------------|\n| **Block 1: Extraction** | Raw MLflow runs (`/home/petteri/mlruns/`) | `data/public/foundation_plr_results.db` | Only the original authors |\n| **Block 2: Analysis** | DuckDB database (portable) | Figures, stats, LaTeX | **Everyone** (external researchers start here) |\n\n::: {.callout-tip}\n## External researchers: you only need Block 2\n\nThe public DuckDB file contains all 406 pipeline configurations with predictions,\nmetrics, calibration curves, and DCA curves. You do **not** need access to the\noriginal MLflow experiments.\n:::\n\n\n# Running the Analysis Pipeline\n\n## The Simple Command\n\nThe entire analysis pipeline runs with a single command from the project root:\n\n```bash\nmake analyze\n```\n\n## What Happens Under the Hood\n\nThe `make analyze` target calls `scripts/reproduce_all_results.py --analyze-only`,\nwhich orchestrates a Prefect flow with these sequential steps:\n\n\n```{mermaid}\n%%| label: fig-analysis-flow\n%%| fig-cap: \"Sequence of steps in the analysis flow.\"\nsequenceDiagram\n    participant M as make analyze\n    participant S as reproduce_all_results.py\n    participant F as analysis_flow()\n    participant DB as DuckDB\n\n    M->>S: --analyze-only\n    S->>F: analysis_flow(db_path)\n    F->>DB: check_public_data()\n    DB-->>F: 406 configs, tables OK\n    F->>F: generate_all_figures()\n    F->>F: compute_statistics()\n    F->>F: generate_latex_artifacts()\n    F->>F: generate_report()\n    F-->>S: analysis_report.json\n```\n\n\n## The Subprocess Call\n\n::: {#run-analysis .cell execution_count=1}\n``` {.python .cell-code}\n# This cell is eval: false -- it shows the command but does not\n# execute the full pipeline inside this notebook.  Run it from\n# the terminal instead:  make analyze\nimport subprocess\n\nresult = subprocess.run(\n    [\"make\", \"analyze\"],\n    capture_output=True, text=True,\n    cwd=\"..\",\n)\nprint(result.stdout[-2000:])\n```\n:::\n\n\n::: {.callout-note}\n## Why `eval: false`?\n\nThe full analysis pipeline generates all manuscript figures, computes statistics,\nand writes LaTeX artifacts. Running it inside a notebook is slow and produces\nfile-system side effects. Use `make analyze` from your terminal instead.\n:::\n\n## Output Artifacts\n\nAfter a successful run, these paths are populated:\n\n| Output | Path | Description |\n|--------|------|-------------|\n| Figures | `outputs/figures/` | PNG/PDF manuscript figures |\n| LaTeX numbers | `artifacts/latex/numbers.tex` | `\\providecommand` for inline stats |\n| Report | `analysis_report.json` | Machine-readable run summary |\n\n\n# Displaying Loguru/Prefect Output in Notebooks\n\nThe project uses [Loguru](https://github.com/Delgan/loguru) for structured logging.\nBy default, Loguru writes to `stderr`, which Jupyter/Quarto may not display cleanly.\nHere is how to redirect log output into notebook cells.\n\n::: {#cell-setup-loguru-sink .cell execution_count=2}\n``` {.python .cell-code}\nimport sys\nfrom loguru import logger\n\n# Remove default stderr sink, add a notebook-friendly one\nlogger.remove()\nlogger.add(\n    sys.stdout,\n    format=(\"<green>{time:HH:mm:ss}</green> | \"\n            \"<level>{level: <8}</level> | {message}\"),\n    level=\"INFO\",\n    colorize=False,\n)\n```\n\n::: {#setup-loguru-sink .cell-output .cell-output-display execution_count=1}\n```\n1\n```\n:::\n:::\n\n\n::: {#demo-loguru .cell execution_count=3}\n``` {.python .cell-code}\nlogger.info(\"Notebook loguru sink is active\")\nlogger.warning(\"Warnings render inline too\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11:02:36 | INFO     | Notebook loguru sink is active\n11:02:36 | WARNING  | Warnings render inline too\n```\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Rich HTML log output (optional)\n\nFor HTML-rendered logs with collapsible sections, you can use `IPython.display`:\n\n```python\nfrom IPython.display import HTML, display\n\ndef log_html(msg: str, level: str = \"info\"):\n    colors = {\"info\": \"steelblue\", \"warning\": \"orange\", \"error\": \"crimson\"}\n    color = colors.get(level, \"gray\")\n    display(HTML(\n        f'<span style=\"color:{color};font-family:monospace\">'\n        f'[{level.upper()}] {msg}</span>'\n    ))\n```\n:::\n\n\n# Custom Analyses from DuckDB {#sec-custom}\n\nThis is where the tutorial becomes hands-on. We connect to the public DuckDB\nand run analyses that go beyond the published figures.\n\n## Connect to the Database\n\n::: {#connect-db .cell execution_count=4}\n``` {.python .cell-code}\nimport duckdb\nimport pandas as pd\nimport numpy as np\n\nDB_PATH = \"../data/public/foundation_plr_results.db\"\nconn = duckdb.connect(DB_PATH, read_only=True)\n\n# Verify connection\ntables = [t[0] for t in conn.execute(\"SHOW TABLES\").fetchall()]\nprint(f\"Tables: {tables}\")\n\nn_configs = conn.execute(\n    \"SELECT COUNT(DISTINCT config_id) FROM essential_metrics\"\n).fetchone()[0]\nprint(f\"Total configs: {n_configs}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTables: ['calibration_curves', 'cohort_metrics', 'dca_curves', 'distribution_stats', 'essential_metrics', 'extraction_checkpoints', 'predictions', 'probability_distributions', 'retention_metrics', 'supplementary_metrics']\nTotal configs: 406\n```\n:::\n:::\n\n\n## Example 1: Integrated Calibration Index (ICI)\n\nThe Integrated Calibration Index (ICI) is a calibration metric not included\nin our standard STRATOS set. It measures the weighted average absolute\ndifference between observed and predicted probabilities across the\ncalibration curve. Lower is better.\n\nWe compute it from the `calibration_curves` table -- no `sklearn` needed.\n\n::: {#cell-ici-single-config .cell execution_count=5}\n``` {.python .cell-code}\n# Compute ICI for one config from pre-computed calibration curves\ncal = conn.execute(\"\"\"\n    SELECT c.bin_midpoint, c.observed_proportion, c.predicted_mean\n    FROM calibration_curves c\n    JOIN essential_metrics e ON c.config_id = e.config_id\n    WHERE e.classifier = 'CatBoost'\n      AND e.featurization = 'simple1.0'\n      AND e.outlier_method = 'pupil-gt'\n      AND e.imputation_method = 'pupil-gt'\n    ORDER BY c.bin_index\n\"\"\").fetchdf()\n\nprint(f\"Calibration bins: {len(cal)}\")\ncal.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalibration bins: 10\n```\n:::\n\n::: {#ici-single-config .cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_midpoint</th>\n      <th>observed_proportion</th>\n      <th>predicted_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.111111</td>\n      <td>0.000016</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.222222</td>\n      <td>0.073830</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.333333</td>\n      <td>0.285145</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.444444</td>\n      <td>0.496889</td>\n      <td>0.444444</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#ici-compute .cell execution_count=6}\n``` {.python .cell-code}\ndef compute_ici(cal_df: pd.DataFrame) -> float:\n    \"\"\"Integrated Calibration Index from calibration curve data.\n\n    ICI = weighted mean |observed - predicted| across bins.\n    \"\"\"\n    diff = np.abs(\n        cal_df[\"observed_proportion\"].values\n        - cal_df[\"predicted_mean\"].values\n    )\n    return float(np.mean(diff))\n\nici_gt = compute_ici(cal)\nprint(f\"ICI (ground truth config): {ici_gt:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nICI (ground truth config): 0.1140\n```\n:::\n:::\n\n\n::: {#ici-top5 .cell execution_count=7}\n``` {.python .cell-code}\n# Compare ICI across top 5 configs by AUROC\ntop5 = conn.execute(\"\"\"\n    SELECT config_id, auroc\n    FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n    ORDER BY auroc DESC\n    LIMIT 5\n\"\"\").fetchdf()\n\nici_results = []\nfor cid in top5[\"config_id\"]:\n    cal_i = conn.execute(\"\"\"\n        SELECT bin_midpoint, observed_proportion, predicted_mean\n        FROM calibration_curves\n        WHERE config_id = ?\n        ORDER BY bin_index\n    \"\"\", [cid]).fetchdf()\n    if len(cal_i) > 0:\n        ici_results.append({\"config_id\": cid, \"ici\": compute_ici(cal_i)})\n\nici_df = pd.DataFrame(ici_results)\nprint(ici_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n config_id      ici\n       131 0.087259\n       344 0.170345\n       280 0.139451\n       217 0.168750\n       232 0.087875\n```\n:::\n:::\n\n\n::: {.callout-important}\n## No `sklearn.metrics` imports\n\nThis project enforces a strict computation-decoupling rule: visualization and\nnotebook code reads from DuckDB only. All metric computation happens during\nextraction. We computed ICI above using only `numpy` on pre-computed calibration\nbin data.\n:::\n\n## Example 2: Multi-Metric Ranking (STRATOS in Action)\n\nThe entire point of STRATOS-compliant reporting is that **AUROC ranking does not\nequal calibration ranking**. Let us verify this empirically.\n\n::: {#cell-multi-metric-query .cell execution_count=8}\n``` {.python .cell-code}\nrankings = conn.execute(\"\"\"\n    SELECT\n        outlier_method,\n        AVG(auroc) AS mean_auroc,\n        AVG(ABS(calibration_slope - 1.0)) AS cal_slope_deviation,\n        AVG(brier) AS mean_brier\n    FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n    GROUP BY outlier_method\n    ORDER BY mean_auroc DESC\n\"\"\").fetchdf()\n\nrankings.head(11)\n```\n\n::: {#multi-metric-query .cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>outlier_method</th>\n      <th>mean_auroc</th>\n      <th>cal_slope_deviation</th>\n      <th>mean_brier</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>anomaly</td>\n      <td>0.910996</td>\n      <td>1.135122</td>\n      <td>0.108231</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ensembleThresholded-MOMENT-TimesNet-UniTS-gt-f...</td>\n      <td>0.907818</td>\n      <td>2.230338</td>\n      <td>0.126834</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA...</td>\n      <td>0.905084</td>\n      <td>1.752736</td>\n      <td>0.119468</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UniTS-orig-zeroshot</td>\n      <td>0.892881</td>\n      <td>1.886168</td>\n      <td>0.131298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MOMENT-gt-finetune</td>\n      <td>0.892227</td>\n      <td>2.089422</td>\n      <td>0.137887</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>UniTS-gt-finetune</td>\n      <td>0.888420</td>\n      <td>0.895381</td>\n      <td>0.117651</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>TimesNet-gt</td>\n      <td>0.886546</td>\n      <td>2.592191</td>\n      <td>0.128898</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>TimesNet-orig</td>\n      <td>0.885658</td>\n      <td>2.768704</td>\n      <td>0.145743</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>UniTS-orig-finetune</td>\n      <td>0.883200</td>\n      <td>1.914606</td>\n      <td>0.133481</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>pupil-gt</td>\n      <td>0.878295</td>\n      <td>1.280345</td>\n      <td>0.125268</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>MOMENT-gt-zeroshot</td>\n      <td>0.874623</td>\n      <td>1.611882</td>\n      <td>0.134476</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#multi-metric-ranks .cell execution_count=9}\n``` {.python .cell-code}\n# Assign ranks for each metric\nrankings[\"rank_auroc\"] = rankings[\"mean_auroc\"].rank(ascending=False)\nrankings[\"rank_cal_slope\"] = rankings[\"cal_slope_deviation\"].rank(ascending=True)\nrankings[\"rank_brier\"] = rankings[\"mean_brier\"].rank(ascending=True)\n\ncols = [\"outlier_method\", \"rank_auroc\", \"rank_cal_slope\", \"rank_brier\"]\nprint(rankings[cols].to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                           outlier_method  rank_auroc  rank_cal_slope  rank_brier\n                                                                  anomaly         1.0             5.0         1.0\n                    ensembleThresholded-MOMENT-TimesNet-UniTS-gt-finetune         2.0            15.0         6.0\nensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune         3.0            11.0         4.0\n                                                      UniTS-orig-zeroshot         4.0            12.0         9.0\n                                                       MOMENT-gt-finetune         5.0            14.0        14.0\n                                                        UniTS-gt-finetune         6.0             2.0         3.0\n                                                              TimesNet-gt         7.0            16.0         7.0\n                                                            TimesNet-orig         8.0            17.0        17.0\n                                                      UniTS-orig-finetune         9.0            13.0        11.0\n                                                                 pupil-gt        10.0             9.0         5.0\n                                                       MOMENT-gt-zeroshot        11.0            10.0        12.0\n                                                                  PROPHET        12.0             6.0         8.0\n                                                     MOMENT-orig-finetune        13.0             3.0        10.0\n                                                                      LOF        14.0             7.0        15.0\n                                                                  exclude        15.0             1.0         2.0\n                                                              OneClassSVM        16.0             8.0        13.0\n                                                                   SubPCA        17.0             4.0        16.0\n```\n:::\n:::\n\n\n::: {#multi-metric-correlation .cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy.stats import spearmanr\n\nrho_auroc_cal, p_auroc_cal = spearmanr(\n    rankings[\"rank_auroc\"], rankings[\"rank_cal_slope\"]\n)\nprint(f\"Spearman rho (AUROC rank vs calibration rank): {rho_auroc_cal:.3f}\")\nprint(f\"p-value: {p_auroc_cal:.3f}\")\nprint()\nif abs(rho_auroc_cal) < 0.7:\n    print(\"Rankings DIFFER -- AUROC alone is insufficient.\")\n    print(\"This is why STRATOS requires multiple metric domains.\")\nelse:\n    print(\"Rankings are similar for this dataset.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpearman rho (AUROC rank vs calibration rank): -0.475\np-value: 0.054\n\nRankings DIFFER -- AUROC alone is insufficient.\nThis is why STRATOS requires multiple metric domains.\n```\n:::\n:::\n\n\n## Example 3: Decision Curve Analysis Exploration\n\nDCA curves show the **net clinical benefit** of using a model at different\ndecision thresholds, compared to treat-all and treat-none strategies.\n\n::: {#dca-query .cell execution_count=11}\n``` {.python .cell-code}\n# Load DCA curves for three representative configs\n# Query DCA curves for three representative preprocessing pipelines\ncombos = [\n    {\"outlier\": \"pupil-gt\", \"imputation\": \"pupil-gt\", \"label\": \"Ground Truth\"},\n    {\"outlier\": \"ensemble-LOF-MOMENT-OneClassSVM-PROPHET-SubPCA-TimesNet-UniTS-gt-finetune\",\n     \"imputation\": \"CSDI\", \"label\": \"Best Ensemble\"},\n    {\"outlier\": \"LOF\", \"imputation\": \"SAITS\", \"label\": \"Traditional (LOF+SAITS)\"},\n]\n\ndca_data = {}\nfor combo in combos:\n    df = conn.execute(\"\"\"\n        SELECT d.threshold, d.net_benefit_model,\n               d.net_benefit_all, d.net_benefit_none\n        FROM dca_curves d\n        JOIN essential_metrics e ON d.config_id = e.config_id\n        WHERE e.classifier = 'CatBoost'\n          AND e.featurization = 'simple1.0'\n          AND e.outlier_method = ?\n          AND e.imputation_method = ?\n        ORDER BY d.threshold\n    \"\"\", [combo[\"outlier\"], combo[\"imputation\"]]).fetchdf()\n    if len(df) > 0:\n        dca_data[combo[\"label\"]] = df\n\nprint(f\"Loaded DCA curves for {len(dca_data)} configs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded DCA curves for 3 configs\n```\n:::\n:::\n\n\n::: {#cell-dca-plot .cell execution_count=12}\n``` {.python .cell-code}\nimport sys\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Use project style system -- package installed via `uv sync`\n# COLORS dict provides semantic, non-hardcoded color references\nsys.path.insert(0, str(Path(\"..\").resolve()))\nfrom src.viz.plot_config import COLORS, setup_style\n\nsetup_style()\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\ncolor_map = {\n    \"Ground Truth\": COLORS[\"ground_truth\"],\n    \"Best Ensemble\": COLORS[\"ensemble\"],\n    \"Traditional (LOF+SAITS)\": COLORS[\"traditional\"],\n}\n\nfor label, df in dca_data.items():\n    ax.plot(\n        df[\"threshold\"], df[\"net_benefit_model\"],\n        label=label, color=color_map[label], linewidth=2,\n    )\n\n# Plot treat-all and treat-none reference lines from the first config\nref = list(dca_data.values())[0]\nax.plot(\n    ref[\"threshold\"], ref[\"net_benefit_all\"],\n    label=\"Treat All\", color=COLORS[\"neutral\"],\n    linestyle=\"--\", linewidth=1,\n)\nax.axhline(y=0, color=COLORS[\"grid\"], linestyle=\":\", label=\"Treat None\")\n\nax.set_xlabel(\"Threshold Probability\")\nax.set_ylabel(\"Net Benefit\")\nax.set_title(\"Decision Curve Analysis\")\nax.legend(loc=\"upper right\", fontsize=8)\nax.set_xlim(0.0, 0.4)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](02-reproduce-and-extend_files/figure-html/dca-plot-output-1.png){#dca-plot}\n:::\n:::\n\n\n::: {.callout-note}\n## Inline display only\n\nNotice we use `plt.show()` for inline rendering, **never** `plt.savefig()`.\nPublication figures are generated by the pipeline (`make figures`), not by notebooks.\n:::\n\n\n# For Data Scientists -- Contributing New Analyses {#sec-contributing}\n\n\n```{mermaid}\n%%| label: fig-contribution-workflow\n%%| fig-cap: \"Contribution workflow for new analyses.\"\nflowchart LR\n    A[\"Write function\\nin src/\"] --> B[\"Write tests\\nin tests/\"]\n    B --> C[\"Create .qmd\\nnotebook\"]\n    C --> D[\"CI validates\\nquarto render\"]\n    D --> E[\"PR review\"]\n\n    style A fill:#f9f0ff,stroke:#7B68EE\n    style B fill:#fff0f0,stroke:#D64045\n    style C fill:#f0fff4,stroke:#45B29D\n    style D fill:#fffff0,stroke:#F5A623\n    style E fill:#f0f8ff,stroke:#2E5090\n```\n\n\n## The Contract: Quarto-Only Policy\n\nAll notebook contributions **must** be `.qmd` (Quarto Markdown) files. This\nis enforced by a pre-commit hook (`scripts/validation/check_notebook_format.py`)\nand by CI.\n\n| Format | Status | Reason |\n|--------|--------|--------|\n| `.qmd` (Quarto) | **Required** | Plain-text diffs, CI testability, no output bloat |\n| `.ipynb` (Jupyter) | **Banned** | Binary JSON, merge conflicts, output leaks |\n| `.py` (marimo) | **Banned** | Non-standard, no Quarto integration |\n\n**Separation of concerns:**\n\n- **Logic** goes in `src/` modules (testable, importable, reusable).\n- **Notebooks** are orchestration + narrative. They import from `src/` and\n  read from DuckDB. They should be short, readable, and pedagogical.\n\n::: {.callout-warning}\n## Pre-commit will reject `.ipynb` files\n\nIf you try to commit a `.ipynb` file under `notebooks/`, the pre-commit hook\nwill fail with `POLICY VIOLATION`. Convert your notebook first:\n\n```bash\nquarto convert my_notebook.ipynb  # produces my_notebook.qmd\n```\n:::\n\n## Step-by-Step: Adding a New Statistical Test\n\nSuppose you want to add the **Hosmer-Lemeshow test** as a supplementary\ncalibration metric.\n\n### Step 1: Write the function in `src/`\n\nCreate `src/stats/hosmer_lemeshow.py`:\n\n::: {#example-new-stat .cell execution_count=13}\n``` {.python .cell-code}\n# src/stats/hosmer_lemeshow.py\n\"\"\"Hosmer-Lemeshow goodness-of-fit test.\"\"\"\n\nimport numpy as np\nfrom scipy.stats import chi2\n\n\ndef hosmer_lemeshow(\n    y_true: np.ndarray,\n    y_prob: np.ndarray,\n    n_groups: int = 10,\n) -> dict:\n    \"\"\"Compute Hosmer-Lemeshow statistic.\n\n    Returns dict with 'statistic', 'p_value', 'n_groups'.\n    \"\"\"\n    order = np.argsort(y_prob)\n    groups = np.array_split(order, n_groups)\n\n    hl_stat = 0.0\n    for g in groups:\n        obs = y_true[g].sum()\n        exp = y_prob[g].sum()\n        n = len(g)\n        if exp > 0 and (n - exp) > 0:\n            hl_stat += (obs - exp) ** 2 / exp\n            hl_stat += ((n - obs) - (n - exp)) ** 2 / (n - exp)\n\n    p_value = 1.0 - chi2.cdf(hl_stat, df=n_groups - 2)\n    return {\n        \"statistic\": hl_stat,\n        \"p_value\": p_value,\n        \"n_groups\": n_groups,\n    }\n```\n:::\n\n\n### Step 2: Write a pytest test\n\nCreate `tests/unit/test_hosmer_lemeshow.py`:\n\n::: {#example-new-test .cell execution_count=14}\n``` {.python .cell-code}\n# tests/unit/test_hosmer_lemeshow.py\nimport numpy as np\nimport pytest\n\nfrom src.stats.hosmer_lemeshow import hosmer_lemeshow\n\n\ndef test_perfect_calibration():\n    \"\"\"Perfect predictions should have high p-value.\"\"\"\n    rng = np.random.default_rng(42)\n    y_prob = rng.uniform(0, 1, size=500)\n    y_true = (rng.uniform(0, 1, size=500) < y_prob).astype(int)\n    result = hosmer_lemeshow(y_true, y_prob)\n    assert result[\"p_value\"] > 0.05\n\n\ndef test_returns_expected_keys():\n    y_true = np.array([0, 1, 0, 1, 1, 0, 0, 1, 0, 1])\n    y_prob = np.array([.1, .9, .2, .8, .7, .3, .4, .6, .15, .85])\n    result = hosmer_lemeshow(y_true, y_prob, n_groups=2)\n    assert \"statistic\" in result\n    assert \"p_value\" in result\n    assert \"n_groups\" in result\n```\n:::\n\n\n### Step 3: Create the notebook\n\nCreate `notebooks/extensions/hosmer_lemeshow_analysis.qmd` that imports\nfrom `src/stats/` and reads predictions from DuckDB:\n\n::: {#example-notebook-usage .cell execution_count=15}\n``` {.python .cell-code}\n# In the notebook:\nfrom src.stats.hosmer_lemeshow import hosmer_lemeshow\n\npreds = conn.execute(\"\"\"\n    SELECT y_true, y_prob FROM predictions\n    WHERE config_id = ?\n\"\"\", [config_id]).fetchdf()\n\nresult = hosmer_lemeshow(\n    preds[\"y_true\"].values,\n    preds[\"y_prob\"].values,\n)\nprint(f\"HL statistic: {result['statistic']:.2f}\")\nprint(f\"p-value: {result['p_value']:.4f}\")\n```\n:::\n\n\n### Step 4: CI validates\n\nCI runs `quarto render notebooks/extensions/hosmer_lemeshow_analysis.qmd` to\nverify the notebook executes without errors.\n\n## Step-by-Step: Adding a New Visualization\n\nThe pattern mirrors the statistical test workflow:\n\n1. Write the plot function in `src/viz/my_plot.py`. Use `setup_style()`,\n   `COLORS` dict, and `save_figure()` from `plot_config.py`.\n2. Register the figure in `configs/VISUALIZATION/figure_registry.yaml` with\n   its ID, description, and privacy level.\n3. Create a companion `.qmd` notebook with the narrative explanation.\n\n::: {#example-new-viz .cell execution_count=16}\n``` {.python .cell-code}\n# src/viz/my_raincloud.py\nfrom src.viz.plot_config import COLORS, setup_style\n\n\ndef raincloud_auroc(df, ax):\n    \"\"\"Raincloud plot of AUROC by outlier method.\"\"\"\n    setup_style()\n    # ... plot implementation using COLORS[\"primary\"], etc.\n    # NEVER hardcode hex colors\n    # NEVER call plt.savefig() -- use save_figure()\n    return ax\n```\n:::\n\n\n## Integration with Existing Pipeline\n\n### Prefect integration\n\nIf your analysis should run as part of `make analyze`, you can register it as\na Prefect task inside `src/orchestration/flows/analysis_flow.py`:\n\n::: {#example-prefect-task .cell execution_count=17}\n``` {.python .cell-code}\n# In analysis_flow.py:\nfrom src.orchestration._prefect_compat import task\n\n@task(name=\"render_extension_notebook\")\ndef render_extension_notebook(notebook_name: str):\n    import subprocess\n    subprocess.run(\n        [\"quarto\", \"render\", f\"notebooks/extensions/{notebook_name}\"],\n        check=True,\n    )\n```\n:::\n\n\n### Makefile target pattern\n\nFor convenience, add a Make target:\n\n```makefile\n# In Makefile:\nrender-notebook:\n    quarto render notebooks/$(NOTEBOOK).qmd\n```\n\nUsage: `make render-notebook NOTEBOOK=extensions/hosmer_lemeshow_analysis`\n\n### Parameterization\n\nQuarto supports command-line parameters. Define them in the YAML header:\n\n```yaml\nparams:\n  db_path: \"../data/public/foundation_plr_results.db\"\n```\n\nThen render with a custom path:\n\n```bash\nquarto render my_analysis.qmd -P db_path:path/to/custom.db\n```\n\n\n# Verifying Reproducibility {#sec-verify}\n\nAfter running `make analyze`, verify that results match the published numbers.\n\n## Expected Output Checklist\n\n::: {#verify-outputs .cell execution_count=18}\n``` {.python .cell-code}\nfrom pathlib import Path\n\nexpected = [\n    \"outputs/figures/\",\n    \"artifacts/latex/numbers.tex\",\n    \"analysis_report.json\",\n]\n\nfor p in expected:\n    path = Path(\"..\") / p\n    exists = path.exists()\n    status = \"OK\" if exists else \"MISSING\"\n    print(f\"  [{status}] {p}\")\n```\n:::\n\n\n## SHA256 Checksum Verification\n\nThe project ships a checksum file for data integrity. From the project root:\n\n```bash\nmake verify-data\n# Equivalent to: sha256sum -c data/_checksums.sha256\n```\n\nThis verifies both the public results database and the synthetic demo data.\n\n## Assertion Checks Against Published Numbers\n\n::: {#verify-auroc-values .cell execution_count=19}\n``` {.python .cell-code}\n# These are the key published numbers from the paper.\n# If they do not match, the database may have been corrupted.\n\ngt_auroc = conn.execute(\"\"\"\n    SELECT auroc FROM essential_metrics\n    WHERE classifier = 'CatBoost'\n      AND featurization = 'simple1.0'\n      AND outlier_method = 'pupil-gt'\n      AND imputation_method = 'pupil-gt'\n\"\"\").fetchone()\n\nif gt_auroc is not None:\n    gt_val = gt_auroc[0]\n    print(f\"Ground truth AUROC: {gt_val:.4f}\")\n    assert abs(gt_val - 0.911) < 0.002, (\n        f\"Ground truth AUROC mismatch: {gt_val}\"\n    )\n    print(\"  PASS: matches published value (0.911)\")\nelse:\n    print(\"  SKIP: ground truth config not found in DB\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGround truth AUROC: 0.9110\n  PASS: matches published value (0.911)\n```\n:::\n:::\n\n\n::: {#verify-ensemble-auroc .cell execution_count=20}\n``` {.python .cell-code}\nens_auroc = conn.execute(\"\"\"\n    SELECT auroc FROM essential_metrics\n    WHERE outlier_method LIKE 'ensemble-%'\n      AND imputation_method = 'CSDI'\n      AND classifier = 'CatBoost'\n    ORDER BY auroc DESC\n    LIMIT 1\n\"\"\").fetchone()\n\nif ens_auroc is not None:\n    ens_val = ens_auroc[0]\n    print(f\"Best ensemble AUROC: {ens_val:.4f}\")\n    assert abs(ens_val - 0.913) < 0.002, (\n        f\"Ensemble AUROC mismatch: {ens_val}\"\n    )\n    print(\"  PASS: matches published value (0.913)\")\nelse:\n    print(\"  SKIP: ensemble config not found in DB\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest ensemble AUROC: 0.9130\n  PASS: matches published value (0.913)\n```\n:::\n:::\n\n\n::: {#cleanup-connection .cell execution_count=21}\n``` {.python .cell-code}\nconn.close()\nprint(\"Database connection closed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatabase connection closed.\n```\n:::\n:::\n\n\n# Troubleshooting\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| `Database not found` | DuckDB file missing | Run `make extract` first (Block 1), or download the public DB |\n| `No module named src` | Wrong working directory or venv inactive | Run from project root; activate with `source .venv/bin/activate` |\n| `Private data missing` | No `data/private/` directory | Expected for external researchers. Private figures are skipped automatically |\n| `R package not found` | renv library out of sync | Run `Rscript -e 'renv::restore()'` from project root |\n| `quarto: command not found` | Quarto CLI not installed | Install from [quarto.org](https://quarto.org/docs/get-started/) |\n| `POLICY VIOLATION: .ipynb` | Committed a Jupyter notebook | Convert with `quarto convert notebook.ipynb`, then commit the `.qmd` |\n| `Banned pattern: sklearn.metrics` | Imported sklearn in notebook code | Read metrics from DuckDB instead (see @sec-custom) |\n| `Prefect import error` | Prefect not installed or disabled | Set `PREFECT_DISABLED=1` for local testing without orchestration |\n\n::: {.callout-tip}\n## Getting help\n\nIf you encounter an issue not listed here, check the project's\n[GitHub Issues](https://github.com/petteriTeikari/foundation-PLR/issues)\nor open a new one with the `reproduction` label.\n:::\n\n",
    "supporting": [
      "02-reproduce-and-extend_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}