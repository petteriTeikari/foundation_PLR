{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation PLR: Reproducibility Tutorial\n",
    "\n",
    "**Authors**: [Your Name]\n",
    "**Last Updated**: 2026-01-19\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to reproduce the classification and statistical analyses from the Foundation PLR study without access to the raw clinical data.\n",
    "\n",
    "### What You Can Reproduce\n",
    "\n",
    "| Analysis | Reproducible | Data Required |\n",
    "|----------|--------------|---------------|\n",
    "| Classification training | ✅ Yes | `foundation_plr_features.db` |\n",
    "| Statistical analyses | ✅ Yes | `foundation_plr_results.db` |\n",
    "| Calibration curves | ✅ Yes | `foundation_plr_results.db` |\n",
    "| Decision curve analysis | ✅ Yes | `foundation_plr_results.db` |\n",
    "| Uncertainty propagation | ✅ Yes | `foundation_plr_features.db` |\n",
    "| Publication figures | ✅ Yes | Either database |\n",
    "\n",
    "### What Cannot Be Reproduced\n",
    "\n",
    "Due to clinical data privacy restrictions:\n",
    "- Raw PLR signal preprocessing\n",
    "- Anomaly detection model training\n",
    "- Imputation model training\n",
    "- Feature extraction from raw signals\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install duckdb pandas numpy scipy scikit-learn matplotlib seaborn\n",
    "pip install xgboost catboost statsmodels pingouin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Optional: Advanced classifiers\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "    print(\"CatBoost not available. Install with: pip install catboost\")\n",
    "\n",
    "# Project imports (add project root to path)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Foundation PLR statistics modules\n",
    "from src.stats import (\n",
    "    # Effect sizes\n",
    "    cohens_d, hedges_g, partial_eta_squared,\n",
    "    # Multiple comparison correction\n",
    "    benjamini_hochberg, bonferroni,\n",
    "    # Bootstrap\n",
    "    bca_bootstrap_ci, percentile_bootstrap_ci,\n",
    "    # Calibration\n",
    "    calibration_slope_intercept, brier_decomposition,\n",
    "    # Clinical utility\n",
    "    net_benefit, decision_curve_analysis,\n",
    "    # Uncertainty propagation\n",
    "    monte_carlo_classifier_uncertainty,\n",
    "    clinical_decision_stability,\n",
    "    sensitivity_analysis_delta,\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading Data from DuckDB\n",
    "\n",
    "The shared databases contain:\n",
    "- **`foundation_plr_features.db`**: Hand-crafted PLR features (de-identified)\n",
    "- **`foundation_plr_results.db`**: All classifier predictions and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to shared databases\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "FEATURES_DB = DATA_DIR / \"foundation_plr_features.db\"\n",
    "RESULTS_DB = DATA_DIR / \"foundation_plr_results.db\"\n",
    "\n",
    "# Check which databases are available\n",
    "print(\"Checking data availability:\")\n",
    "print(f\"  Features DB: {'✅ Found' if FEATURES_DB.exists() else '❌ Not found'}\")\n",
    "print(f\"  Results DB:  {'✅ Found' if RESULTS_DB.exists() else '❌ Not found'}\")\n",
    "\n",
    "if not FEATURES_DB.exists() and not RESULTS_DB.exists():\n",
    "    print(\"\\n⚠️  Neither database found. Download from [repository URL] or run export script.\")\n",
    "    print(\"   This tutorial will use synthetic demo data instead.\")\n",
    "    USE_DEMO_DATA = True\n",
    "else:\n",
    "    USE_DEMO_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploring the Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_duckdb(db_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Explore a DuckDB database and return schema information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : Path\n",
    "        Path to DuckDB database file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Database schema information\n",
    "    \"\"\"\n",
    "    if not db_path.exists():\n",
    "        return {\"error\": \"Database not found\"}\n",
    "    \n",
    "    with duckdb.connect(str(db_path), read_only=True) as con:\n",
    "        # Get all tables\n",
    "        tables = con.execute(\"SHOW TABLES\").df()\n",
    "        \n",
    "        schema = {}\n",
    "        for table_name in tables['name']:\n",
    "            # Get column info\n",
    "            columns = con.execute(f\"DESCRIBE {table_name}\").df()\n",
    "            \n",
    "            # Get row count\n",
    "            count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "            \n",
    "            schema[table_name] = {\n",
    "                'columns': columns['column_name'].tolist(),\n",
    "                'types': columns['column_type'].tolist(),\n",
    "                'row_count': count\n",
    "            }\n",
    "    \n",
    "    return schema\n",
    "\n",
    "# Explore available databases\n",
    "if FEATURES_DB.exists():\n",
    "    print(\"\\n=== Features Database Schema ===\")\n",
    "    features_schema = explore_duckdb(FEATURES_DB)\n",
    "    for table, info in features_schema.items():\n",
    "        print(f\"\\nTable: {table} ({info['row_count']:,} rows)\")\n",
    "        print(f\"  Columns: {', '.join(info['columns'][:5])}...\" if len(info['columns']) > 5 else f\"  Columns: {', '.join(info['columns'])}\")\n",
    "\n",
    "if RESULTS_DB.exists():\n",
    "    print(\"\\n=== Results Database Schema ===\")\n",
    "    results_schema = explore_duckdb(RESULTS_DB)\n",
    "    for table, info in results_schema.items():\n",
    "        print(f\"\\nTable: {table} ({info['row_count']:,} rows)\")\n",
    "        print(f\"  Columns: {', '.join(info['columns'][:5])}...\" if len(info['columns']) > 5 else f\"  Columns: {', '.join(info['columns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading Features for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(\n",
    "    db_path: Path,\n",
    "    source_name: str = None,\n",
    "    split: str = None,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load features and labels from DuckDB.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : Path\n",
    "        Path to features database\n",
    "    source_name : str, optional\n",
    "        Filter by pipeline configuration\n",
    "    split : str, optional\n",
    "        Filter by data split ('train', 'val', 'test')\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Labels (n_samples,)\n",
    "    feature_names : list\n",
    "        Feature column names\n",
    "    metadata : pd.DataFrame\n",
    "        Subject metadata (subject_id, eye, split)\n",
    "    \"\"\"\n",
    "    with duckdb.connect(str(db_path), read_only=True) as con:\n",
    "        # Build query with optional filters\n",
    "        query = \"\"\"\n",
    "            SELECT f.*, m.has_glaucoma, m.split, m.subject_id, m.eye\n",
    "            FROM plr_features f\n",
    "            JOIN feature_metadata m\n",
    "                ON f.subject_id = m.subject_id AND f.eye = m.eye\n",
    "            WHERE 1=1\n",
    "        \"\"\"\n",
    "        \n",
    "        if source_name:\n",
    "            query += f\" AND f.source_name = '{source_name}'\"\n",
    "        if split:\n",
    "            query += f\" AND m.split = '{split}'\"\n",
    "            \n",
    "        df = con.execute(query).df()\n",
    "    \n",
    "    # Separate metadata from features\n",
    "    metadata_cols = ['subject_id', 'eye', 'source_name', 'has_glaucoma', 'split']\n",
    "    feature_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "    \n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y = df['has_glaucoma'].values.astype(int)\n",
    "    metadata = df[['subject_id', 'eye', 'split']]\n",
    "    \n",
    "    return X, y, feature_cols, metadata\n",
    "\n",
    "\n",
    "def create_demo_data(n_samples: int = 200, n_features: int = 15, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Create synthetic demo data for tutorial demonstration.\n",
    "    \n",
    "    The synthetic features mimic PLR characteristics:\n",
    "    - Baseline diameter (normally ~3-6 mm)\n",
    "    - Constriction amplitude (typically 0.5-2.0 mm)\n",
    "    - Latency features (typically 200-500 ms)\n",
    "    - Velocity features (mm/s)\n",
    "    - PIPR features (percentage)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    \n",
    "    # Class labels with ~30% prevalence (like glaucoma in clinical settings)\n",
    "    y = rng.binomial(1, 0.3, n_samples)\n",
    "    \n",
    "    # Feature names mimicking PLR features\n",
    "    feature_names = [\n",
    "        'baseline_diameter', 'constriction_amplitude', 'constriction_amplitude_rel',\n",
    "        'max_constriction_diameter', 'latency_to_constriction', 'latency_75pct',\n",
    "        'time_to_redilation', 'max_constriction_velocity', 'mean_constriction_velocity',\n",
    "        'max_redilation_velocity', 'pipr_6s', 'pipr_10s', 'recovery_time',\n",
    "        'constriction_duration', 'auc_constriction'\n",
    "    ][:n_features]\n",
    "    \n",
    "    # Generate correlated features with class-dependent means\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Glaucoma typically shows: smaller PIPR, slower velocities, reduced amplitudes\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:  # Glaucoma\n",
    "            base_shift = -0.3\n",
    "        else:  # Control\n",
    "            base_shift = 0.3\n",
    "        \n",
    "        X[i, :] = rng.normal(base_shift, 1.0, n_features)\n",
    "        \n",
    "        # Add realistic feature correlations\n",
    "        X[i, 0] = rng.normal(4.5 + base_shift * 0.5, 0.8)  # baseline_diameter ~3-6mm\n",
    "        X[i, 1] = rng.normal(1.2 + base_shift * 0.3, 0.4)  # constriction_amplitude\n",
    "        X[i, 4] = rng.normal(350 - base_shift * 30, 50)     # latency ~250-450ms\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = pd.DataFrame({\n",
    "        'subject_id': [f'S{i:04d}' for i in range(n_samples)],\n",
    "        'eye': rng.choice(['OD', 'OS'], n_samples),\n",
    "        'split': np.where(\n",
    "            np.arange(n_samples) < int(0.7 * n_samples),\n",
    "            'train',\n",
    "            np.where(np.arange(n_samples) < int(0.85 * n_samples), 'val', 'test')\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    return X.astype(np.float32), y, feature_names, metadata\n",
    "\n",
    "\n",
    "# Load data (real or demo)\n",
    "if USE_DEMO_DATA or not FEATURES_DB.exists():\n",
    "    print(\"Using synthetic demo data...\")\n",
    "    X, y, feature_names, metadata = create_demo_data(n_samples=300)\n",
    "else:\n",
    "    print(f\"Loading features from {FEATURES_DB}...\")\n",
    "    X, y, feature_names, metadata = load_features(FEATURES_DB)\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Class distribution: {np.bincount(y)} (Control: {np.sum(y==0)}, Glaucoma: {np.sum(y==1)})\")\n",
    "print(f\"  Features: {feature_names[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Re-running Classification\n",
    "\n",
    "This section demonstrates training classifiers on the shared features and comparing results with the published values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifiers() -> dict:\n",
    "    \"\"\"\n",
    "    Get dictionary of classifiers used in the study.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping from classifier name to (model, hyperparameters)\n",
    "    \"\"\"\n",
    "    classifiers = {\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            solver='lbfgs',\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    if HAS_XGBOOST:\n",
    "        classifiers['XGBoost'] = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    if HAS_CATBOOST:\n",
    "        classifiers['CatBoost'] = CatBoostClassifier(\n",
    "            iterations=100,\n",
    "            depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "classifiers = get_classifiers()\n",
    "print(f\"Available classifiers: {list(classifiers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Validated Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    clf,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_folds: int = 5,\n",
    "    n_bootstrap: int = 1000,\n",
    "    random_state: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive classifier evaluation with bootstrap confidence intervals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : sklearn estimator\n",
    "        Classifier with fit/predict_proba methods\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray\n",
    "        Labels\n",
    "    n_folds : int\n",
    "        Number of CV folds\n",
    "    n_bootstrap : int\n",
    "        Bootstrap iterations for CIs\n",
    "    random_state : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Evaluation results with metrics and CIs\n",
    "    \"\"\"\n",
    "    from sklearn.base import clone\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Storage for predictions\n",
    "    y_prob_all = np.zeros(len(y))\n",
    "    y_pred_all = np.zeros(len(y), dtype=int)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Clone and fit\n",
    "        model = clone(clf)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        \n",
    "        y_prob_all[test_idx] = y_prob\n",
    "        y_pred_all[test_idx] = y_pred\n",
    "        \n",
    "        # Fold metrics\n",
    "        fold_metrics.append({\n",
    "            'fold': fold,\n",
    "            'auroc': roc_auc_score(y_test, y_prob),\n",
    "            'auprc': average_precision_score(y_test, y_prob),\n",
    "            'brier': brier_score_loss(y_test, y_prob),\n",
    "        })\n",
    "    \n",
    "    # Overall metrics\n",
    "    auroc = roc_auc_score(y, y_prob_all)\n",
    "    auprc = average_precision_score(y, y_prob_all)\n",
    "    brier = brier_score_loss(y, y_prob_all)\n",
    "    \n",
    "    # Bootstrap CIs\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    boot_aurocs = []\n",
    "    boot_auprcs = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.choice(len(y), len(y), replace=True)\n",
    "        if len(np.unique(y[idx])) < 2:\n",
    "            continue\n",
    "        boot_aurocs.append(roc_auc_score(y[idx], y_prob_all[idx]))\n",
    "        boot_auprcs.append(average_precision_score(y[idx], y_prob_all[idx]))\n",
    "    \n",
    "    return {\n",
    "        'auroc': auroc,\n",
    "        'auroc_ci': (np.percentile(boot_aurocs, 2.5), np.percentile(boot_aurocs, 97.5)),\n",
    "        'auprc': auprc,\n",
    "        'auprc_ci': (np.percentile(boot_auprcs, 2.5), np.percentile(boot_auprcs, 97.5)),\n",
    "        'brier': brier,\n",
    "        'y_prob': y_prob_all,\n",
    "        'y_pred': y_pred_all,\n",
    "        'fold_metrics': pd.DataFrame(fold_metrics),\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate all classifiers\n",
    "results = {}\n",
    "print(\"Evaluating classifiers (5-fold CV with bootstrap CIs)...\\n\")\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"  {name}...\", end=' ')\n",
    "    results[name] = evaluate_classifier(clf, X, y, n_bootstrap=500)\n",
    "    auroc = results[name]['auroc']\n",
    "    ci = results[name]['auroc_ci']\n",
    "    print(f\"AUROC = {auroc:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary table\n",
    "summary_data = []\n",
    "for name, res in results.items():\n",
    "    summary_data.append({\n",
    "        'Classifier': name,\n",
    "        'AUROC': f\"{res['auroc']:.4f}\",\n",
    "        '95% CI': f\"[{res['auroc_ci'][0]:.4f}, {res['auroc_ci'][1]:.4f}]\",\n",
    "        'AUPRC': f\"{res['auprc']:.4f}\",\n",
    "        'Brier Score': f\"{res['brier']:.4f}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Statistical Analyses\n",
    "\n",
    "This section demonstrates the biostatistical analyses used in the publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Calibration Analysis (STRATOS-Compliant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_calibration(y_true: np.ndarray, y_prob: np.ndarray, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive calibration analysis following STRATOS guidelines.\n",
    "    \n",
    "    Reports:\n",
    "    - Calibration slope (target = 1.0)\n",
    "    - Calibration intercept (calibration-in-the-large)\n",
    "    - E:O ratio (Expected:Observed)\n",
    "    - Brier score decomposition\n",
    "    \"\"\"\n",
    "    # STRATOS-compliant calibration metrics\n",
    "    cal_result = calibration_slope_intercept(y_true, y_prob)\n",
    "    brier_result = brier_decomposition(y_true, y_prob)\n",
    "    \n",
    "    print(f\"\\nCalibration Analysis: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Calibration slope:     {cal_result.slope:.4f}\")\n",
    "    print(f\"  Interpretation:      {'Well-calibrated' if 0.8 < cal_result.slope < 1.2 else 'Miscalibrated'}\")\n",
    "    print(f\"  (slope < 1 = overfitting, slope > 1 = underfitting)\")\n",
    "    print(f\"\")\n",
    "    print(f\"Calibration intercept: {cal_result.intercept:.4f}\")\n",
    "    print(f\"E:O ratio:             {cal_result.e_o_ratio:.4f}\")\n",
    "    print(f\"  (E:O > 1 = over-prediction, E:O < 1 = under-prediction)\")\n",
    "    print(f\"\")\n",
    "    print(f\"Brier score:           {cal_result.brier_score:.4f}\")\n",
    "    print(f\"  - Reliability:       {brier_result.scalars.get('reliability', np.nan):.4f}\")\n",
    "    print(f\"  - Resolution:        {brier_result.scalars.get('resolution', np.nan):.4f}\")\n",
    "    print(f\"  - Uncertainty:       {brier_result.scalars.get('uncertainty', np.nan):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'slope': cal_result.slope,\n",
    "        'intercept': cal_result.intercept,\n",
    "        'e_o_ratio': cal_result.e_o_ratio,\n",
    "        'brier': cal_result.brier_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze calibration for each classifier\n",
    "calibration_results = {}\n",
    "for name, res in results.items():\n",
    "    calibration_results[name] = analyze_calibration(y, res['y_prob'], name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curves(results: dict, y_true: np.ndarray, n_bins: int = 10):\n",
    "    \"\"\"\n",
    "    Plot calibration curves for all classifiers.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Left: Calibration curves\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated', alpha=0.7)\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(results)))\n",
    "    for (name, res), color in zip(results.items(), colors):\n",
    "        prob_true, prob_pred = calibration_curve(y_true, res['y_prob'], n_bins=n_bins, strategy='uniform')\n",
    "        ax1.plot(prob_pred, prob_true, 's-', color=color, label=name, markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Mean predicted probability')\n",
    "    ax1.set_ylabel('Observed proportion')\n",
    "    ax1.set_title('Calibration Curves')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Right: Prediction distribution\n",
    "    ax2 = axes[1]\n",
    "    for (name, res), color in zip(results.items(), colors):\n",
    "        ax2.hist(res['y_prob'][y_true == 0], bins=20, alpha=0.5, color=color, \n",
    "                 label=f'{name} (Control)', density=True)\n",
    "        ax2.hist(res['y_prob'][y_true == 1], bins=20, alpha=0.5, color=color,\n",
    "                 density=True, hatch='//', edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Predicted probability')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Prediction Distributions by True Class')\n",
    "    ax2.legend(['Control', 'Glaucoma'], loc='upper center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_ROOT / 'notebooks' / 'calibration_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: notebooks/calibration_curves.png\")\n",
    "\n",
    "plot_calibration_curves(results, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decision Curve Analysis (Clinical Utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_curves(results: dict, y_true: np.ndarray):\n",
    "    \"\"\"\n",
    "    Decision Curve Analysis for clinical utility assessment.\n",
    "    \n",
    "    For glaucoma screening:\n",
    "    - Threshold range: 1% to 30%\n",
    "    - Lower thresholds favor sensitivity (screening)\n",
    "    - Higher thresholds favor specificity (confirmation)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Threshold range relevant for glaucoma screening\n",
    "    threshold_range = (0.01, 0.30)\n",
    "    \n",
    "    # Baseline strategies\n",
    "    thresholds = np.linspace(threshold_range[0], threshold_range[1], 30)\n",
    "    prevalence = np.mean(y_true)\n",
    "    \n",
    "    # Treat-all net benefit\n",
    "    nb_all = [prevalence - (1 - prevalence) * (pt / (1 - pt)) for pt in thresholds]\n",
    "    nb_all = np.maximum(nb_all, 0)  # Cap at 0\n",
    "    \n",
    "    ax.plot(thresholds * 100, nb_all, 'k--', label='Treat All', linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle=':', label='Treat None', linewidth=2)\n",
    "    \n",
    "    # Model net benefits\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(results)))\n",
    "    for (name, res), color in zip(results.items(), colors):\n",
    "        dca_df = decision_curve_analysis(\n",
    "            y_true, res['y_prob'],\n",
    "            threshold_range=threshold_range\n",
    "        )\n",
    "        ax.plot(dca_df['threshold'] * 100, dca_df['nb_model'], \n",
    "                color=color, label=name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Threshold Probability (%)', fontsize=12)\n",
    "    ax.set_ylabel('Net Benefit', fontsize=12)\n",
    "    ax.set_title('Decision Curve Analysis for Glaucoma Screening', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlim([threshold_range[0] * 100, threshold_range[1] * 100])\n",
    "    ax.set_ylim([-0.05, max(0.15, prevalence + 0.05)])\n",
    "    \n",
    "    # Add clinical interpretation\n",
    "    ax.axvspan(1, 10, alpha=0.1, color='green', label='_Screening zone')\n",
    "    ax.axvspan(10, 30, alpha=0.1, color='blue', label='_Diagnosis zone')\n",
    "    ax.text(5, ax.get_ylim()[1] * 0.9, 'Screening', ha='center', fontsize=9, color='darkgreen')\n",
    "    ax.text(20, ax.get_ylim()[1] * 0.9, 'Diagnosis', ha='center', fontsize=9, color='darkblue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_ROOT / 'notebooks' / 'decision_curve_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: notebooks/decision_curve_analysis.png\")\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  - A model provides clinical utility when its curve is above both baselines\")\n",
    "    print(\"  - Net benefit = (TP/n) - (FP/n) × (threshold / (1 - threshold))\")\n",
    "    print(f\"  - Prevalence in this data: {prevalence:.1%}\")\n",
    "\n",
    "plot_decision_curves(results, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Effect Sizes with Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classifier_comparison_effect_sizes(results: dict):\n",
    "    \"\"\"\n",
    "    Compute effect sizes (Cohen's d) for pairwise classifier comparisons.\n",
    "    \n",
    "    Uses fold-level AUROC values for comparison.\n",
    "    \"\"\"\n",
    "    classifiers = list(results.keys())\n",
    "    comparisons = []\n",
    "    \n",
    "    for i, clf1 in enumerate(classifiers):\n",
    "        for clf2 in classifiers[i+1:]:\n",
    "            # Get fold-level AUROCs\n",
    "            aurocs1 = results[clf1]['fold_metrics']['auroc'].values\n",
    "            aurocs2 = results[clf2]['fold_metrics']['auroc'].values\n",
    "            \n",
    "            # Compute Cohen's d\n",
    "            effect = cohens_d(aurocs1, aurocs2)\n",
    "            \n",
    "            comparisons.append({\n",
    "                'Comparison': f\"{clf1} vs {clf2}\",\n",
    "                'Mean Diff': f\"{np.mean(aurocs1) - np.mean(aurocs2):.4f}\",\n",
    "                \"Cohen's d\": f\"{effect.effect_size:.3f}\",\n",
    "                '95% CI': f\"[{effect.ci_lower:.3f}, {effect.ci_upper:.3f}]\",\n",
    "                'Interpretation': effect.interpretation,\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(comparisons)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLASSIFIER COMPARISON: EFFECT SIZES\")\n",
    "    print(\"=\"*70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nInterpretation guide:\")\n",
    "    print(\"  |d| < 0.2: negligible\")\n",
    "    print(\"  0.2 ≤ |d| < 0.5: small\")\n",
    "    print(\"  0.5 ≤ |d| < 0.8: medium\")\n",
    "    print(\"  |d| ≥ 0.8: large\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if len(results) > 1:\n",
    "    effect_sizes_df = compute_classifier_comparison_effect_sizes(results)\n",
    "else:\n",
    "    print(\"Need multiple classifiers for effect size comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Uncertainty Propagation Analysis\n",
    "\n",
    "This section demonstrates how feature uncertainty affects clinical decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Monte Carlo Uncertainty Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_uncertainty_analysis(\n",
    "    X: np.ndarray,\n",
    "    clf,\n",
    "    uncertainty_level: float = 0.1,\n",
    "    n_simulations: int = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo uncertainty propagation analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    clf : trained classifier\n",
    "        Must have predict_proba method\n",
    "    uncertainty_level : float\n",
    "        Feature uncertainty as fraction of feature std\n",
    "    n_simulations : int\n",
    "        Number of MC simulations\n",
    "    \"\"\"\n",
    "    # Estimate feature uncertainties (fraction of feature standard deviation)\n",
    "    feature_stds = np.std(X, axis=0)\n",
    "    uncertainties = feature_stds * uncertainty_level\n",
    "    \n",
    "    print(f\"\\nRunning uncertainty analysis...\")\n",
    "    print(f\"  Feature uncertainty: {uncertainty_level:.0%} of feature std\")\n",
    "    print(f\"  MC simulations: {n_simulations}\")\n",
    "    \n",
    "    # Define prediction function\n",
    "    def predict_fn(X_perturbed):\n",
    "        return clf.predict_proba(X_perturbed)[:, 1]\n",
    "    \n",
    "    # Run MC simulation\n",
    "    mc_result = monte_carlo_classifier_uncertainty(\n",
    "        X, uncertainties, predict_fn,\n",
    "        n_simulations=n_simulations,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Analyze decision stability\n",
    "    stability_result = clinical_decision_stability(\n",
    "        mc_result.arrays['all_predictions'],\n",
    "        threshold=0.5,\n",
    "        stability_criterion='majority'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Mean prediction uncertainty (std): {mc_result.scalars['mean_prediction_std']:.4f}\")\n",
    "    print(f\"  Max prediction uncertainty (std): {mc_result.scalars['max_prediction_std']:.4f}\")\n",
    "    print(f\"  Decision stability: {stability_result.decision_stability_pct:.1f}%\")\n",
    "    print(f\"  Subjects with unstable decisions: {stability_result.n_unstable}\")\n",
    "    \n",
    "    return mc_result, stability_result\n",
    "\n",
    "\n",
    "# Train a classifier for uncertainty analysis\n",
    "best_clf_name = max(results.keys(), key=lambda k: results[k]['auroc'])\n",
    "print(f\"Using {best_clf_name} for uncertainty analysis...\")\n",
    "\n",
    "# Fit on full data for this analysis\n",
    "from sklearn.base import clone\n",
    "trained_clf = clone(classifiers[best_clf_name])\n",
    "trained_clf.fit(X, y)\n",
    "\n",
    "# Run uncertainty analysis at different uncertainty levels\n",
    "uncertainty_levels = [0.05, 0.10, 0.20]\n",
    "uncertainty_results = {}\n",
    "\n",
    "for level in uncertainty_levels:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"UNCERTAINTY LEVEL: {level:.0%}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    mc_result, stability_result = run_uncertainty_analysis(X, trained_clf, level)\n",
    "    uncertainty_results[level] = (mc_result, stability_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sensitivity_analysis(X: np.ndarray, clf, feature_names: list):\n",
    "    \"\"\"\n",
    "    Identify which features most affect predictions under uncertainty.\n",
    "    \"\"\"\n",
    "    # Feature uncertainties\n",
    "    uncertainties = np.std(X, axis=0) * 0.1\n",
    "    \n",
    "    def predict_fn(X_perturbed):\n",
    "        return clf.predict_proba(X_perturbed)[:, 1]\n",
    "    \n",
    "    # Run sensitivity analysis\n",
    "    sensitivity = sensitivity_analysis_delta(\n",
    "        X, uncertainties, predict_fn,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Sort by sensitivity\n",
    "    sorted_idx = np.argsort(sensitivity.sensitivity_normalized)[::-1]\n",
    "    sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "    sorted_sens = sensitivity.sensitivity_normalized[sorted_idx]\n",
    "    \n",
    "    colors = plt.cm.RdYlGn_r(sorted_sens / max(sorted_sens))\n",
    "    bars = ax.barh(range(len(sorted_names)), sorted_sens, color=colors)\n",
    "    ax.set_yticks(range(len(sorted_names)))\n",
    "    ax.set_yticklabels(sorted_names)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Normalized Sensitivity Index')\n",
    "    ax.set_title('Feature Sensitivity Analysis\\n(Which features most affect predictions under uncertainty)')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, sorted_sens)):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{val:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_ROOT / 'notebooks' / 'sensitivity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: notebooks/sensitivity_analysis.png\")\n",
    "    \n",
    "    print(f\"\\nMost influential feature: {sensitivity.most_influential}\")\n",
    "    print(f\"Top 3 features account for {sum(sorted_sens[:3]):.1%} of total sensitivity\")\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "sensitivity = run_sensitivity_analysis(X, trained_clf, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Extending the Analysis\n",
    "\n",
    "This section shows how to add custom metrics, classifiers, or statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Adding a Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youden_j_index(y_true: np.ndarray, y_prob: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute Youden's J statistic and optimal threshold.\n",
    "    \n",
    "    J = Sensitivity + Specificity - 1\n",
    "    \n",
    "    The optimal threshold maximizes J.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    \n",
    "    # Youden's J\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    \n",
    "    return {\n",
    "        'youden_j': j_scores[optimal_idx],\n",
    "        'optimal_threshold': thresholds[optimal_idx],\n",
    "        'sensitivity_at_optimal': tpr[optimal_idx],\n",
    "        'specificity_at_optimal': 1 - fpr[optimal_idx],\n",
    "    }\n",
    "\n",
    "# Apply custom metric to all classifiers\n",
    "print(\"\\nYouden's J Index Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "for name, res in results.items():\n",
    "    j_result = youden_j_index(y, res['y_prob'])\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Youden's J: {j_result['youden_j']:.4f}\")\n",
    "    print(f\"  Optimal threshold: {j_result['optimal_threshold']:.4f}\")\n",
    "    print(f\"  At optimal: Sens={j_result['sensitivity_at_optimal']:.3f}, Spec={j_result['specificity_at_optimal']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Adding a Custom Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delong_test(y_true: np.ndarray, y_prob1: np.ndarray, y_prob2: np.ndarray):\n",
    "    \"\"\"\n",
    "    DeLong test for comparing two ROC curves.\n",
    "    \n",
    "    Tests H0: AUROC1 = AUROC2\n",
    "    \n",
    "    Note: This is a simplified implementation. For publication,\n",
    "    consider using the 'pROC' R package via rpy2.\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    auroc1 = roc_auc_score(y_true, y_prob1)\n",
    "    auroc2 = roc_auc_score(y_true, y_prob2)\n",
    "    \n",
    "    # Bootstrap for variance estimate\n",
    "    rng = np.random.default_rng(42)\n",
    "    n_boot = 1000\n",
    "    diffs = []\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.choice(len(y_true), len(y_true), replace=True)\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        a1 = roc_auc_score(y_true[idx], y_prob1[idx])\n",
    "        a2 = roc_auc_score(y_true[idx], y_prob2[idx])\n",
    "        diffs.append(a1 - a2)\n",
    "    \n",
    "    # Test statistic\n",
    "    se = np.std(diffs)\n",
    "    z = (auroc1 - auroc2) / se if se > 0 else 0\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    \n",
    "    return {\n",
    "        'auroc1': auroc1,\n",
    "        'auroc2': auroc2,\n",
    "        'difference': auroc1 - auroc2,\n",
    "        'z_statistic': z,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# Compare classifiers if we have multiple\n",
    "if len(results) >= 2:\n",
    "    clf_names = list(results.keys())\n",
    "    print(\"\\nDeLong Test (Comparing ROC Curves):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, name1 in enumerate(clf_names):\n",
    "        for name2 in clf_names[i+1:]:\n",
    "            test = delong_test(y, results[name1]['y_prob'], results[name2]['y_prob'])\n",
    "            sig = \"*\" if test['significant'] else \"\"\n",
    "            print(f\"\\n{name1} vs {name2}:\")\n",
    "            print(f\"  AUROC difference: {test['difference']:.4f}\")\n",
    "            print(f\"  p-value: {test['p_value']:.4f}{sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Exporting Results for Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_latex(results: dict, calibration_results: dict, output_path: Path):\n",
    "    \"\"\"\n",
    "    Export results as LaTeX tables for publication.\n",
    "    \"\"\"\n",
    "    # Main results table\n",
    "    latex_content = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Classification Performance Comparison}\n",
    "\\label{tab:classification-results}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "\\textbf{Classifier} & \\textbf{AUROC} & \\textbf{95\\% CI} & \\textbf{AUPRC} & \\textbf{Brier} \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for name, res in results.items():\n",
    "        auroc = res['auroc']\n",
    "        ci = res['auroc_ci']\n",
    "        auprc = res['auprc']\n",
    "        brier = res['brier']\n",
    "        latex_content += f\"{name} & {auroc:.3f} & [{ci[0]:.3f}, {ci[1]:.3f}] & {auprc:.3f} & {brier:.3f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_content += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(latex_content)\n",
    "    \n",
    "    print(f\"LaTeX table saved to: {output_path}\")\n",
    "    return latex_content\n",
    "\n",
    "# Export\n",
    "latex_output = export_results_to_latex(\n",
    "    results, \n",
    "    calibration_results,\n",
    "    PROJECT_ROOT / 'notebooks' / 'classification_results.tex'\n",
    ")\n",
    "print(\"\\nGenerated LaTeX:\")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary and Next Steps\n",
    "\n",
    "This tutorial demonstrated:\n",
    "\n",
    "1. **Loading shared data** from DuckDB databases\n",
    "2. **Re-running classification** with multiple algorithms\n",
    "3. **STRATOS-compliant calibration analysis**\n",
    "4. **Decision curve analysis** for clinical utility\n",
    "5. **Uncertainty propagation** via Monte Carlo simulation\n",
    "6. **Effect size computation** with confidence intervals\n",
    "7. **Extending the analysis** with custom metrics and tests\n",
    "\n",
    "### Where to Modify Code\n",
    "\n",
    "| Task | Location |\n",
    "|------|----------|\n",
    "| Add new classifier | `get_classifiers()` function |\n",
    "| Add new metric | `evaluate_classifier()` function |\n",
    "| Change statistical tests | `src/stats/` modules |\n",
    "| Modify visualizations | Individual plotting functions |\n",
    "| Export to different format | `export_results_to_latex()` function |\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use this code in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@article{foundation_plr_2026,\n",
    "  title={Foundation Models for Pupillary Light Reflex Analysis in Glaucoma Screening},\n",
    "  author={...},\n",
    "  journal={...},\n",
    "  year={2026}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
