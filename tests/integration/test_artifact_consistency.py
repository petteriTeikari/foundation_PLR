#!/usr/bin/env python3
"""
test_artifact_consistency.py - Integration tests for artifact consistency.

Validates that statistics are consistent across:
- JSON data files (generated by viz scripts)
- CSV data files (source data)
- LaTeX numbers.tex commands

CRITICAL ISSUES IDENTIFIED BY EXPERT PANEL:
1. Friedman test uses bootstrap iterations (n=1000) which are NOT independent
2. Different n values for different analyses need documentation
3. Subject counts differ by task (507 for preprocessing, 208 for classification)
"""

import json
from pathlib import Path

import pandas as pd
import pytest

pytestmark = pytest.mark.data


# =============================================================================
# Fixtures
# =============================================================================


PROJECT_ROOT = Path(__file__).parent.parent.parent


@pytest.fixture
def manuscript_root() -> Path:
    """Return the manuscript root directory."""
    # This path will need to be configured per environment
    path = Path(
        "/home/petteri/Dropbox/github-personal/sci-llm-writer/manuscripts/foundationPLR"
    )
    if not path.exists():
        pytest.skip(f"Manuscript directory not found (expected on CI): {path}")
    return path


@pytest.fixture
def generated_data_dir(manuscript_root) -> Path:
    """Return the generated data directory."""
    return manuscript_root / "figures" / "generated" / "data"


@pytest.fixture
def source_data_dir() -> Path:
    """Return the source data directory (in main repo, not manuscript)."""
    return PROJECT_ROOT / "data" / "public"


@pytest.fixture
def preprocessing_vs_auroc_json(generated_data_dir) -> dict:
    """Load preprocessing vs AUROC correlation data."""
    json_path = generated_data_dir / "fig_preprocessing_vs_auroc.json"
    if not json_path.exists():
        pytest.skip("fig_preprocessing_vs_auroc.json not found")
    return json.loads(json_path.read_text())


@pytest.fixture
def parallel_preprocessing_json(generated_data_dir) -> dict:
    """Load parallel coordinates data."""
    json_path = generated_data_dir / "fig_parallel_preprocessing.json"
    if not json_path.exists():
        pytest.skip("fig_parallel_preprocessing.json not found")
    return json.loads(json_path.read_text())


@pytest.fixture
def cd_comparison_json(generated_data_dir) -> dict:
    """Load CD diagram comparison data."""
    json_path = generated_data_dir / "cd_preprocessing_comparison.json"
    if not json_path.exists():
        pytest.skip("cd_preprocessing_comparison.json not found")
    return json.loads(json_path.read_text())


@pytest.fixture
def outlier_difficulty_json(generated_data_dir) -> dict:
    """Load outlier difficulty analysis data."""
    json_path = generated_data_dir / "fig_outlier_easy_vs_hard.json"
    if not json_path.exists():
        pytest.skip("fig_outlier_easy_vs_hard.json not found")
    return json.loads(json_path.read_text())


@pytest.fixture
def outlier_difficulty_csv(source_data_dir) -> pd.DataFrame:
    """Load outlier difficulty CSV source data."""
    csv_path = source_data_dir / "outlier_difficulty_analysis.csv"
    if not csv_path.exists():
        pytest.skip("outlier_difficulty_analysis.csv not found")
    return pd.read_csv(csv_path)


# =============================================================================
# Subject Count Consistency Tests
# =============================================================================


class TestSubjectCounts:
    """Verify subject counts are consistent and correctly documented."""

    def test_total_subjects_from_csv(self, outlier_difficulty_csv):
        """Total subjects should be 507."""
        assert len(outlier_difficulty_csv) == 507, (
            f"Expected 507 subjects, got {len(outlier_difficulty_csv)}"
        )

    def test_control_subjects_count(self, outlier_difficulty_csv):
        """Control subjects should be 152."""
        n_control = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "control"]
        )
        assert n_control == 152, f"Expected 152 control subjects, got {n_control}"

    def test_glaucoma_subjects_count(self, outlier_difficulty_csv):
        """Glaucoma subjects should be 56."""
        n_glaucoma = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "glaucoma"]
        )
        assert n_glaucoma == 56, f"Expected 56 glaucoma subjects, got {n_glaucoma}"

    def test_unknown_subjects_count(self, outlier_difficulty_csv):
        """Unknown subjects should be 299."""
        n_unknown = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "Unknown"]
        )
        assert n_unknown == 299, f"Expected 299 unknown subjects, got {n_unknown}"

    def test_labeled_subjects_sum(self, outlier_difficulty_csv):
        """Labeled subjects (control + glaucoma) should be 208."""
        n_control = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "control"]
        )
        n_glaucoma = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "glaucoma"]
        )
        n_labeled = n_control + n_glaucoma
        assert n_labeled == 208, f"Expected 208 labeled subjects, got {n_labeled}"

    def test_json_matches_csv_control(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """JSON control count should match CSV."""
        json_control = outlier_difficulty_json["by_class"]["control"]["n_subjects"]
        csv_control = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "control"]
        )
        assert json_control == csv_control, (
            f"JSON control={json_control}, CSV control={csv_control}"
        )

    def test_json_matches_csv_glaucoma(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """JSON glaucoma count should match CSV."""
        json_glaucoma = outlier_difficulty_json["by_class"]["glaucoma"]["n_subjects"]
        csv_glaucoma = len(
            outlier_difficulty_csv[outlier_difficulty_csv["class_label"] == "glaucoma"]
        )
        assert json_glaucoma == csv_glaucoma, (
            f"JSON glaucoma={json_glaucoma}, CSV glaucoma={csv_glaucoma}"
        )


# =============================================================================
# Correlation Value Consistency Tests
# =============================================================================


class TestCorrelationConsistency:
    """Verify correlation values are documented and explained."""

    def test_f1_auroc_correlation_value(self, preprocessing_vs_auroc_json):
        """F1→AUROC correlation should be approximately 0.421."""
        r = preprocessing_vs_auroc_json["panel_a"]["correlation_r"]
        assert abs(r - 0.421) < 0.001, f"Expected r≈0.421, got {r}"

    def test_f1_auroc_sample_size(self, preprocessing_vs_auroc_json):
        """F1→AUROC correlation should use n=40."""
        n = preprocessing_vs_auroc_json["panel_a"]["n_points"]
        assert n == 40, f"Expected n=40, got {n}"

    def test_mae_auroc_correlation_value(self, preprocessing_vs_auroc_json):
        """MAE→AUROC correlation should be approximately 0.138."""
        r = preprocessing_vs_auroc_json["panel_b"]["correlation_r"]
        assert abs(r - 0.138) < 0.001, f"Expected r≈0.138, got {r}"

    def test_mae_auroc_sample_size(self, preprocessing_vs_auroc_json):
        """MAE→AUROC correlation should use n=45."""
        n = preprocessing_vs_auroc_json["panel_b"]["n_points"]
        assert n == 45, f"Expected n=45, got {n}"

    def test_parallel_coords_different_n(
        self, preprocessing_vs_auroc_json, parallel_preprocessing_json
    ):
        """
        Parallel coordinates uses different n than scatter plot.

        This is EXPECTED because parallel coords requires complete data
        (both F1 and MAE for each config), while scatter allows pairwise complete.

        Document the difference explicitly.
        """
        scatter_f1_n = preprocessing_vs_auroc_json["panel_a"]["n_points"]
        parallel_n = parallel_preprocessing_json["n_configurations"]

        # These SHOULD be different - parallel requires complete data
        assert scatter_f1_n != parallel_n, (
            "Expected different n values between scatter and parallel coords"
        )

        # Document expected values
        assert parallel_n == 24, (
            f"Parallel coords expected n=24 complete configs, got {parallel_n}"
        )
        assert scatter_f1_n == 40, (
            f"Scatter expected n=40 F1 values, got {scatter_f1_n}"
        )

    def test_parallel_coords_correlation_documented(
        self, preprocessing_vs_auroc_json, parallel_preprocessing_json
    ):
        """
        Parallel coords shows different r because of different n.

        Scatter (n=40): r=0.421
        Parallel (n=24): r=0.407

        Both are valid, but the manuscript should use the scatter value (larger n).
        """
        parallel_r = parallel_preprocessing_json["correlations"]["auroc_vs_f1"]
        scatter_r = preprocessing_vs_auroc_json["panel_a"]["correlation_r"]

        # They should be different but within reasonable range
        assert abs(parallel_r - 0.407) < 0.01, (
            f"Parallel expected r≈0.407, got {parallel_r}"
        )
        assert abs(scatter_r - 0.421) < 0.01, (
            f"Scatter expected r≈0.421, got {scatter_r}"
        )

        # Scatter should be the canonical value (larger sample)
        assert scatter_r > parallel_r, (
            "Scatter r should be > parallel r (different subsets)"
        )


# =============================================================================
# Friedman Test Consistency Tests
# =============================================================================


class TestFriedmanStatistics:
    """Verify Friedman-Nemenyi test statistics."""

    def test_friedman_chi_square(self, cd_comparison_json):
        """Friedman chi-square should be approximately 833.10."""
        chi_sq = cd_comparison_json["friedman_stat"]
        assert abs(chi_sq - 833.10) < 0.1, f"Expected χ²≈833.10, got {chi_sq}"

    def test_nemenyi_critical_difference(self, cd_comparison_json):
        """Nemenyi CD should be approximately 0.415."""
        cd = cd_comparison_json["critical_difference"]
        assert abs(cd - 0.415) < 0.001, f"Expected CD≈0.415, got {cd}"

    def test_bootstrap_iterations(self, cd_comparison_json):
        """Should use 1000 bootstrap iterations."""
        n_iter = cd_comparison_json["n_iterations"]
        assert n_iter == 1000, f"Expected 1000 iterations, got {n_iter}"

    def test_n_pipelines(self, cd_comparison_json):
        """Should compare 12 preprocessing pipelines."""
        n_pipelines = cd_comparison_json["n_pipelines"]
        assert n_pipelines == 12, f"Expected 12 pipelines, got {n_pipelines}"

    def test_friedman_independence_warning(self, cd_comparison_json):
        """
        CRITICAL: Verify awareness of independence violation.

        The Friedman test uses n=1000 bootstrap iterations as "datasets",
        but bootstrap samples share ~63% overlap and are NOT independent.

        The p-value (1.48e-171) is suspiciously small due to this violation.

        This test documents the issue - results should be interpreted with caution.
        """
        p_value = cd_comparison_json["friedman_p"]
        n_iterations = cd_comparison_json["n_iterations"]

        # Flag: extremely small p-value with bootstrap samples
        assert p_value < 1e-100, (
            "P-value should be extremely small (this is a FLAG, not validation)"
        )

        # Document: bootstrap samples are NOT independent
        # In a proper test with k-fold CV (k=10), we'd expect n=10, not n=1000
        assert n_iterations == 1000, (
            f"Using {n_iterations} bootstrap iterations as 'datasets' - "
            "these are NOT independent! Interpret CD with caution."
        )


# =============================================================================
# Outlier Difficulty Consistency Tests
# =============================================================================


class TestOutlierDifficulty:
    """Verify outlier difficulty analysis consistency."""

    def test_total_outliers(self, outlier_difficulty_json):
        """Total outliers should be 76,991."""
        total = outlier_difficulty_json["total_outliers"]
        assert total == 76991, f"Expected 76991 outliers, got {total}"

    def test_easy_outliers_count(self, outlier_difficulty_json):
        """EASY outliers should be 12,727."""
        easy = outlier_difficulty_json["easy_count"]
        assert easy == 12727, f"Expected 12727 EASY outliers, got {easy}"

    def test_hard_outliers_count(self, outlier_difficulty_json):
        """HARD outliers should be 64,264."""
        hard = outlier_difficulty_json["hard_count"]
        assert hard == 64264, f"Expected 64264 HARD outliers, got {hard}"

    def test_easy_hard_sum(self, outlier_difficulty_json):
        """EASY + HARD should equal total."""
        total = outlier_difficulty_json["total_outliers"]
        easy = outlier_difficulty_json["easy_count"]
        hard = outlier_difficulty_json["hard_count"]
        assert easy + hard == total, f"EASY({easy}) + HARD({hard}) ≠ total({total})"

    def test_easy_percentage(self, outlier_difficulty_json):
        """EASY percentage should be approximately 16.5%."""
        easy_pct = outlier_difficulty_json["easy_pct"]
        assert abs(easy_pct - 16.5) < 0.1, f"Expected ~16.5%, got {easy_pct}%"

    def test_hard_percentage(self, outlier_difficulty_json):
        """HARD percentage should be approximately 83.5%."""
        hard_pct = outlier_difficulty_json["hard_pct"]
        assert abs(hard_pct - 83.5) < 0.1, f"Expected ~83.5%, got {hard_pct}%"

    def test_csv_json_easy_count_match(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """JSON easy_count should match sum from CSV."""
        json_easy = outlier_difficulty_json["easy_count"]
        csv_easy = outlier_difficulty_csv["easy_count"].sum()
        assert json_easy == csv_easy, f"JSON easy={json_easy}, CSV easy={csv_easy}"

    def test_csv_json_hard_count_match(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """JSON hard_count should match sum from CSV."""
        json_hard = outlier_difficulty_json["hard_count"]
        csv_hard = outlier_difficulty_csv["hard_count"].sum()
        assert json_hard == csv_hard, f"JSON hard={json_hard}, CSV hard={csv_hard}"


# =============================================================================
# Cross-Artifact Consistency Tests
# =============================================================================


class TestCrossArtifactConsistency:
    """Verify consistency across different artifact types."""

    def test_auroc_range_consistent(
        self, parallel_preprocessing_json, cd_comparison_json
    ):
        """AUROC range should be consistent across figures."""
        parallel_max = parallel_preprocessing_json["auroc_range"]["max"]

        # CD diagram should have similar max AUROC for top pipeline
        cd_aurocs = cd_comparison_json["pipeline_auroc"]
        cd_max = max(cd_aurocs.values())

        # Allow small tolerance due to different aggregations
        assert abs(parallel_max - cd_max) < 0.01, (
            f"Parallel max AUROC={parallel_max}, CD max AUROC={cd_max}"
        )

    def test_control_outlier_rate_consistent(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """Control outlier rate should match between JSON and CSV."""
        json_rate = outlier_difficulty_json["by_class"]["control"]["mean_outlier_pct"]

        # Calculate from CSV
        control_df = outlier_difficulty_csv[
            outlier_difficulty_csv["class_label"] == "control"
        ]
        csv_rate = control_df["outlier_pct"].mean()

        assert abs(json_rate - csv_rate) < 0.001, (
            f"JSON control rate={json_rate}, CSV control rate={csv_rate}"
        )

    def test_glaucoma_outlier_rate_consistent(
        self, outlier_difficulty_json, outlier_difficulty_csv
    ):
        """Glaucoma outlier rate should match between JSON and CSV."""
        json_rate = outlier_difficulty_json["by_class"]["glaucoma"]["mean_outlier_pct"]

        # Calculate from CSV
        glaucoma_df = outlier_difficulty_csv[
            outlier_difficulty_csv["class_label"] == "glaucoma"
        ]
        csv_rate = glaucoma_df["outlier_pct"].mean()

        assert abs(json_rate - csv_rate) < 0.001, (
            f"JSON glaucoma rate={json_rate}, CSV glaucoma rate={csv_rate}"
        )


# =============================================================================
# Main
# =============================================================================


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
