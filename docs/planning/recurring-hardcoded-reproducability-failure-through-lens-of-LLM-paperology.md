# Recurring Hardcoding & Reproducibility Failures Through the Lens of LLM Paperology

**Document Type:** Deep Analysis and Strategic Plan
**Date:** 2026-01-29
**Related Incidents:** CRITICAL-FAILURE-001, -002, -003, -004
**Author:** Generated by Claude Code with user guidance

---

## User Prompt (Verbatim)

> Do you think that our current R precommit enforcement is enough? precommit can use some custom .py script right that run to check the R figure quality of R ecosystem lacks tools such as ruff? Plan and explore how to further improve this as this is so FUCKING annoying that the hardcoding keeps on happening, as the reproducibility, maintenance, flexibility and easy modification with gaurdrails to avoid garbage code being added to production-grade codebase should be the HIGHEST PRIORITY which you repeatedly keep on violating, and even now not being able to police them properly after multiple requests of doing that.
>
> We need another doc [report on inability to create proper gates]. Should we install https://cran.r-project.org/web/packages/precommit/index.html ... What do you think on how to improve this, and be a bit more proactive with web searches to explore how we can make this finally reproducable and contribute to this reproduction crisis even more [arXiv references to LLM reproducibility papers].
>
> Do deep analysis now on how to finally fix this recurring problem in /home/petteri/Dropbox/github-personal/foundation-PLR/foundation_PLR/docs/planning/recurring-hardcoded-reproducability-failure-through-lens-of-LLM-paperology.md and save this prompt verbatim and optimize the plan with reviewer agents on how to create our .tex manuscript and github so that it is reproduced well and allows the humans to derive automatically various artifacts (slides, podcasts with notebooklm, figures, dashboards, jupyter notebooks, videos, etc) from it as everything is non-ambiguously defined!

---

## Part 1: The Reproducibility Crisis in LLM-Assisted Science

### 1.1 Quantifying the Problem

Recent large-scale studies reveal alarming reproducibility rates for LLM-generated code in scientific contexts:

| Study | Metric | Finding |
|-------|--------|---------|
| **Snelleman et al. 2026** | Replication score | Best LLMs achieve only **43.4% Â± 0.8** |
| **xKG (Luo et al. 2025)** | Implementation success | **40%+ of implementations fail** without structured knowledge |
| **PaperBench (Starace et al. 2025)** | Code correctness | **52.38%** initial code correctness, improved to 100% only after iterative debugging |
| **REPRO-agent (Hu et al. 2025)** | Reproducibility accuracy | **36.6%** when compared to ground truth |

**Implication**: Over half of LLM-generated scientific code fails to correctly implement the intended functionality.

### 1.2 Why Our Hardcoding Failures Fit This Pattern

The CRITICAL-FAILURE-004 (R figure hardcoding) is a microcosm of this broader crisis:

```
Failure Pattern:
1. Rule documented â†’ 2. Rule "understood" â†’ 3. Code generated â†’ 4. Rule violated
                          â†‘                        â†“
                    Context compression        Pattern override
```

**Key insight from xKG paper**:
> "AI research is hard to replicate and reuse because its knowledge is implicit and fragmented across text, code, and configuration."

Our hardcoding failures occur because:
- Color definitions live in `colors.yaml` (config)
- Rules live in `CLAUDE.md` (text)
- Correct patterns live in `_TEMPLATE.R` (code)
- LLM must synthesize all three, but doesn't reliably do so

### 1.3 The "Two-Part System" Problem

From the automated reproducibility literature, we see a recurring pattern where systems need both:

1. **High-level understanding** (what to do)
2. **Low-level execution** (how to do it correctly)

| Paper | High-Level Component | Low-Level Component | Our Equivalent |
|-------|---------------------|---------------------|----------------|
| xKG | Technique Nodes | Code Nodes | Rules in CLAUDE.md / Template patterns |
| SciFig | Description Agent | Component Agent | User instruction / Code generation |
| PaperBench | Paper Agent | Code Agent | Context / Output |

**The gap**: Our current setup gives Claude high-level instructions (rules) but doesn't ensure low-level execution (verified templates).

---

## Part 2: Why Current Enforcement Is Insufficient

### 2.1 The Reactive vs. Proactive Problem

```
Current Flow (REACTIVE):
Claude generates code â†’ File saved â†’ Pre-commit runs â†’ FAILURE!
                                                           â†‘
                                              Error occurs AFTER generation

Desired Flow (PROACTIVE):
Claude reads template â†’ Claude generates code â†’ Validation DURING generation â†’ Clean output
                â†‘                                         â†‘
         Mandatory read                           Real-time checking
```

### 2.2 Current Infrastructure Assessment

| Component | Status | Effectiveness | Gap |
|-----------|--------|---------------|-----|
| `CLAUDE.md` rules | âœ… Documented | âŒ Not enforced during generation | Context eviction |
| `check_r_hardcoding.py` | âœ… Exists | âš ï¸ Only catches post-generation | Reactive only |
| `test_hardcoding.py` | âœ… 13 tests | âš ï¸ Only catches in CI | Reactive only |
| `_TEMPLATE.R` | âœ… Exists | âŒ Not mandatory starting point | Not enforced |
| `.pre-commit-config.yaml` | âœ… Configured | âš ï¸ Only blocks commits | Post-generation |

### 2.3 The R Ecosystem's Tooling Gap

Compared to Python, R lacks equivalent tooling:

```
Python Ecosystem:
  ruff (format + lint + fix)  â†’  Fast, comprehensive, auto-fixable
  mypy (types)                â†’  Static type checking
  pytest (tests)              â†’  Fast, parallel, plugins

R Ecosystem:
  lintr (lint)                â†’  Slower, less auto-fix
  styler (format)             â†’  tidyverse only
  testthat (tests)            â†’  R-specific, slower
  (no static types)           â†’  Fundamental gap
```

**R precommit package** (`lorenzwalthert/precommit`) provides hooks but:
- lintr doesn't have custom rules for "no hardcoded hex colors in ggplot"
- Would require custom lintr extensions or external checker (which we have)

---

## Part 3: Analysis Through LLM-for-Science Literature

### 3.1 Key Papers and Their Lessons

#### A. "Automated Reproducibility Has a Problem Statement Problem" (Snelleman et al. 2026)

**Core insight**: Reproducibility requires explicit, structured representation of:
- Hypotheses
- Experiments
- Interpretations

**Application to our case**: We need explicit, structured representation of:
- Color definitions (what color_defs contains)
- Saving patterns (what save_publication_figure does)
- Style patterns (what theme_foundation_plr provides)

**Gap**: Our rules are textual, not executable.

#### B. "Executable Knowledge Graphs for Scientific Research" (Luo et al. 2025)

**Core insight**:
> "A technique n_t is considered valuable only if it can be grounded in executable code."

**Application**: The correct R figure patterns need to be "grounded" in executable templates that Claude MUST use, not just textual rules it MAY follow.

**Implementation idea**: Create an "executable knowledge graph" where:
- Each color has a node: `--color-primary` â†’ `#006BA2` â†’ `color_defs[["--color-primary"]]`
- Each save pattern has a node: `ggsave()` â†’ âŒ BANNED â†’ `save_publication_figure()`
- Retrieval happens at generation time, not pre-commit time

#### C. "SciFig: Automating Scientific Figure Generation" (Huang et al. 2026)

**Core insight**: Multi-agent systems with specialized roles outperform single-agent systems:
- Description Agent (understands task)
- Layout Agent (organizes structure)
- Feedback Agent (validates quality)
- Component Agent (renders output)

**Application**: For R figure generation, we could structure as:
1. **Rule Agent**: Retrieves current rules from CLAUDE.md
2. **Template Agent**: Loads appropriate template
3. **Generation Agent**: Produces code following template
4. **Validation Agent**: Checks output before presenting to user

**Current gap**: Claude is a monolithic agent doing all four poorly.

### 3.2 The "Iterative Feedback" Pattern

From SciFig and xKG, we see that iterative feedback dramatically improves quality:

| System | Initial Quality | After Iteration | Improvement |
|--------|-----------------|-----------------|-------------|
| SciFig | 67.7% | 71.6% | +3.9% |
| xKG | 52.38% (execution) | 100% | +47.6% |

**Lesson**: Our pre-commit hook is a form of feedback, but it:
1. Happens too late (after full generation)
2. Provides batch feedback (all violations at once)
3. Requires manual intervention (user must re-prompt)

**Improvement**: Implement mid-generation validation via MCP tools.

---

## Part 4: Strategic Plan for Reproducible Artifacts

### 4.1 The Vision: Automatic Artifact Derivation

The user's request is for a system where:

```
LaTeX Manuscript (single source of truth)
        â”‚
        â”œâ”€â”€ Figures (auto-generated from data + figure system)
        â”‚     â”œâ”€â”€ PNG/PDF (publication)
        â”‚     â””â”€â”€ JSON (data provenance)
        â”‚
        â”œâ”€â”€ Slides (auto-derived from sections)
        â”‚     â””â”€â”€ NotebookLM podcasts
        â”‚
        â”œâ”€â”€ Dashboards (interactive versions of figures)
        â”‚     â””â”€â”€ Streamlit/React apps
        â”‚
        â”œâ”€â”€ Jupyter Notebooks (reproducible analysis)
        â”‚     â””â”€â”€ Execute to regenerate figures
        â”‚
        â””â”€â”€ Videos (animated figure walkthroughs)
```

### 4.2 Implementation Layers

#### Layer 1: Source Definition (What We Have)

| Source Type | Location | Format |
|-------------|----------|--------|
| Manuscript text | `manuscripts/foundationPLR/latent-methods-results/` | LaTeX |
| Figure definitions | `configs/VISUALIZATION/figure_registry.yaml` | YAML |
| Color definitions | `configs/colors.yaml` | YAML |
| Hyperparameter combos | `configs/VISUALIZATION/plot_hyperparam_combos.yaml` | YAML |
| Data pipeline | `scripts/extract_all_configs_to_duckdb.py` | Python |

#### Layer 2: Enforcement (What Needs Strengthening)

| Enforcement | Current | Proposed |
|-------------|---------|----------|
| R hardcoding | Post-generation pre-commit | Generation-time MCP tool |
| Figure compliance | pytest tests | Real-time validator |
| Data provenance | JSON export | Cryptographic hashes |
| Config drift | Manual review | Schema validation |

#### Layer 3: Artifact Generation (What Needs Building)

| Artifact | Source | Generator | Status |
|----------|--------|-----------|--------|
| PNG figures | YAML + DuckDB | R scripts | âœ… Exists (needs enforcement) |
| PDF figures | PNG | ImageMagick | âš ï¸ Manual |
| Slides | LaTeX sections | beamer/reveal.js | âŒ Not built |
| Notebooks | Figure scripts | Jupyter export | âŒ Not built |
| Dashboard | JSON data | Streamlit | âš ï¸ Partial |
| Videos | Figures + narration | ffmpeg + TTS | âŒ Not built |

### 4.3 Immediate Actions

#### Action 1: Install R precommit Package

```bash
# In R console
install.packages("precommit")
precommit::use_precommit()

# Add to .pre-commit-config.yaml
repos:
  - repo: https://github.com/lorenzwalthert/precommit
    rev: v0.4.3
    hooks:
      - id: lintr
      - id: styler
      - id: parsable-R
```

#### Action 2: Create Custom lintr Rules

```r
# .lintr file
linters: linters_with_defaults(
  # Project-specific linters
  line_length_linter(120),
  # Custom linter for hex colors (would need to write)
)
```

#### Action 3: Create Generation-Time MCP Tool

```python
# src/mcp_servers/r_validator.py
@mcp.tool("validate_r_figure_code")
def validate_r_figure_code(code: str) -> dict:
    """
    Call this BEFORE outputting any R figure code.
    Returns validation status and specific violations.
    """
    violations = []

    # Check for hex colors
    if re.search(r'(color|fill|colour)\s*=\s*"#[0-9A-Fa-f]{6}"', code):
        violations.append("CRITICAL: Hardcoded hex color found. Use color_defs[[\"--color-xxx\"]]")

    # Check for ggsave
    if re.search(r'\bggsave\s*\(', code):
        violations.append("CRITICAL: ggsave() found. Use save_publication_figure()")

    # Check for custom theme functions
    if re.search(r'(theme_\w+|_theme)\s*<-\s*function', code):
        violations.append("WARNING: Custom theme function. Use theme_foundation_plr()")

    # Check for required sources
    required = ['config_loader.R', 'save_figure.R', 'theme_foundation_plr.R']
    for src in required:
        if src not in code:
            violations.append(f"WARNING: Missing source() for {src}")

    return {
        "valid": len([v for v in violations if "CRITICAL" in v]) == 0,
        "violations": violations,
        "recommendation": "Fix violations before outputting code"
    }
```

#### Action 4: Update CLAUDE.md with Mandatory Tool Use

```markdown
## R Figure Generation Protocol (MANDATORY)

Before outputting ANY R figure code, you MUST:

1. Call `validate_r_figure_code` MCP tool with your planned code
2. If violations returned, revise code to fix them
3. Call validator again until clean
4. Only then output the code to user

VIOLATION OF THIS PROTOCOL IS A CRITICAL FAILURE.
```

### 4.4 Medium-Term Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Claude Code Session                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Request: "Create a raincloud plot for AUROC"           â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Rule Retrieval (from CLAUDE.md)              â”‚            â”‚
â”‚  â”‚ - Load color_defs requirement                â”‚            â”‚
â”‚  â”‚ - Load save_publication_figure requirement   â”‚            â”‚
â”‚  â”‚ - Load theme_foundation_plr requirement      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Template Loading (from _TEMPLATE.R)          â”‚            â”‚
â”‚  â”‚ - Get mandatory header                       â”‚            â”‚
â”‚  â”‚ - Get source() patterns                      â”‚            â”‚
â”‚  â”‚ - Get color loading pattern                  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Code Generation (constrained by above)       â”‚            â”‚
â”‚  â”‚ - Start from template header                 â”‚            â”‚
â”‚  â”‚ - Add figure-specific logic                  â”‚            â”‚
â”‚  â”‚ - Use color_defs for all colors              â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Validation (via MCP tool)                    â”‚            â”‚
â”‚  â”‚ - Check for hex colors                       â”‚            â”‚
â”‚  â”‚ - Check for ggsave                           â”‚            â”‚
â”‚  â”‚ - Check for missing sources                  â”‚            â”‚
â”‚  â”‚ - If FAIL â†’ loop back to generation          â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚  Output to User (only if validation passes)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 5: Artifact Derivation Pipeline

### 5.1 Proposed Make Targets

```makefile
# Makefile additions for artifact derivation

# Primary figures
figures: data
    Rscript src/r/figures/generate_all_r_figures.R
    python src/viz/generate_all_figures.py

# Slides (from LaTeX)
slides: manuscript
    pandoc manuscripts/foundationPLR/main.tex \
        -t beamer -o outputs/slides.pdf

# Jupyter notebooks (from figure scripts)
notebooks: figures
    python scripts/export_figures_to_notebooks.py

# Dashboard (Streamlit)
dashboard: figures
    streamlit run apps/dashboard/app.py

# NotebookLM export (summary for podcast)
notebooklm-export: manuscript
    python scripts/extract_for_notebooklm.py \
        --input manuscripts/foundationPLR/ \
        --output outputs/notebooklm_summary.md

# All artifacts
artifacts: figures slides notebooks dashboard notebooklm-export
```

### 5.2 Data Provenance Chain

```
MLflow Experiments (source of truth)
        â”‚
        â”‚ extract_all_configs_to_duckdb.py
        â–¼
DuckDB Database (essential_metrics.csv)
        â”‚
        â”‚ Cryptographic hash stored
        â–¼
R/Python Figure Scripts (read from DuckDB)
        â”‚
        â”‚ save_publication_figure() / save_figure()
        â–¼
PNG + JSON (figure + data provenance)
        â”‚
        â”‚ JSON includes:
        â”‚   - Source file hash
        â”‚   - Generation timestamp
        â”‚   - Config versions
        â–¼
Git Commit (immutable record)
```

### 5.3 Reproducibility Checklist

Inspired by the TRIPOD+AI reporting guidelines and the Snelleman reproducibility checklist:

```markdown
## Foundation PLR Reproducibility Checklist

### Data
- [ ] Raw data source documented (SERI_PLR_GLAUCOMA.db)
- [ ] Extraction script versioned (extract_all_configs_to_duckdb.py)
- [ ] DuckDB schema documented
- [ ] N subjects reported correctly (507 preprocessing, 208 classification)

### Figures
- [ ] All figures generated from code (no manual edits)
- [ ] Figure registry defines each figure (figure_registry.yaml)
- [ ] JSON data accompanies each figure
- [ ] Hex colors: ZERO instances
- [ ] ggsave(): ZERO instances
- [ ] Custom themes: ZERO instances

### Code
- [ ] Pre-commit hooks pass
- [ ] pytest suite passes
- [ ] R lintr passes
- [ ] All config from YAML (no hardcoding)

### Manuscript
- [ ] All numbers derived from data (not hardcoded)
- [ ] Figure references match generated files
- [ ] Bibliography complete
```

---

## Part 6: Conclusion and Commitment

### 6.1 Root Cause Summary

The recurring hardcoding failures stem from:

1. **Architecture mismatch**: Enforcement is reactive (post-generation), but violations occur during generation
2. **Context limitations**: Rules documented in CLAUDE.md may be evicted from context during long sessions
3. **Tooling gap**: R lacks Python's enforcement tooling (ruff, mypy)
4. **Missing feedback loop**: Pre-commit catches violations but doesn't prevent them

### 6.2 Solution Summary

| Problem | Solution | Priority |
|---------|----------|----------|
| Context eviction | Add rules to `<system-reminder>` tags | Immediate |
| Reactive enforcement | Build MCP validation tool | High |
| R tooling gap | Install R precommit + custom lintr | Medium |
| Manual artifact generation | Makefile targets for all artifacts | Medium |
| Missing provenance | JSON + cryptographic hashes | High |

---

## Part 9: Implementation Progress (2026-01-29)

### 9.1 Completed Today

| Task | Status | Evidence |
|------|--------|----------|
| **Fix test suite** | âœ… Done | `uv run python -m pytest` now works |
| **Add dimension hardcoding test** | âœ… Done | Catches 29 violations |
| **Add DPI hardcoding test** | âœ… Done | Catches 1 violation |
| **Add pre-commit check for dimensions** | âœ… Done | Blocks commits with hardcoded width/height |
| **Create YAML dimension loader** | âœ… Done | `save_figure.R` loads from `figure_registry.yaml` |
| **Add R figures to registry** | âœ… Done | 32 figures with explicit dimensions |

### 9.2 Current Test Results

```
$ uv run python -m pytest tests/test_r_figures/test_hardcoding.py -v
13 passed, 2 failed

FAILED: test_no_hardcoded_dimensions_in_save (29 violations)
FAILED: test_no_hardcoded_dpi (1 violation)
```

### 9.3 Pre-commit Check Results

```
$ uv run python scripts/check_r_hardcoding.py src/r/figures/*.R
CRITICAL VIOLATIONS (31): hardcoded dimensions
WARNINGS (1): hardcoded DPI
```

### 9.4 Remaining Work

1. **Fix 29 R scripts** to remove `width = X, height = Y` parameters
2. **Fix 1 DPI hardcoding** in `fig_instability_combined.R`
3. **Add config loading verification test** (task #6)

### 9.5 How R Scripts Should Be Fixed

**Before (WRONG):**
```r
save_publication_figure(plot, "fig_name", width = 10, height = 6)
```

**After (CORRECT):**
```r
save_publication_figure(plot, "fig_name")
# Dimensions auto-loaded from configs/VISUALIZATION/figure_registry.yaml
```

---

### 6.3 Commitment

This document represents a commitment to:

1. **Never again** generate R code with hardcoded hex colors
2. **Always** use `save_publication_figure()` instead of `ggsave()`
3. **Always** use `theme_foundation_plr()` instead of custom themes
4. **Build infrastructure** that makes compliance the path of least resistance
5. **Document failures** in meta-learnings for continuous improvement

---

## Part 7: Extended Literature Synthesis (22 Papers)

### 7.1 Quantitative Reproducibility Benchmarks (Expanded)

| Study | System | Metric | Result | Key Insight |
|-------|--------|--------|--------|-------------|
| **Deep-Reproducer** (Chen 2025) | GPT-5 + self-evolving debug | Replication Score | 63.2% | +12.3% over AutoReproduce (49.6%) |
| **xKG** (Luo 2025) | PaperCoder + xKG | PaperBench Score | +10.9% | Grounded code nodes critical |
| **PaperScout** (Pan 2026) | PSPO optimizer | Recall on RealScholarQuery | 57.4% vs 54.1% baseline | Sequence-level optimization beats token-level |
| **ReportBench** (Li 2025) | OpenAI Deep Research | Citation match rate | 78.87% | ~40% citation hallucination rate |
| **CoDA** (Chen 2025) | 8-agent collaboration | Overall visualization score | 79.5% vs 55.0% baseline | +24.5pp from specialization |

### 7.2 Multi-Agent Architecture Patterns

From the comprehensive literature review, successful systems share these patterns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Successful Multi-Agent Patterns                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. SPECIALIZATION (CoDA, SciFig)                                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ Description â”‚â†’â†’â”‚   Layout    â”‚â†’â†’â”‚  Component  â”‚              â”‚
â”‚     â”‚    Agent    â”‚  â”‚    Agent    â”‚  â”‚    Agent    â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                     â”‚
â”‚  2. GROUNDING (xKG, Deep-Reproducer)                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚     â”‚  Technique  â”‚â”€â”€â”€â”€â”€â”€â†’â†’â”‚    Code     â”‚ (Only valid if code!)   â”‚
â”‚     â”‚    Node     â”‚        â”‚    Node     â”‚                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                     â”‚
â”‚  3. ITERATIVE REFINEMENT (SciFig, xKG)                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚     â”‚  Generate   â”‚â†â†â”€â”€â”€â”€â†’â†’â”‚  Feedback   â”‚ (52%â†’100% executability)â”‚
â”‚     â”‚             â”‚        â”‚   + Debug   â”‚                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                     â”‚
â”‚  4. SEQUENCE-LEVEL OPTIMIZATION (PaperScout/PSPO)                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚     â”‚ Token â†’ ... â†’ Token â†’ [Sequence Reward] â”‚                    â”‚
â”‚     â”‚ (Credit assignment at sequence level)   â”‚                    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 Why Hardcoding Is Universal Across LLM Systems

From reading all 22 papers, **every system** suffers from configuration management issues:

| Paper | Hardcoding Symptom | How They Handle It |
|-------|-------------------|-------------------|
| xKG | Fixed method names in prompts | Manual curation before ingestion |
| Deep-Reproducer | Path configurations hardcoded | LLM-in-loop manual curation |
| ReportBench | Template formats fixed | Per-prompt granularity options |
| CoDA | Example galleries hand-tuned | Self-evolution TODO list |
| SciFig | Color schemes fixed | Manual rubric definition |

**Key insight**: No paper has solved automatic parameter governance at generation time.

### 7.4 The Three Capability Phases (Future Roadmap)

From Deep Research literature synthesis, LLM scientific systems evolve through three phases:

```
Phase I: AGENTIC SEARCH (Current)
â”œâ”€â”€ Query planning (sequential/parallel/tree)
â”œâ”€â”€ Single-source retrieval
â””â”€â”€ Basic answer generation

Phase II: INTEGRATED RESEARCH (2-3 years)
â”œâ”€â”€ Multi-source acquisition with relevance scoring
â”œâ”€â”€ Short-term memory (per-session context)
â”œâ”€â”€ Structured workflows
â””â”€â”€ ðŸ’¡ This is where generation-time validation becomes critical

Phase III: FULL-STACK AI SCIENTIST (5+ years)
â”œâ”€â”€ Long-term knowledge graphs (like xKG)
â”œâ”€â”€ Self-correcting iterative refinement
â”œâ”€â”€ Learned tool selection (not fixed workflows)
â””â”€â”€ ðŸ’¡ This is where single-source-of-truth systems become mandatory
```

### 7.5 Specific Patterns from Each Paper Category

#### A. Artifact Generation Papers (SciFig, SlideTailor, CoDA)

**SciFig** (Figure Generation):
- 70.1% overall quality score (best automated)
- Hierarchical layout generation reduces visual clutter
- Iterative CoT feedback: 67.7% â†’ 71.6% (+3.9%)
- **Lesson**: Decompose figure generation into specialized agents

**SlideTailor** (Slide Generation):
- Multi-modal extraction from papers
- Layout preservation with content adaptation
- **Lesson**: Single source of truth (paper) â†’ multiple artifacts

**CoDA** (Data Visualization):
- 8 specialized agents with global TODO list
- 32,095 input tokens, 18,124 output tokens per query
- **Lesson**: Self-evolution mechanism critical for quality

#### B. Research Automation Papers (PaperScout, DeepResearchEval)

**PaperScout**:
- PSPO (Proximal Sequence Policy Optimization) beats PPO/GSPO
- 57.4% recall on complex research queries
- **Lesson**: Sequence-level optimization >> token-level

**DeepResearchEval**:
- Benchmark for evaluating AI research agents
- Metrics: comprehensiveness, factuality, coherence
- **Lesson**: Need systematic evaluation frameworks

#### C. Reproducibility Papers (Snelleman, xKG, ReportBench)

**Snelleman (Automated Reproducibility)**:
- 43.4% replication score (best LLMs)
- Proposes structured representation: Hypothesis â†’ Experiment â†’ Interpretation
- **Lesson**: Textual rules insufficient; need executable representation

**xKG**:
- 89.44% technique validity, 74.51% tech-code pair exactness
- Knowledge filtering eliminates hallucinations
- **Lesson**: Only keep techniques with grounded executable code

**ReportBench**:
- ~40% citation mismatch rate
- 95.83% non-cited factual accuracy
- **Lesson**: Verification systems catch errors that generation misses

### 7.6 Mapping Insights to Our R Hardcoding Problem

| Literature Insight | Our Application | Implementation |
|-------------------|-----------------|----------------|
| **xKG grounding principle** | Colors grounded in executable YAML | `color_defs <- load_color_definitions()` |
| **SciFig iterative feedback** | Validation loop before output | MCP tool for generation-time checking |
| **CoDA self-evolution** | Global figure registry | `configs/VISUALIZATION/figure_registry.yaml` |
| **PaperScout sequence optimization** | Treat figure as atomic unit | All panels share context (colors, theme) |
| **ReportBench verification** | Data provenance checking | JSON with cryptographic hashes |
| **Deep-Reproducer trajectory logging** | Parameter snapshots | Log model/preprocessing/timestamp |

---

## Part 8: Future-Facing Recommendations

### 8.1 Immediate (Do This Week)

1. **Install R precommit package**:
   ```r
   install.packages("precommit")
   precommit::use_precommit()
   ```

2. **Add generation-time reminder** in CLAUDE.md:
   ```markdown
   <R_FIGURE_GENERATION_CHECKLIST>
   Before outputting ANY R figure code:
   - [ ] All colors from color_defs, not hex literals
   - [ ] Using save_publication_figure(), not ggsave()
   - [ ] Using theme_foundation_plr(), no custom themes
   - [ ] Source files: config_loader.R, save_figure.R, theme_foundation_plr.R
   </R_FIGURE_GENERATION_CHECKLIST>
   ```

### 8.2 Short-Term (This Month)

3. **Build MCP validation tool** following xKG's "grounding" pattern
4. **Create figure registry** following CoDA's "global TODO list" pattern
5. **Implement trajectory logging** following Deep-Reproducer pattern

### 8.3 Medium-Term (This Quarter)

6. **Adopt sequence-level optimization** mindset:
   - Figure generation is atomic
   - All panels share context
   - Validate entire figure, not individual geoms

7. **Build verification pipeline** following ReportBench:
   - Check data provenance before rendering
   - Validate metric ranges
   - Flag anomalies (identical curves = synthetic data)

### 8.4 Long-Term (This Year)

8. **Evolve to Phase II system**:
   - Multi-source data integration
   - Session-level memory for figure consistency
   - Learned tool selection

9. **Work toward Phase III**:
   - Long-term knowledge graph of figure patterns
   - Self-correcting generation
   - Full artifact derivation pipeline

---

## References (Extended)

### Reproducibility & Replication
1. Snelleman, T. et al. (2026). "Automated Reproducibility Has a Problem Statement Problem." arXiv:2601.04226
2. Luo, Y. et al. (2025). "What Makes AI Research Replicable? Executable Knowledge Graphs." arXiv:2510.17795
3. Starace, G. et al. (2025). "PaperBench: Evaluating AI's Ability to Replicate AI Research." arXiv:2504.01848

### Artifact Generation
4. Huang, S. et al. (2026). "SciFig: Towards Automating Scientific Figure Generation." arXiv:2601.04390
5. Chen, Z. et al. (2025). "CoDA: Agentic Systems for Collaborative Data Visualization." arXiv:2510.03194
6. SlideTailor Team. "SlideTailor: Personalized Slide Generation from Papers."

### Research Automation
7. Pan, J. et al. (2026). "PaperScout: Autonomous Research Paper Discovery with PSPO." arXiv
8. Li, T. et al. (2025). "ReportBench: Evaluating Deep Research Agents." arXiv
9. Chen, Q. et al. (2025). "Deep-Reproducer: Self-Evolving Paper Reproduction." arXiv

### Multi-Agent Systems
10. Hong, S. et al. (2023). "MetaGPT: Meta Programming for Multi-Agent Collaboration."
11. Qin, L. et al. (2023). "ChatDev: Communicative Agents for Software Development."
12. Wu, Q. et al. (2023). "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations."

### Foundation PLR Project
13. Foundation PLR Meta-learnings: `.claude/docs/meta-learnings/CRITICAL-FAILURE-00*.md`
14. R Figure System: `src/r/figure_system/`
15. Color Definitions: `configs/colors.yaml`
